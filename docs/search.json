[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome SCHOOL Module 3: Disasters",
    "section": "",
    "text": "Welcome to the third module of the SCHOOL curriculum!\nThe Science Core Heuristics for Open Science Outcomes in Learning (SCHOOL) is part of the NASA Transform to Open Science (TOPS) Training (TOPST) initiative, designed to teach the data science lifecycle using data from the NASA Earth Sciences division and to foster an inclusive culture of open science. You can learn more about the SCHOOL Project and other modules on the SCHOOL Project home page.\nThis third SCHOOL module, titled “Disasters” explores natural and human-made hazards, examining the complex factors that lead to their occurrence, their impacts on communities, and the role of data in managing these events. Through several disaster-focused use cases, this module covers various examples and data sources, allowing learners to gain hands-on experience with real-world disaster data.\nEach use case utilizes a unique dataset, guiding users through lessons on data access, analysis, and visualization, including techniques for data cleaning, subsetting, and creating impactful visualizations to share findings.\nThis module is tailored to instruct undergraduate students and early-career researchers with some coding language exposure about the data science life cycle, illustrating how Open Science principles can be effectively applied to earth sciences, particularly in the context of water.\nThe SCHOOL Modules do not intend to teach all-encompassing earth science lessons nor provide learners with total coding expertise. Instead, the SCHOOL Project aims to provide users with the skills to adapt the SCHOOL lessons to the users’ own Open Science workflow. To learn more about Open Science, explore NASA’s TOPS Open Science 101 Curriculum. To explore other themes in the SCHOOL project, visit our Modules Page.\nModule 3: Disaster datasets and use cases cover:\n\nLeveraging IPUMS Census Data with Low Elevation Coastal Zone (LECZ) data\nThis lesson uses cloud based tools to analyze coastal elevation risks on census administrative areas focusing on Puerto Rico.\n\nLesson 1: Population in Low Elevation Coastal Zones (LECZ) in Puerto RICO\n\nGeospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach\nThis lesson focuses on the 2023 Yellowknife wildfire evacuation, using satellite-derived data to understand conditions that led to the evacuation of 20,000 residents.\n\nLesson 2: Analyzing Wildfire Impact Using NASA FIRMS and NOAA GOES-18 Data\n\nMore Lessons coming soon…\n\n\nStart Lesson\n\nThis course was made possible thanks to the work of our NASA Transform to Open Science (TOPS) team, our SCHOOL Open Science team, open science Subject Matter Experts (SMEs), and the SCHOOL Development team!"
  },
  {
    "objectID": "m302-wildfire-assessment.html",
    "href": "m302-wildfire-assessment.html",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "",
    "text": "In this lesson, we will analyze the 2023 Yellowknife wildfire evacuation using multiple satellite-derived datasets to assess evacuation route vulnerability and regional smoke impacts. You will learn how to access and process data from NASA’s Fire Information for Resource Management System (FIRMS), NOAA’s Geostationary Operational Environmental Satellite 18 (GOES-18) satellite, and Visible Infrared Imaging Radiometer Suite (VIIRS) Burned Area Product.\nThe analysis integrates active fire detections, smoke plumes, wind patterns, and infrastructure data to understand the complex conditions that led to the evacuation of approximately 20,000 residents. Through hands-on analysis, you will work with both near real-time data and post-processed remotely-sensed satellite products. The integration of these datasets shows how emergency managers can use publicly-available data to support evacuation decisions and understand hazard conditions.\nThis lesson emphasizes practical applications of remote sensing data for emergency management, while also exploring the strengths and limitations of different satellite products. By analyzing the Yellowknife evacuation, you will gain experience with workflows that can be applied to other wildfire events and natural hazards.\n 1\n\n\n\n\n\n\nPreface: A Note on Data Interpretation and Emergency Management\n\n\n\nThis lesson uses real data from an evacuation that impacted thousands of residents. While we focus on technical analysis, it’s important to remember that wildfire evacuation decisions involve many factors beyond what can be captured in satellite data. The workflows demonstrated here are meant to complement, not replace, the expertise of emergency managers, fire behavior specialists, and local authorities.\nThe integration of multiple data sources can provide valuable insights, but satellite data has limitations: - Cloud cover can obstruct observations - Temporal and spatial resolution may miss important changes - Data latency may affect real-time decision making - Local conditions may differ from satellite-based measurements\nAlways consider these limitations when using remote sensing data to support emergency management decisions."
  },
  {
    "objectID": "m302-wildfire-assessment.html#technical-competencies",
    "href": "m302-wildfire-assessment.html#technical-competencies",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Technical Competencies",
    "text": "Technical Competencies\n\nConfigure and authenticate API access to multiple NASA Earth data products\nProcess and harmonize multi-source remote sensing data using R’s spatial packages (sf, terra, stars)\nImplement spatial analysis workflows combining vector and raster data for wildfire assessment\nCreate publication-quality static visualizations and basic interactive maps of wildfire impacts"
  },
  {
    "objectID": "m302-wildfire-assessment.html#analytical-skills",
    "href": "m302-wildfire-assessment.html#analytical-skills",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Analytical Skills",
    "text": "Analytical Skills\n\nEvaluate the progression and impact of wildfires using multiple remote sensing products\nAssess infrastructure vulnerability and damage using spatial analysis techniques\nIntegrate multiple spatial datasets to conduct a comprehensive wildfire impact assessment\nCritical analyze the strengths and limitations of different remote sensing products for fire analysis"
  },
  {
    "objectID": "m302-wildfire-assessment.html#practical-applications",
    "href": "m302-wildfire-assessment.html#practical-applications",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nApply real-world workflows for accessing and processing wildfire-related remote sensing data\nDemonstrate best practices for reproducible spatial analysis in R\nImplement methods for combining satellite-derived fire products with infrastructure data\nDocument and communicate spatial analysis results effectively"
  },
  {
    "objectID": "m302-wildfire-assessment.html#advanced-understanding-optional-extensions",
    "href": "m302-wildfire-assessment.html#advanced-understanding-optional-extensions",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Advanced Understanding (Optional Extensions)",
    "text": "Advanced Understanding (Optional Extensions)\n\nDevelop automated workflows for multi-sensor data integration\nApply uncertainty assessment in wildfire impact analysis\nCreate advanced visualizations for temporal pattern analysis\nImplement validation approaches using multiple data sources"
  },
  {
    "objectID": "m302-wildfire-assessment.html#wildfires-and-the-2023-season",
    "href": "m302-wildfire-assessment.html#wildfires-and-the-2023-season",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Wildfires and the 2023 Season",
    "text": "Wildfires and the 2023 Season\nWildfires are a natural and essential component of North American forest ecosystems, particularly in the boreal forests that stretch across Canada (Girardin, Ali, and Hély 2010; Johnson, Miyanishi, and Bridge 2001; Zackrisson, Nilsson, and Wardle 1996). These fires play a crucial role in maintaining forest health, recycling nutrients, and promoting biodiversity (Running 2006). However, climate change has begun to alter historical fire regimes, leading to more frequent, intense, and extensive wildfires that pose increasing challenges to both ecosystems and human communities (Mora et al. 2018; Pausas and Keeley 2021; Jones et al. 2024).\nIn Canada’s boreal forests, fire has historically occurred in natural cycles ranging from 50 to 200 years, helping to maintain a mosaic of forest ages and compositions. Indigenous peoples have long understood and managed these fire regimes, using controlled burns as a land management tool (Christianson 2014; Lewis 1978; W. Wang et al. 2022). Today, changing climate conditions are disrupting these traditional patterns. Longer fire seasons, increased lightning activity, and more severe drought conditions are creating unprecedented fire behavior and risk scenarios.\n2\nThe human impacts of these changing fire regimes are particularly acute in Canada’s northern communities, where limited road networks often leave few evacuation options. Additionally, smoke from these fires can affect air quality across vast regions, creating public health challenges even for communities far from the actual flames (Jones et al. 2024). The economic implications are also significant, affecting industries from tourism to mining, and straining emergency response resources.\nThe 2023 wildfire season emerged as a stark illustration of these evolving challenges. The Northwest Territories experienced one of its most devastating fire seasons on record, characterized by record-breaking temperatures and severe drought conditions (Jones et al. 2024). The season saw over 236 fires that burned approximately 4 million hectares of land, over twice the previous record in Canada and over seven times the annual average, forcing the first complete evacuation of Yellowknife in its history. The scale of displacement was unprecedented, with an estimated 70% of Northwest Territories residents forced to leave their homes (Ridgen 2024). Critical infrastructure, including communities, mines, and transportation networks, faced significant threats, highlighting the vulnerability of northern communities to extreme fire events.\nThese events showcased both the immediate and long-term challenges of managing wildfires in a changing climate, while also demonstrating the critical importance of robust monitoring and early warning systems. The season served as a powerful example of why enhanced understanding of fire behavior and improved prediction capabilities are essential for protecting communities and ecosystems in an increasingly fire-prone world.\n3\nThe Northwest Territories (NWT) encompasses a vast expanse of northern Canada, covering approximately 1.14 million square kilometers of diverse terrain dominated by boreal forest ecosystems. This region represents one of Canada’s most fire-prone areas, where the interplay of climate, vegetation, and geography creates conditions conducive to regular fire activity. The landscape is fundamentally shaped by the Canadian Shield, an ancient bedrock formation characterized by exposed Precambrian rock, countless lakes, and wetlands that create a complex mosaic of fire-susceptible and fire-resistant terrain.\n\nYellowknife\n4\nAt the heart of this region lies Yellowknife, the territorial capital, situated on the northern shore of Great Slave Lake. With approximately 20,000 residents, it serves as the primary urban center and economic hub of the NWT (City of Yellowknife 2025). The city’s boreal forest setting is dominated by black spruce and jack pine, species that have evolved with fire and even depend on it for regeneration. Black spruce, in particular, has adapted to release its seeds from sealed cones when exposed to the intense heat of forest fires, while jack pine stands often require fire to maintain their ecological dominance.\nThe region experiences a subarctic climate, characterized by long, cold winters where temperatures regularly drop below -30°C, and short, warm summers with extended daylight hours. This climate regime creates a distinct fire season that, historically, has been concentrated in the brief summer months. However, climate change is increasingly extending this fire season at both ends, creating new challenges for fire management and community safety (Pausas and Keeley 2021; Running 2006).\nInfrastructure in the NWT is notably limited, with Yellowknife connected to southern Canada by a single highway (Highway 3) and air travel serving as a crucial transportation link. This isolation is even more pronounced for the numerous remote communities scattered throughout the territory, many of which are accessible only by air or seasonal ice roads. This limited transportation network creates particular challenges for emergency management and evacuation scenarios during severe fire events, as demonstrated during recent fire seasons.\n\n\nThe 2023 Yellowknife Wildfire Crisis\n5\nThe summer of 2023 brought unprecedented wildfire activity to the Northwest Territories, culminating in the evacuation of Yellowknife in mid-August. The immediate threat came from the Behchokǫ̀ wildfire (designated ZF015-23 (MNP 2024)), which began southwest of Yellowknife and rapidly expanded under extreme fire weather conditions. By August 15th, the fire had grown to over 167,000 hectares and was advancing approximately 1.2 kilometers per hour toward the territorial capital (Northwest Territories 2024).\n6\nOn August 16th, territorial officials issued a mandatory evacuation order for all 20,000 Yellowknife residents, marking the largest evacuation in NWT history (Graveland 2023). The evacuation presented significant logistical challenges due to limited egress routes. Highway 3, the only road connecting Yellowknife to southern Canada, became a critical lifeline with evacuees forming long convoys south toward Alberta. Meanwhile, military and civilian aircraft conducted one of the largest air evacuations in Canadian history, prioritizing elderly residents, those with medical needs, and individuals without personal vehicles (Lindeman 2023).\nThe fire’s behavior was particularly concerning due to the extreme conditions: temperatures reaching 30°C, relative humidity below 30%, and strong, erratic winds gusting over 40 km/h. These conditions, combined with the region’s unusually dry summer, created what fire behavior specialists termed “crown fire conditions” - where fires spread rapidly through the upper canopy of the forest (X. Wang et al. 2024). By August 17th, the fire had reached within 15 kilometers of Yellowknife’s city limits.\nEmergency response efforts focused on establishing protective measures around the city, including fire breaks and sprinkler systems. Fire crews, supported by the Canadian military, worked to protect critical infrastructure including the airport, power plants, and communications facilities. The nearby communities of Dettah and Ndilǫ also faced evacuation orders, highlighting the fire’s broad regional impact.\nThe evacuation order remained in effect for 21 days, with residents unable to return until September 6th. This extended displacement period had significant social and economic impacts on the community, while also demonstrating the challenges of protecting isolated northern communities from wildfire threats. The event served as a stark reminder of the increasing vulnerability of northern communities to extreme fire events in a changing climate.\n\n\nWildfire and Climate Change\nThe unprecedented scale and intensity of the 2023 NWT fire season exemplifies the broader impacts of climate change on northern fire regimes. Canada’s Arctic regions are warming at approximately three times the global average rate, fundamentally altering the conditions that have historically governed wildfire behavior (Environment and Canada 2023). This accelerated warming is creating longer fire seasons, with snow melting earlier in spring and winter arriving later in fall, extending the period during which fires can ignite and spread (Climate Change (IPCC) 2023). The warming trend is particularly pronounced in the Northwest Territories, where average temperatures have increased by 2.3°C since 1948, compared to a 1.1°C increase globally.\nThese changing conditions are challenging traditional approaches to fire management and community protection. Historical fire behavior models, based on decades of past observations, are becoming less reliable as new climate patterns emerge. Fire intensity is increasing due to drier fuels and more frequent extreme weather events, while fire spread patterns are becoming more erratic and less predictable. For northern communities like Yellowknife, these changes represent an existential challenge: traditional fire breaks may prove insufficient, evacuation routes may become more frequently threatened, and the resources required for fire suppression may exceed historical requirements. The 2023 evacuation of Yellowknife serves as a harbinger of the complex challenges that northern communities face in adapting to this new reality of fire risk in a warming climate.\n\n\nGeospatial Analysis in Emergency Response and Resource Management\nModern wildfire management increasingly relies on geospatial analysis and remote sensing technologies to support critical decision-making. Natural resource managers and emergency response teams leverage multiple satellite platforms, weather data, and spatial analysis techniques to monitor fire progression, assess risks, and coordinate response efforts in near-real-time. During the Yellowknife evacuation, these tools proved instrumental in tracking the Behchokǫ̀ wildfire’s progression, predicting its behavior, and making informed evacuation decisions.\nThe integration of multiple data sources - from active fire detections to smoke forecasts - allows emergency managers to develop a comprehensive understanding of evolving fire situations. NASA’s FIRMS provides near-real-time fire locations, while GOES satellites track smoke plume development and movement. When combined with infrastructure data and analyzed through modern spatial techniques, these datasets enable rapid assessment of threats to communities, critical infrastructure, and evacuation routes. Post-fire, these same tools support damage assessment and recovery planning through the analysis of burn severity and infrastructure impacts.\nThe Yellowknife wildfire crisis of 2023 serves as an ideal case study for demonstrating these analytical approaches. In the following analysis, we will explore how different remote sensing products can be integrated to monitor and assess wildfire impacts, particularly in northern communities where traditional ground-based monitoring may be limited. We will demonstrate techniques for processing and analyzing multiple satellite data streams, assessing infrastructure vulnerability, and quantifying fire impacts - the same types of analyses that support real-world emergency response and resource management decisions. This practical application of geospatial analysis techniques provides valuable insights into both the technical methods and their critical role in modern fire management."
  },
  {
    "objectID": "m302-wildfire-assessment.html#analysis",
    "href": "m302-wildfire-assessment.html#analysis",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\nData Science Review\n\n\n\nBefore beginning, please note that this lesson uses the R programming language and the following R packages:\n\nsf: Essential for handling vector spatial data and manipulation of geographic objects.\nstars: Extends spatial functionality to handle raster data and spatiotemporal arrays, particularly useful for satellite imagery.\nosmdata: Provides interface for OpenStreetMap data, enabling programmatic access to road networks and infrastructure data.\naws.s3: Enables direct access to AWS S3 storage, essential for retrieving GOES satellite data.\nhttr2: Modern HTTP client for R, used for making API requests and handling responses.\nncdf4: Interface to Unidata netCDF format, crucial for working with satellite data files.\nconcaveman: Implements concave hull algorithms for creating boundaries around point sets.\ndbscan: Provides density-based clustering algorithms for spatial point pattern analysis.\ndplyr: Part of tidyverse, essential for data manipulation and transformation.\nggplot2: Creates elegant data visualizations and is the primary plotting tool in this lesson.\ntidyterra: Bridges terra spatial objects with tidyverse tools, particularly for visualization.\nterra: Advanced package for spatial data handling, particularly raster data processing.\nknitr: Generates dynamic reports and handles table formatting.\nkableExtra: Enhances table formatting capabilities with additional styling options.\nunits: Handles unit conversions and dimensional analysis in spatial calculations.\n\nIf you’d like to learn more about the functions used in this lesson, you can refer to the documentation on their respective websites.\nThe sf package is fundamental for spatial data operations, while stars handles raster and spatiotemporal data. osmdata enables access to OpenStreetMap data for infrastructure analysis. aws.s3 and httr2 are crucial for data access via cloud storage and APIs respectively.\nThe ncdf4 package is essential for working with satellite data files, while concaveman and dbscan provide specialized spatial analysis capabilities. dplyr and ggplot2 form the backbone of data manipulation and visualization, enhanced by tidyterra for spatial data integration.\nterra provides advanced spatial data processing capabilities, particularly for raster analysis. knitr and kableExtra handle report generation and table formatting, while units ensures proper handling of physical quantities and conversions.\nMake sure these packages are installed before you begin working with the code in this document.\nYou can install the required packages using R:\n\n# List of required packages\nrequired_packages &lt;- c(\n  \"sf\", \"stars\", \"osmdata\", \"aws.s3\", \"httr2\", \"ncdf4\",\n  \"concaveman\", \"dbscan\", \"dplyr\", \"ggplot2\", \"tidyterra\",\n  \"terra\", \"knitr\", \"kableExtra\", \"units\", \"maptiles\"\n)\n\n# Function to check and install missing packages\ninstall_if_missing &lt;- function(packages) {\n  installed &lt;- installed.packages()\n  for (pkg in packages) {\n    if (!pkg %in% rownames(installed)) {\n      message(sprintf(\"Installing package: %s\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    } else {\n      message(sprintf(\"Package %s is already installed.\", pkg))\n    }\n  }\n}\n\n# Run the function\ninstall_if_missing(required_packages)\n\n\n\n\nInitial Data Processing & Study Area Definition\nTo begin our analysis, we’ll need to define our study area around Yellowknife. The City of Yellowknife provides municipal boundary data through their open data portal. We’ll use this boundary as our primary study area and create buffered zones for different aspects of our analysis.\nFirst, let’s set up our data directory and download the boundary file:\n\n# Create a data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")\n}\n\n# Download Yellowknife municipal boundary\nboundary_url &lt;- \"https://hub.arcgis.com/api/v3/datasets/5959bad3315946c0827f85d8cdeb3b8c_0/downloads/data?format=shp&spatialRefId=26911&where=1%3D1\"\n\n# Perform GET request with httr\nres &lt;- httr::GET(\n  url = boundary_url,\n  httr::write_disk(\"data/yellowknife_boundary.zip\", overwrite = TRUE),\n  httr::config(followlocation = TRUE),\n  httr::verbose()\n)\n\n\nres\n\nResponse [https://hub.arcgis.com/api/v3/datasets/5959bad3315946c0827f85d8cdeb3b8c_0/downloads/data?format=shp&spatialRefId=26911&where=1%3D1]\n  Date: 2025-04-11 19:47\n  Status: 200\n  Content-Type: application/zip\n  Size: 2.59 kB\n&lt;ON DISK&gt;  F:\\TOPSSCHOOL\\git\\TOPSTSCHOOL-disasters\\data\\yellowknife_boundary.zip\n\n\nNext, we’ll unzip the downloaded file and read the municipal boundary layer. The data comes in a geodatabase format and is already in UTM Zone 11N (EPSG:26911):\n\n# Unzip the file\nutils::unzip(\"data/yellowknife_boundary.zip\", \n             exdir = \"data/yellowknife_boundaries\")\n\n\n# Read the municipal boundary layer\nyk_boundary &lt;- sf::st_read(\n  \"data/yellowknife_boundaries/Municipal_Boundary.shp\"\n) |&gt; \n  identity()\n\nReading layer `Municipal_Boundary' from data source \n  `F:\\TOPSSCHOOL\\git\\TOPSTSCHOOL-disasters\\data\\yellowknife_boundaries\\Municipal_Boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 627843.7 ymin: 6922154 xmax: 639196 ymax: 6937449\nProjected CRS: NAD83 / UTM zone 11N\n\n\nLet’s examine the structure of our boundary object to ensure we have the data we need:\n\n# Examine the structure of the boundary object\nutils::str(yk_boundary)\n\nClasses 'sf' and 'data.frame':  1 obs. of  5 variables:\n $ OBJECTID  : int 2\n $ Shape__Are: num 1.37e+08\n $ Shape__Len: num 52067\n $ GlobalID  : chr \"{4686254D-0548-4E65-A506-388EA2A1414E}\"\n $ geometry  :sfc_POLYGON of length 1; first list element: List of 1\n  ..$ : num [1:11, 1:2] 627844 632135 631950 638805 639196 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:4] \"OBJECTID\" \"Shape__Are\" \"Shape__Len\" \"GlobalID\"\n\n\nFinally, we’ll create a basic map of our study area using ggplot2. This will help us verify that we’ve properly loaded the boundary data:\n\n# Create a basic map of our study area\nggplot2::ggplot() +\n  ggplot2::geom_sf(data = yk_boundary, \n                   fill = \"lightblue\", \n                   color = \"black\") +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Yellowknife Municipal Boundary\",\n    subtitle = paste0(\"Municipal Area: \", \n                     round(sf::st_area(yk_boundary)/1000000, 2), \n                     \" sq km\"),\n    caption = \"Data source: City of Yellowknife Open Data Portal\"\n  ) +\n  ggplot2::coord_sf()\n\n\n\n\n\n\n\n\nThis boundary will serve as our primary reference for analyzing the wildfire impacts around Yellowknife. Throughout our analysis, we’ll create different buffer zones around this boundary to capture various aspects of the fire event, such as: - A 50km buffer for analyzing local fire detections - A 500km buffer for analyzing smoke plume dispersion - Custom buffers for specific analyses of evacuation routes and infrastructure\n\n\n\n\n\n\nData Science Review\n\n\n\nWhen working with spatial data it’s important to understand a few key concepts:\n\nCoordinate Reference Systems (CRS): Different spatial datasets might use different coordinate systems. The Yellowknife boundary data comes in UTM Zone 11N (EPSG:26911), which is appropriate for this region of Canada.\nSpatial Objects in R: We’re using the sf (Simple Features) package, which is the modern standard for handling vector spatial data in R. The sf package integrates well with the tidyverse and provides efficient tools for spatial operations.\nData Formats: Spatial data comes in many formats. Here we’re working with a geodatabase (.gdb), but you might also encounter shapefiles (.shp), GeoJSON, and other formats. The sf package can handle most common spatial data formats.\n\n\n\n\n\nFire Detection\nSatellite-based fire detection has transformed our ability to monitor and respond to wildfires, particularly in remote regions like Canada’s Northwest Territories. These systems use thermal anomaly detection algorithms that identify areas significantly warmer than their surroundings, enabling near real-time monitoring of fire activity across vast landscapes. When combined with ground observations and other data sources, satellite fire products provide crucial information for fire managers, emergency responders, and researchers studying fire behavior and impacts.\nSeveral major satellite systems provide operational fire detection products. NASA’s MODIS (Moderate Resolution Imaging Spectroradiometer) instruments aboard the Terra and Aqua satellites have been providing global fire detections at 1km resolution since 2000, making them valuable for long-term studies. The VIIRS (Visible Infrared Imaging Radiometer Suite) sensors on the Suomi-NPP and NOAA-20 satellites offer improved detection capabilities with 375m resolution, able to detect smaller fires and provide more precise fire perimeter mapping. Geostationary satellites like GOES-16 and GOES-17 provide rapid fire detection updates every 5-15 minutes, though at coarser spatial resolution. The European Space Agency’s Sentinel-3 satellites also provide global fire detection through their Sea and Land Surface Temperature Radiometer (SLSTR) instrument.\nIn this analysis, we’ll focus on two complementary fire products that helped track the Yellowknife fires. The VIIRS 375m active fire product from NOAA-20, accessed through NASA’s FIRMS (Fire Information for Resource Management System), provides detailed fire detection points with associated fire radiative power measurements. We’ll combine this with the VIIRS Burned Area product (VNP64A1), which uses changes in surface reflectance to map the final extent of burned areas at 500m resolution. Together, these products allow us to analyze both the progression of active burning and the ultimate footprint of the fires.\n\nFire Information for Resource Management System (FIRMS)\nNASA’s Fire Information for Resource Management System (FIRMS) distributes near real-time active fire data within 3 hours of satellite observation. The system synthesizes fire detections from both MODIS and VIIRS instruments, though we’ll focus on the VIIRS 375m data from NOAA-20 for its enhanced spatial resolution. Each FIRMS record provides not just the location of detected fires, but also their intensity measured as Fire Radiative Power (FRP) in megawatts. FRP serves as a proxy for fire intensity, with higher values generally indicating more intense burning and greater fuel consumption. This combination of location, timing, and intensity data makes FIRMS an invaluable tool for tracking fire progression and identifying areas of most intense fire activity. During the Yellowknife evacuation, FIRMS data provided critical information about fire location and intensity to emergency managers and the public.\nFirst, we need to create our search area and acquire relevant basemap data. We’ll create a 50km buffer around Yellowknife to capture the local fire activity:\n\n# Create search area and get bounding box\nsearch_area &lt;- sf::st_buffer(yk_boundary, 50000)\nsearch_area_wgs84 &lt;- sf::st_transform(search_area, 4326)\nbbox &lt;- sf::st_bbox(search_area_wgs84) |&gt;\n  round(4)\n\n# Download satellite imagery for our area\nbasemap_tiles &lt;- maptiles::get_tiles(\n  search_area_wgs84,\n  provider = \"Esri.WorldImagery\",\n  zoom = 11  # Adjust zoom level as needed\n)\n\nNext, we’ll query the FIRMS API to get fire detections for our area. The FIRMS API requires registration for an API Map key, which should be stored securely in your environment variables. The key will be sent to your email. Request a FIRMS API Map Key Here\n\n# Get FIRMS data\nfirms_url &lt;- sprintf(\n  \"https://firms.modaps.eosdis.nasa.gov/api/area/csv/%s/%s/%.4f,%.4f,%.4f,%.4f/%d/%s\",\n  \"API_MAP_KEY\",   # your MAP_KEY\n  \"VIIRS_NOAA20_SP\",                   # product (SOURCE)\n  bbox[\"xmin\"], bbox[\"ymin\"],         # WEST, SOUTH\n  bbox[\"xmax\"], bbox[\"ymax\"],         # EAST, NORTH\n  10,                                   # DAY_RANGE\n  \"2023-08-10\"                         # DATE\n)\n\nresp &lt;- httr2::request(firms_url) |&gt;\n  httr2::req_perform()\n\nWe’ll process the FIRMS data into a spatial object and add proper datetime fields:\n\ndf &lt;- httr2::resp_body_string(resp) |&gt;\n  textConnection() |&gt;\n  utils::read.csv()\n\nprint(head(df))\n\n  latitude longitude bright_ti4 scan track   acq_date acq_time satellite\n1 62.23220 -113.5545     299.16 0.61  0.71 2023-08-10      821       N20\n2 62.23757 -113.5919     296.29 0.61  0.71 2023-08-10      821       N20\n3 62.23775 -113.6764     295.23 0.62  0.71 2023-08-10      821       N20\n4 62.10626 -113.4029     312.93 0.39  0.36 2023-08-10     1002       N20\n5 62.10647 -113.4500     311.58 0.39  0.36 2023-08-10     1002       N20\n6 62.10658 -113.4736     302.50 0.39  0.36 2023-08-10     1002       N20\n  instrument confidence version bright_ti5  frp daynight type\n1      VIIRS          n       2     275.69 3.59        N    0\n2      VIIRS          n       2     270.43 2.53        N    0\n3      VIIRS          n       2     269.47 1.98        N    0\n4      VIIRS          n       2     283.30 3.07        N    0\n5      VIIRS          n       2     285.95 2.63        N    0\n6      VIIRS          n       2     284.55 1.81        N    0\n\nstr(df)\n\n'data.frame':   8886 obs. of  15 variables:\n $ latitude  : num  62.2 62.2 62.2 62.1 62.1 ...\n $ longitude : num  -114 -114 -114 -113 -113 ...\n $ bright_ti4: num  299 296 295 313 312 ...\n $ scan      : num  0.61 0.61 0.62 0.39 0.39 0.39 0.39 0.39 0.39 0.39 ...\n $ track     : num  0.71 0.71 0.71 0.36 0.36 0.36 0.36 0.36 0.36 0.36 ...\n $ acq_date  : chr  \"2023-08-10\" \"2023-08-10\" \"2023-08-10\" \"2023-08-10\" ...\n $ acq_time  : int  821 821 821 1002 1002 1002 1002 1002 1002 1002 ...\n $ satellite : chr  \"N20\" \"N20\" \"N20\" \"N20\" ...\n $ instrument: chr  \"VIIRS\" \"VIIRS\" \"VIIRS\" \"VIIRS\" ...\n $ confidence: chr  \"n\" \"n\" \"n\" \"n\" ...\n $ version   : int  2 2 2 2 2 2 2 2 2 2 ...\n $ bright_ti5: num  276 270 269 283 286 ...\n $ frp       : num  3.59 2.53 1.98 3.07 2.63 1.81 2.38 2.24 1.49 1.63 ...\n $ daynight  : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ type      : int  0 0 0 0 0 0 0 0 0 0 ...\n\n\n\n# Process FIRMS data\nfirms_points &lt;- httr2::resp_body_string(resp) |&gt;\n  textConnection() |&gt;\n  utils::read.csv() |&gt;\n  sf::st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  ) |&gt; \n  sf::st_transform(sf::st_crs(yk_boundary))\n\n\n# Process FIRMS data\nfirms_points &lt;- httr2::resp_body_string(resp) |&gt;\n  textConnection() |&gt;\n  utils::read.csv() |&gt;\n  sf::st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  ) |&gt; \n  sf::st_transform(sf::st_crs(yk_boundary))\n\n# Add proper datetime field and date\nfirms_points$datetime &lt;- as.POSIXct(\n  paste(firms_points$acq_date, \n        sprintf(\"%04d\", firms_points$acq_time)),\n  format = \"%Y-%m-%d %H%M\",\n  tz = \"UTC\"\n)\nfirms_points$date &lt;- as.Date(firms_points$acq_date)\n\nTo understand the temporal progression of fire activity, we’ll calculate daily statistics:\n\n# Calculate daily statistics\ndaily_stats &lt;- firms_points |&gt;\n  as.data.frame() |&gt;\n  dplyr::group_by(date) |&gt;\n  dplyr::summarise(\n    detections = dplyr::n(),\n    mean_frp = mean(frp),\n    max_frp = max(frp),\n    total_frp = sum(frp)\n  )\ndaily_stats$date &lt;- as.Date(daily_stats$date)\n\n\ndaily_stats\n\n# A tibble: 10 x 5\n   date       detections mean_frp max_frp total_frp\n   &lt;date&gt;          &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 2023-08-10        898     7.74  259.      6946. \n 2 2023-08-11        569     2.88   73.9     1640. \n 3 2023-08-12        286     4.87  102.      1394. \n 4 2023-08-13        880    57.4  1644.     50503. \n 5 2023-08-14       1772     2.74   27.2     4862. \n 6 2023-08-15       2345     4.63  173.     10849. \n 7 2023-08-16       2067     7.11  438.     14705. \n 8 2023-08-17         33     4.89   33.3      161. \n 9 2023-08-18         16     6.58   25.6      105. \n10 2023-08-19         20     1.20    2.48      24.0\n\n\n\n\n\n\n\n\nUnderstanding Fire Radiative Power (FRP)\n\n\n\nFire Radiative Power (FRP) is a measure of the rate of radiant heat output from a fire, measured in megawatts (MW). Higher FRP values generally indicate more intense fires. The VIIRS sensor can detect:\n\nSmaller/cooler fires: ~1-10 MW\nModerate fires: ~10-100 MW\nLarge/intense fires: &gt;100 MW\n\nFRP helps fire managers assess fire intensity and potential severity, though it’s important to note that factors like cloud cover can affect detection capabilities.\n\n\nNow we can create a series of visualizations to understand the spatial and temporal patterns of fire activity. First, let’s create a map showing all fire detections colored by their intensity (FRP):\n\n# Create main static visualization\np1 &lt;- ggplot2::ggplot() +\n  tidyterra::geom_spatraster_rgb(data = basemap_tiles) +\n  ggplot2::geom_sf(data = search_area, \n                   fill = NA, \n                   color = \"gray60\", \n                   linetype = \"dashed\") +\n  ggplot2::geom_sf(data = yk_boundary, \n                   fill = NA, \n                   color = \"#00BFFF\", \n                   linewidth = 1.2) +\n  ggplot2::geom_sf(data = firms_points,\n                   ggplot2::aes(color = frp),\n                   alpha = 0.7,\n                   size = 2) +\n  ggplot2::scale_color_gradientn(\n    colors = c(\"orange\", \"orange red\", \"red\", \"dark red\", \"purple\"),\n    name = \"Fire Radiative\\nPower (MW)\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Yellowknife Fire Detections\",\n    subtitle = paste(\"August 10-23, 2023\\nTotal detections:\", nrow(firms_points)),\n    caption = \"Data: VIIRS 375m from NOAA-20 satellite\"\n  )\n\nprint(p1)\n\n\n\n\n\n\n\n\nTo understand how fire activity changed over time, we’ll create two temporal plots - one showing the number of daily detections and another showing fire intensity:\n\n# Define evacuation date\nevac_date &lt;- as.Date(\"2023-08-16\")\n\n# Create temporal progression plot\np2 &lt;- ggplot2::ggplot(daily_stats, ggplot2::aes(x = date)) +\n  ggplot2::geom_vline(xintercept = evac_date, \n                      color = \"purple\", \n                      linetype = \"dashed\",\n                      alpha = 0.85,\n                      linewidth = 1.5) +\n  ggplot2::geom_line(ggplot2::aes(y = detections), color = \"#00BFFF\", linewidth = 1) +\n  ggplot2::geom_point(ggplot2::aes(y = detections), color = \"#00BFFF\", size = 3) +\n  ggplot2::scale_x_date(date_breaks = \"1 day\", date_labels = \"%b %d\") +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Daily Fire Detections\",\n    subtitle = \"Purple line indicates evacuation date (Aug 16)\",\n    y = \"Number of detections\",\n    x = \"Date\"\n  )\n\nprint(p2)\n\n\n\n\n\n\n\n\nFinally, let’s examine how fire intensity changed throughout the period:\n\n# Create FRP intensity plot with proper legend\np3 &lt;- ggplot2::ggplot(daily_stats, ggplot2::aes(x = date)) +\n    ggplot2::geom_vline(xintercept = evac_date, \n                        color = \"purple\", \n                        linetype = \"dashed\",\n                        alpha = 0.85,\n                        linewidth = 1.5) +\n    ggplot2::geom_line(ggplot2::aes(y = mean_frp, color = \"Mean FRP\"), linewidth = 1) +\n    ggplot2::geom_point(ggplot2::aes(y = mean_frp, color = \"Mean FRP\"), size = 3) +\n    ggplot2::geom_line(ggplot2::aes(y = max_frp, color = \"Max FRP\"), linewidth = 1) +\n    ggplot2::geom_point(ggplot2::aes(y = max_frp, color = \"Max FRP\"), size = 3) +\n     ggplot2::scale_x_date(date_breaks = \"1 day\", date_labels = \"%b %d\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(\n      title = \"Fire Radiative Power Over Time\",\n      subtitle = \"Purple line indicates evacuation date (Aug 16)\",\n      y = \"Fire Radiative Power (MW)\",\n      x = \"Date\"\n    ) +\n    ggplot2::scale_color_manual(\n      values = c(\"Mean FRP\" = \"#00BFFF\", \"Max FRP\" = \"#0040ff\"),\n      name = \"Metric\"\n    )\n\nprint(p3)\n\n\n\n\n\n\n\n\nLet’s summarize our findings in a clear table:\n\n# Create summary statistics table using kableExtra\nstats_df &lt;- data.frame(\n  Metric = c(\n    \"Date Range\",\n    \"Total Detections\",\n    \"Mean FRP\",\n    \"Max FRP\",\n    \"Peak Detection Date\",\n    \"Peak Daily Detections\"\n  ),\n  Value = c(\n    paste(min(firms_points$date), \"to\", max(firms_points$date)),\n    as.character(nrow(firms_points)),\n    paste(round(mean(firms_points$frp), 2), \"MW\"),\n    paste(round(max(firms_points$frp), 2), \"MW\"),\n    as.character(daily_stats$date[which.max(daily_stats$detections)]),\n    as.character(daily_stats$detections[which.max(daily_stats$detections)])\n  )\n)\n\n# Create formatted HTML table\nknitr::kable(stats_df, \n             format = \"html\",\n             caption = \"Yellowknife Fire Analysis Summary\") |&gt;\n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\"),\n    full_width = FALSE,\n    position = \"left\"\n  ) |&gt;\n  kableExtra::row_spec(0, bold = TRUE)\n\n\nYellowknife Fire Analysis Summary\n\n\nMetric\nValue\n\n\n\n\nDate Range\n2023-08-10 to 2023-08-19\n\n\nTotal Detections\n8886\n\n\nMean FRP\n10.26 MW\n\n\nMax FRP\n1643.92 MW\n\n\nPeak Detection Date\n2023-08-15\n\n\nPeak Daily Detections\n2345\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Review\n\n\n\nSeveral key data science concepts are demonstrated in this analysis:\n\nAPI Interaction: We use the FIRMS API to programmatically access fire detection data, demonstrating how to construct and execute API requests.\nSpatial Data Processing: The analysis shows how to:\n\nTransform between coordinate systems\nCreate spatial buffers\nJoin spatial and temporal data\n\nData Visualization Best Practices:\n\nUsing appropriate color scales for fire intensity\nIncluding reference lines for key events (evacuation date)\nProviding clear labels and legends\nCreating multi-panel visualizations for different aspects of the data\n\n\n\n\n\nFire Complex Perimeters\nWhile individual fire detections give us point-based information about fire activity, understanding the overall fire extent and progression requires analyzing these points as continuous complexes. We’ll use spatial clustering and concave hull techniques to create meaningful fire perimeters from our point detections.\nFirst, let’s create a function that will generate fire perimeters for a given date using concave hull geometry:\n\n  # Create perimeters with adjusted parameters\n\n # The concavity parameter (default = 1) controls how tightly the hull wraps around points:\n  # - Lower values (e.g., 0.5) create more detailed, tighter boundaries\n  # - Higher values (e.g., 2) create smoother, more generalized boundaries\n  # The threshold parameter sets the minimum segment length (in coordinate units)\n  # to prevent very small segments in the hull\n\ncreate_fire_perimeters &lt;- function(points,\n                                   date,\n                                   # Controls how tightly hull fits points\n                                   concavity = 1,\n                                   # Minimum segment length\n                                   threshold = 1) {\n  \n  daily_points &lt;- points |&gt;\n    dplyr::filter(date == !!date)\n  \n  if (nrow(daily_points) &lt; 3) {\n    return(NULL)\n  }\n  \n  coords &lt;- sf::st_coordinates(daily_points)\n  hull &lt;- concaveman::concaveman(coords, concavity = concavity, length_threshold = threshold)\n  \n  polygon_sf &lt;- sf::st_polygon(list(hull)) |&gt;\n    sf::st_sfc(crs = sf::st_crs(points)) |&gt;\n    sf::st_sf()\n  \n  perimeter &lt;- polygon_sf |&gt;\n    dplyr::mutate(\n      date = date,\n      n_points = nrow(daily_points),\n      mean_frp = mean(daily_points$frp),\n      area_km2 = as.numeric(units::set_units(sf::st_area(polygon_sf), \"km^2\"))\n    )\n  \n  return(perimeter)\n}\n\n\n\n\n\n\n\nUnderstanding Concave Hulls\n\n\n\nA concave hull (also known as a concave closure or α-shape) provides a more realistic boundary around a set of points compared to a convex hull. While a convex hull creates the smallest convex polygon that contains all points, a concave hull can “wrap” more tightly around the points, capturing the true shape of the point distribution. This is particularly useful for fire perimeter mapping, where we want to represent the actual extent of fire activity rather than just the outer bounds.\n\n\nNext, let’s create daily perimeters for our analysis period:\n\n# Create daily perimeters\nunique_dates &lt;- unique(firms_points$date)\nfire_perimeters &lt;-\n  do.call(rbind,\n                lapply(unique_dates, function(d)\n                  create_fire_perimeters(firms_points, d, 1, 0.01)))\n\n# Find dates of maximum area and point counts\nmax_stats &lt;- list(\n  area_date = fire_perimeters |&gt;\n    dplyr::arrange(dplyr::desc(area_km2)) |&gt;\n    dplyr::slice(1) |&gt;\n    dplyr::pull(date),\n  points_date = fire_perimeters |&gt;\n    dplyr::arrange(dplyr::desc(n_points)) |&gt;\n    dplyr::slice(1) |&gt;\n    dplyr::pull(date)\n)\n\nHowever, single perimeters can sometimes oversimplify complex fire situations. To better represent distinct fire complexes, we’ll create a clustering function that can identify separate fire groups:\n\n  # eps (epsilon) is the maximum distance between two points for them to be considered neighbors\n  # eps = 5000 means points within 5km of each other are considered part of the same cluster\n  # This value was chosen based on:\n  # - Typical fire spread distances in boreal forests\n  # - Spatial resolution of VIIRS fire detections (375m)\n  # - Desire to identify distinct fire complexes while avoiding over-segmentation\n\ncreate_clustered_perimeters &lt;- function(points, \n                                        date,\n                                        eps = 5000) {  \n\n  # Filter points for the given date\n  daily_points &lt;- points |&gt;\n    dplyr::filter(date == !!date)\n  \n  if (nrow(daily_points) &lt; 3) {\n    return(NULL)\n  }\n  \n  # Get coordinates for clustering\n  coords &lt;- sf::st_coordinates(daily_points)\n  \n  # Perform DBSCAN clustering\n  clusters &lt;- dbscan::dbscan(coords, eps = eps, minPts = 3)\n  \n  # Add cluster information to points\n  daily_points$cluster &lt;- clusters$cluster\n  \n  # Create separate hull for each cluster\n  hulls &lt;- daily_points |&gt;\n    dplyr::filter(cluster &gt; 0) |&gt;  # Remove noise points (cluster = 0)\n    dplyr::group_by(cluster) |&gt;\n    dplyr::group_map(function(cluster_points, cluster_id) {\n      if (nrow(cluster_points) &lt; 3) {\n        return(NULL)\n      }\n      \n      coords &lt;- sf::st_coordinates(cluster_points)\n      hull &lt;- concaveman::concaveman(coords, \n                                   concavity = 1,\n                                   length_threshold = 1)\n      \n      current_poly &lt;- sf::st_polygon(list(hull)) |&gt;\n        sf::st_sfc(crs = sf::st_crs(points)) |&gt;\n        sf::st_sf()\n      \n      current_area &lt;- as.numeric(\n        units::set_units(sf::st_area(current_poly), \"km^2\")\n      )\n      \n      current_poly |&gt;\n        dplyr::mutate(\n          date = date,\n          cluster = cluster_id$cluster,\n          n_points = nrow(cluster_points),\n          mean_frp = mean(cluster_points$frp),\n          area_km2 = current_area\n        )\n    })\n  \n  # Combine all hulls for this date\n  hulls_combined &lt;- do.call(rbind, hulls)\n  \n  return(hulls_combined)\n}\n\n\n\n\n\n\n\nUnderstanding DBSCAN Clustering\n\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is particularly well-suited for identifying fire complexes because it:\n\nCan find clusters of arbitrary shape\nAutomatically determines the number of clusters\nCan identify outliers as noise\nUses two intuitive parameters:\n\neps: The maximum distance between two points to be considered neighbors\nminPts: The minimum number of points to form a cluster\n\n\n\n\nNow we can create clustered perimeters for all dates and visualize the progression:\n\n# Create clustered perimeters for all dates\nall_clustered_perimeters &lt;- do.call(rbind,\n  lapply(unique_dates, function(d) {\n    perims &lt;- create_clustered_perimeters(firms_points, d)\n    if (!is.null(perims)) {\n      perims$date &lt;- d\n    }\n    return(perims)\n  })\n)\n\n# Create progression plot with layered hulls\np4_progression &lt;- ggplot2::ggplot() +\n  tidyterra::geom_spatraster_rgb(data = basemap_tiles) +\n  ggplot2::geom_sf(\n    data = search_area,\n    fill = NA,\n    color = \"#00BFFF\",\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_sf(\n    data = yk_boundary,\n    fill = NA,\n    color = \"#00BFFF\",\n    linewidth = 1.2\n  ) +\n  # Add hulls in reverse chronological order\n  ggplot2::geom_sf(\n    data = all_clustered_perimeters |&gt;\n      dplyr::arrange(dplyr::desc(date)),\n    ggplot2::aes(fill = date),\n    alpha = 0.4,\n    linewidth = 0.8\n  ) +\n  ggplot2::scale_fill_viridis_c(\n    name = \"Date\",\n    option = \"magma\",\n    labels = function(x) format(as.Date(x), \"%B %d\"),\n    breaks = unique_dates[seq(1, length(unique_dates), by = 2)]  # Show every other date\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Fire Complex Progression\",\n    subtitle = paste(format(min(unique_dates), \"%B %d\"), \n                    \"to\", \n                    format(max(unique_dates), \"%B %d\")),\n    caption = \"Note: Clustered using DBSCAN algorithm (eps = 3km)\\nEarlier dates displayed on top\"\n  )\n\nprint(p4_progression)\n\n\n\n\n\n\n\n\nThis is a bit messy, so let’s also examine the fire complexes on days of peak activity by comparing the maximum area date with the maximum detection count date:\n\n# Create dataset for faceted plot\ncomparison_perimeters &lt;- fire_perimeters |&gt;\n  dplyr::filter(date %in% c(max_stats$area_date, max_stats$points_date)) |&gt;\n  dplyr::mutate(\n    type = dplyr::case_when(\n      date == max_stats$area_date ~ \"Maximum Hull Area\",\n      date == max_stats$points_date ~ \"Maximum Detection Count\"\n    )\n  )\n\n# Get corresponding VIIRS points for these dates\ncomparison_points &lt;- firms_points |&gt;\n  dplyr::filter(date %in% c(max_stats$area_date, max_stats$points_date)) |&gt;\n  dplyr::mutate(\n    type = dplyr::case_when(\n      date == max_stats$area_date ~ \"Maximum Hull Area\",\n      date == max_stats$points_date ~ \"Maximum Detection Count\"\n    )\n  )\n\n# Get clustered perimeters for comparison dates\ncomparison_clusters &lt;- rbind(\n  create_clustered_perimeters(firms_points, max_stats$area_date) |&gt;\n    dplyr::mutate(type = \"Maximum Hull Area\"),\n  create_clustered_perimeters(firms_points, max_stats$points_date) |&gt;\n    dplyr::mutate(type = \"Maximum Detection Count\")\n)\n\nUpdate the labels to include dates for better context:\n\n# Modify the facet labels to include dates\ncomparison_clusters &lt;- comparison_clusters |&gt;\n  dplyr::mutate(\n    type = dplyr::case_when(\n      type == \"Maximum Hull Area\" ~\n        paste(\"Maximum Hull Area -\", format(max_stats$area_date, \"%B %d\")),\n      type == \"Maximum Detection Count\" ~\n        paste(\n          \"Maximum Detection Count -\",\n          format(max_stats$points_date, \"%B %d\")\n        )\n    )\n  )\n\n# Update points labels to match\ncomparison_points &lt;- comparison_points |&gt;\n  dplyr::mutate(\n    type = dplyr::case_when(\n      type == \"Maximum Hull Area\" ~\n        paste(\"Maximum Hull Area -\", format(max_stats$area_date, \"%B %d\")),\n      type == \"Maximum Detection Count\" ~\n        paste(\n          \"Maximum Detection Count -\",\n          format(max_stats$points_date, \"%B %d\")\n        )\n    )\n  )\n\nFinally, create a comparative visualization:\n\n# Create faceted comparison plot with clustered hulls\np4c &lt;- ggplot2::ggplot() +\n  # Base layers\n  tidyterra::geom_spatraster_rgb(data = basemap_tiles) +\n  ggplot2::geom_sf(\n    data = search_area,\n    fill = NA,\n    color = \"#00BFFF\",\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_sf(\n    data = yk_boundary,\n    fill = NA,\n    color = \"#00BFFF\",\n    linewidth = 1.2\n  ) +\n  # Add clustered hull polygons\n  ggplot2::geom_sf(\n    data = comparison_clusters,\n    ggplot2::aes(fill = factor(cluster)),\n    alpha = 0.60,\n    linewidth = 1.2,\n    show.legend = FALSE\n  ) +\n  # Add VIIRS points\n  ggplot2::geom_sf(\n    data = comparison_points,\n    color = \"darkred\",\n    size = 1,\n    alpha = 0.25\n  ) +\n  # Facet by type\n  ggplot2::facet_wrap( ~ type) +\n  # Style\n  ggplot2::scale_color_viridis_c(name = \"Fire Radiative\\nPower (MW)\", \n                                option = \"inferno\") +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Comparison of Fire Complexes\",\n    caption = \"Note: Points clustered using DBSCAN algorithm (eps = 5km)\"\n  )\n\nprint(p4c)\n\n\n\n\n\n\n\n# Print statistics about clusters\ncat(\"\\nCluster Statistics:\\n\")\n\n\nCluster Statistics:\n\nfor (date_type in unique(comparison_clusters$type)) {\n  clusters_date &lt;- comparison_clusters |&gt;\n    dplyr::filter(type == date_type)\n  \n  cat(\"\\n\", date_type, \":\\n\")\n  cat(\"Number of distinct fire complexes:\",\n            length(unique(clusters_date$cluster)),\n            \"\\n\")\n  cat(\"Total area across all complexes:\",\n            round(sum(clusters_date$area_km2), 2),\n            \"km²\\n\")\n  cat(\"Largest single complex:\",\n            round(max(clusters_date$area_km2), 2),\n            \"km²\\n\")\n}\n\n\n Maximum Hull Area - August 10 :\nNumber of distinct fire complexes: 9 \nTotal area across all complexes: 374.64 km²\nLargest single complex: 241.83 km²\n\n Maximum Detection Count - August 15 :\nNumber of distinct fire complexes: 9 \nTotal area across all complexes: 356.48 km²\nLargest single complex: 215.1 km²\n\n\n\n\nInterpreting Fire Complex Analysis\nWhen analyzing fire complexes, several key metrics help us understand the fire situation:\n\nNumber of Distinct Complexes: Multiple fire complexes can indicate either natural fire spread patterns or multiple ignition sources.\nComplex Size: The size of individual complexes helps fire managers assess resource needs and potential threats.\nComplex Distribution: The spatial arrangement of complexes relative to infrastructure and communities informs evacuation planning.\nTemporal Progression: Understanding how complexes grow and merge over time helps predict future fire behavior.\n\n\n\n\n\n\n\nKnowledge Check: FIRMS\n\n\n\nWhat is the primary advantage of FIRMS data for wildfire monitoring?\n\nIt provides weather forecasts for fire-prone areas\nIt delivers near real-time fire detections within 3 hours of satellite observation\nIt measures ground-level temperature\nIt predicts future fire spread\n\n\n\n\n\n\nVIIRS Burned Area\nThe VIIRS Burned Area product (VNP64A1) provides a comprehensive assessment of burned areas by detecting changes in surface reflectance characteristics that occur after fire. Unlike FIRMS data which captures active burning, VNP64A1 identifies the full extent of burned areas at 500m resolution, including areas that may have burned without being detected as active fires due to cloud cover or timing of satellite overpasses. The product assigns each pixel a “burn date” indicating when the burn was first detected, allowing for temporal analysis of fire progression. This approach is particularly valuable for post-fire assessment and validation of active fire detections, though it has limitations in near real-time monitoring due to its processing latency. For the Yellowknife fires, VNP64A1 data helps us understand the total area impacted and verify the progression patterns suggested by FIRMS detections.\nThe VNP64A1 data used in this analysis was acquired through NASA’s Earthdata Cloud Search Portal (earthdata.nasa.gov/search), which allows users to search, preview, and download specific granules based on location and date range. After identifying granules covering the Yellowknife region during August 2023, the data was downloaded in HDF5 format for processing into a folder called “data”.\nFirst, let’s locate and read our VIIRS burned area files. We’ll focus on files from August 1 (day 213) to September 1 (day 244):\n\n# Get a list of VIIRS Burned Area files\nviirs_files &lt;- list.files(\n  \"data/VNP64A1.002/\",\n  pattern = \"A2023(213|244).*\\\\.hdf$\",\n  full.names = TRUE\n)\n\n# Print the files we'll be working with\nprint(\"VIIRS Burned Area files to process:\")\n\n[1] \"VIIRS Burned Area files to process:\"\n\nprint(viirs_files)\n\n[1] \"data/VNP64A1.002/VNP64A1.A2023213.h12v02.002.2023276103300.hdf\"\n[2] \"data/VNP64A1.002/VNP64A1.A2023244.h12v02.002.2023312150240.hdf\"\n\n\n\n\n\n\n\n\nUnderstanding VIIRS Burned Area Data\n\n\n\nThe VNP64A1 product contains several data layers, but the primary “Burn Date” layer records the day of year when a burn was detected. Special values include: - 0: Unburned - -1: Unmapped - -2: Water\n\n\nNow we’ll process each HDF file, extracting the burn date information:\n\n# Create empty list to store processed burn date layers\nburn_dates_list &lt;- list()\n\n# Process each HDF file\nfor (i in seq_along(viirs_files)) {\n  # Read the HDF file using terra\n  burn_rast &lt;- terra::rast(viirs_files[i])\n  \n  # Print layer names for first file to verify structure\n  if (i == 1) {\n    print(\"\\nLayer names in the HDF file:\")\n    print(names(burn_rast))\n  }\n  \n  # Extract burn date layer (first layer)\n  burn_date &lt;- burn_rast[[1]]  # Select \"Burn Date\" layer\n  \n  # Process burn dates\n  # Convert to more meaningful values and handle special codes\n  values &lt;- terra::values(burn_date)\n  values[values &lt;= 0] &lt;- NA\n  terra::values(burn_date) &lt;- values\n  \n  # Store processed layer\n  burn_dates_list[[i]] &lt;- burn_date\n  \n  print(paste(\"Processed file\", i, \"of\", length(viirs_files)))\n}\n\n[1] \"\\nLayer names in the HDF file:\"\n[1] \"\\\"Burn Date\\\"\"             \"\\\"Burn Date Uncertainty\\\"\"\n[3] \"QA\"                        \"\\\"First Day\\\"\"            \n[5] \"\\\"Last Day\\\"\"             \n[1] \"Processed file 1 of 2\"\n[1] \"Processed file 2 of 2\"\n\n\nNext, we’ll combine our processed tiles into a single mosaic and align it with our study area:\n\n# Mosaic all tiles together in one step\nburn_mosaic &lt;- terra::mosaic(burn_dates_list[[1]], \n                             burn_dates_list[[2]], \n                            #  burn_dates_list[[3]], \n                            #  burn_dates_list[[4]], \n                             fun = \"last\")  # Use most recent valid observation\n\n# Reproject to match our other data (EPSG:4326)\nburn_mosaic_4326 &lt;- terra::project(burn_mosaic, \"EPSG:4326\")\n\n# Add a buffer to capture the full extent of the fire\nstudy_area_buffer &lt;- sf::st_buffer(yk_boundary, 50000)\n\n# Convert study area buffer to SpatVector for terra operations\nstudy_area_vect &lt;- terra::vect(study_area_buffer)\n\n# Project study area to match burn mosaic's original CRS (sinusoidal)\nstudy_area_proj &lt;- terra::project(study_area_vect, terra::crs(burn_mosaic))\n\n# Crop and mask the burn data\nburn_clipped &lt;- terra::crop(burn_mosaic, study_area_proj)\nburn_clipped &lt;- terra::mask(burn_clipped, study_area_proj)\n\n# Now project everything to EPSG:4326 for visualization\nburn_clipped_4326 &lt;- terra::project(burn_clipped, \"EPSG:4326\")\nstudy_area_4326 &lt;- terra::project(study_area_vect, \"EPSG:4326\")\nyk_boundary_4326 &lt;- terra::project(terra::vect(yk_boundary), \"EPSG:4326\")\n\nTo prepare for visualization, we’ll convert our data to a format suitable for ggplot and create meaningful time periods:\n\n# Convert raster to data frame for plotting\nburn_df &lt;- terra::as.data.frame(burn_clipped_4326, xy = TRUE)\nnames(burn_df)[3] &lt;- \"burn_date\"\n\n# Convert burn dates to actual dates\nburn_df$date &lt;- as.Date(\"2023-01-01\") + burn_df$burn_date - 1\n\n# Create categories focusing on August\nburn_df$period &lt;- cut(burn_df$date,\n                           breaks = as.Date(c(\"2023-08-01\", \"2023-08-08\", \n                                                  \"2023-08-15\", \"2023-08-22\",\n                                                  \"2023-09-01\")),\n                           labels = c(\"Aug 1-7\", \"Aug 8-14\", \n                                    \"Aug 15-21\", \"Aug 22-31\"),\n                           include.lowest = TRUE\n)\n\nNow we can create our first visualization showing the progression of burned areas:\n\n# Create the visualization with discrete categories\np1 &lt;- ggplot2::ggplot() +\n  ggplot2::geom_tile(data = burn_df, \n                     ggplot2::aes(x = x, y = y, fill = period)) +\n  ggplot2::geom_sf(data = study_area_4326,\n                   fill = NA,\n                   color = \"gray60\",\n                   linetype = \"dashed\") +\n  ggplot2::geom_sf(data = yk_boundary_4326,\n                   fill = NA,\n                   color = \"#00BFFF\",\n                   linewidth = 1) +\n  # Using a sequential red color scheme to show progression\n  ggplot2::scale_fill_manual(\n    values = c(\"Aug 1-7\" = \"#FEE5D9\",\n               \"Aug 8-14\" = \"#FCAE91\", \n               \"Aug 15-21\" = \"#FB6A4A\",  # Evacuation period\n               \"Aug 22-31\" = \"#CB181D\"),\n    na.value = \"transparent\",\n    name = \"Burn Period\",\n    drop = FALSE\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Burn Progression Around Yellowknife\",\n    subtitle = paste(\"August 2023\\n\",\n                    \"(Evacuation ordered August 16th)\"),\n    caption = \"Data: VIIRS VNP64A1 Burned Area Product\"\n  )\n\nprint(p1)\n\n\n\n\n\n\n\n# Print summary of area burned in each period\nprint(\"\\nArea burned by period (km²):\")\n\n[1] \"\\nArea burned by period (km²):\"\n\nburn_summary &lt;- table(burn_df$period) * 0.25  # Convert pixel count to km²\nprint(burn_summary)\n\n\n  Aug 1-7  Aug 8-14 Aug 15-21 Aug 22-31 \n    21.25    119.50     30.75      3.50 \n\n\nFor validation and comparison, let’s create a visualization that overlays our FIRMS active fire detections on the burn areas:\n\n# Create comparison with FIRMS data\nfirms_points_4326 &lt;- terra::project(terra::vect(firms_points), \"EPSG:4326\")\n\np2 &lt;- ggplot2::ggplot() +\n  ggplot2::geom_tile(data = burn_df, \n                     ggplot2::aes(x = x, y = y, fill = period)) +\n  ggplot2::geom_sf(data = study_area_4326, \n                   fill = NA, \n                   color = \"gray60\", \n                   linetype = \"dashed\") +\n  ggplot2::geom_sf(data = yk_boundary_4326, \n                   fill = NA, \n                   color = \"#00BFFF\", \n                   linewidth = 1) +\n  ggplot2::geom_sf(data = firms_points, \n                   ggplot2::aes(color = frp), \n                   alpha = 0.6, \n                   size = 0.8) +\n  ggplot2::scale_fill_manual(\n    values = c(\"Aug 1-7\" = \"#FEE5D9\",\n               \"Aug 8-14\" = \"#FCAE91\", \n               \"Aug 15-21\" = \"#FB6A4A\",\n               \"Aug 22-31\" = \"#CB181D\"),\n    na.value = \"transparent\",\n    name = \"Burn Period\",\n    drop = FALSE\n  ) +\n  ggplot2::scale_color_viridis_c(\n    name = \"Fire Radiative\\nPower (MW)\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Burn Scars and Active Fire Detections\",\n    subtitle = \"VIIRS Burned Area with FIRMS Hotspots\",\n    caption = \"Data: VNP64A1 and FIRMS VIIRS\"\n  )\n\nprint(p2)\n\n\n\n\n\n\n\n\n\nInterpreting Burned Area Analysis\nWhen analyzing burned areas, several key considerations help validate and interpret the results:\n\nTemporal Progression: The timing of detected burns should generally align with active fire detections\nSpatial Agreement: Burn scars should correspond with areas where we detected active fires\nResolution Effects: The 500m resolution of the burned area product means small fires might be missed\nEdge Effects: Areas near the edge of satellite swaths or scene boundaries may show artificial patterns\n\nThe combination of active fire detections (FIRMS) and burned area products (VNP64A1) provides a more complete picture of fire impacts than either product alone.\n\n\n\n\n\n\nKnowledge Check: VIIRS Burned Area\n\n\n\nWhy is the VIIRS Burned Area product (VNP64A1) complementary to FIRMS data?\n\nIt provides higher temporal resolution\nIt measures ground temperature\nIt detects changes in surface reflectance to identify burned areas, even those that may have been missed by active fire detection\nIt predicts where fires will occur\n\n\n\n\n\n\n\nGOES-18 Smoke Plume Analysis\nTracking smoke plume extent and movement is crucial for public health and emergency management during wildfires. While several satellite products can detect smoke, aerosol optical depth (AOD) measurements from geostationary satellites provide particularly valuable data for monitoring smoke plumes due to their high temporal frequency. AOD quantifies the degree to which aerosols (including smoke particles) prevent the transmission of light through the atmosphere, serving as a proxy for smoke density and air quality impacts.\nThe GOES-18 ABI (Advanced Baseline Imager) Level 2 Aerosol Optical Depth (AOD) product provides near real-time monitoring of aerosol loading across North America at 15-minute intervals. This high temporal resolution allows for detailed tracking of smoke plume development and transport, though at a coarser spatial resolution than some polar-orbiting satellites. The product provides AOD values ranging from 0 (clear sky) to 5 (very thick aerosol), with values above 1 typically indicating significant smoke impacts. During the Yellowknife evacuation, this data helped emergency managers assess potential impacts on communities and transportation routes as smoke complicated both ground and air evacuation efforts.\nI’ll help break down the “Visualizing a Snapshot” code section into meaningful chunks with clear narrative:\n\nVisualizing a Snapshot\nLet’s begin by selecting a timestamp during peak fire activity to examine smoke conditions. We’ll focus on August 15th, the day before the evacuation order:\n\n# Pick a timestamp during peak fire activity\ntest_date &lt;- as.Date(\"2023-08-15\")  # Day before evacuation\ntest_hour &lt;- 18  # Mid-afternoon local time\n\nThe GOES-18 data is stored in Amazon Web Services (AWS) S3 buckets, organized by product, year, day of year, and hour. We’ll construct the appropriate path and retrieve the file list:\n\n# Get GOES file list\ngoes_url &lt;- sprintf(\n  \"%s/%s/%s/%02d\",\n  \"ABI-L2-AODF\",\n  format(test_date, \"%Y\"),\n  format(test_date, \"%j\"),\n  test_hour\n)\n\nfiles_df &lt;- aws.s3::get_bucket_df(\n  bucket = \"noaa-goes18\",\n  prefix = goes_url,\n  region = \"us-east-1\",\n  max = 20\n)\n\n\n\n\n\n\n\nUnderstanding GOES Data Organization\n\n\n\nGOES satellite data follows a consistent naming convention: - ABI-L2-AODF: Product name (Aerosol Optical Depth Full Disk) - Year/DayOfYear/Hour: Temporal organization - Multiple files per hour represent different scan times\n\n\nNow we’ll download and read the most recent file for our selected timestamp:\n\n# Get most recent file\nlatest_file &lt;- files_df |&gt;\n  dplyr::arrange(dplyr::desc(LastModified)) |&gt;\n  dplyr::slice(1)\n\n# Download and read file\nurl &lt;- sprintf(\"https://noaa-goes18.s3.amazonaws.com/%s\", latest_file$Key)\ntemp_file &lt;- tempfile(fileext = \".nc\")\ndownload.file(url, temp_file, mode = \"wb\", quiet = TRUE)\naod_data &lt;- stars::read_stars(temp_file, sub = \"AOD\", quiet = TRUE)\nunlink(temp_file)\n\nTo focus our analysis on the Yellowknife area, we’ll create two buffer zones around the city:\n\n# Create visualization extent (100km buffer for context)\nyk_buffer_big &lt;- sf::st_buffer(yk_boundary, 100000)\nyk_buffer_small &lt;- sf::st_buffer(yk_boundary, 50000)  # 50km for analysis\n\n# Transform to GOES projection\nyk_buffer_goes &lt;- sf::st_transform(yk_buffer_big, sf::st_crs(aod_data))\nyk_small_goes &lt;- sf::st_transform(yk_buffer_small, sf::st_crs(aod_data))\n\n# Crop to our area of interest\naod_local &lt;- sf::st_crop(aod_data, yk_buffer_goes)\n\nFinally, we can create a visualization of the smoke conditions:\n\n# Create visualization\nggplot2::ggplot() +\n  stars::geom_stars(data = aod_local) +\n  ggplot2::geom_sf(data = sf::st_transform(yk_boundary, sf::st_crs(aod_data)),\n          fill = NA, color = \"#00BFFF\", linewidth = 1) +\n  ggplot2::geom_sf(data = yk_small_goes,\n          fill = NA, color = \"blue\", linetype = \"dashed\") +\n  ggplot2::scale_fill_viridis_c(\n    name = \"AOD\",\n    na.value = NA,\n    option = \"inferno\",\n    limits = c(0, 3)\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"GOES-18 AOD Data Around Yellowknife\",\n    subtitle = format(test_date, \"%B %d, %Y %H:00 UTC\"),\n    caption = \"Red: City boundary\\nBlue dashed: 50km analysis buffer\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting AOD Values\n\n\n\nAerosol Optical Depth values can be roughly interpreted as: - 0.0-0.1: Clear sky - 0.1-0.3: Light haze - 0.3-0.5: Moderate haze - 0.5-1.0: Heavy haze - &gt;1.0: Very heavy smoke/haze\nHigher values indicate more particles in the atmospheric column, suggesting thicker smoke plumes and potentially worse air quality.\n\n\n\n\nTemporal Analysis of AOD Values\nTo understand how smoke conditions evolved during the evacuation period, we’ll analyze AOD values over several days. First, let’s create some helper functions to streamline our data processing:\n\n# Functions for GOES data handling\ngoes_date_format &lt;- function(date) {\n  list(\n    year = format(date, \"%Y\"),\n    doy = format(date, \"%j\")\n  )\n}\n\n# Function to construct https URL from Key\nconstruct_url &lt;- function(key, bucket = \"noaa-goes18\") {\n  sprintf(\"https://%s.s3.amazonaws.com/%s\", bucket, key)\n}\n\n# Function to get GOES files\nget_goes_files &lt;- function(date, hour, product = \"ABI-L2-AODF\", satellite = \"goes18\") {\n  # Get date components\n  date_parts &lt;- goes_date_format(date)\n  \n  # Construct prefix\n  prefix &lt;- sprintf(\n    \"%s/%s/%s/%02d/\",\n    product,\n    date_parts$year,\n    date_parts$doy,\n    hour\n  )\n  \n  message(\"Searching prefix: \", prefix, \" in \", satellite)\n  \n  # Get file list from bucket\n  files_df &lt;- aws.s3::get_bucket_df(\n    bucket = paste0(\"noaa-\", satellite),\n    prefix = prefix,\n    region = \"us-east-1\",\n    max = 20\n  )\n  \n  if (nrow(files_df) &gt; 0) {\n    files_df$LastModified &lt;- as.POSIXct(\n      files_df$LastModified,\n      format = \"%Y-%m-%dT%H:%M:%S.000Z\",\n      tz = \"UTC\"\n    )\n    files_df$satellite &lt;- satellite\n  }\n  \n  return(files_df)\n}\n\nNow we’ll create a function to process AOD data for specific timestamps:\n\n# Function to get and process AOD data for a specific timestamp\nget_aod_data &lt;- function(date, hour) {\n  # Get file list\n  files_df &lt;- get_goes_files(\n    date = date,\n    hour = hour,\n    product = \"ABI-L2-AODF\",\n    satellite = \"goes18\"\n  )\n  \n  if (nrow(files_df) == 0) {\n    message(\"No files found for: \", date, \" \", hour, \":00 UTC\")\n    return(NULL)\n  }\n  \n  # Get most recent file\n  latest_file &lt;- files_df |&gt;\n    dplyr::arrange(dplyr::desc(LastModified)) |&gt;\n    dplyr::slice(1)\n  \n  # Download and read file\n  url &lt;- construct_url(latest_file$Key, bucket = \"noaa-goes18\")\n  temp_file &lt;- tempfile(fileext = \".nc\")\n  \n  utils::download.file(\n    url = url,\n    destfile = temp_file,\n    mode = \"wb\",\n    quiet = TRUE\n  )\n  \n  # Read AOD data\n  aod_data &lt;- stars::read_stars(\n    temp_file,\n    sub = \"AOD\",\n    quiet = TRUE\n  )\n  \n  # Clean up\n  unlink(temp_file)\n  \n  return(aod_data)\n}\n\n\n\n\n\n\n\nGOES Data Processing Tips\n\n\n\nThe GOES AWS bucket provides the most recent 7 days of data at no cost. When downloading multiple files: - Use appropriate time intervals to avoid redundant data - Consider file size limitations and download times - Remember that UTC timestamps are used for data organization\n\n\nWe’ll also need a function to calculate statistics for our area of interest:\n\n# Modify calculate_local_stats function to strip units\ncalculate_local_stats &lt;- function(aod_data, local_area) {\n  # Transform and crop to local area\n  tryCatch({\n    local_goes &lt;- sf::st_transform(local_area, sf::st_crs(aod_data))\n    aod_local &lt;- sf::st_crop(aod_data, local_goes)\n    \n    # Check if we have any valid data\n    if (sum(!is.na(aod_local[[1]])) &gt; 0) {\n      # Convert to numeric to strip units\n      # GOES AOD values come with units attached (dimensionless optical depth units)\n      # We strip these for easier calculation and compatibility with other functions\n      # Raw AOD values typically range from 0-5, where:\n      # 0-0.1: Clear sky\n      # 0.1-0.3: Light haze\n      # &gt;1.0: Heavy smoke\n      values &lt;- as.numeric(aod_local[[1]])\n      # Convert to numeric to strip units\n      values &lt;- as.numeric(aod_local[[1]])\n      \n      stats &lt;- list(\n        mean_aod = mean(values, na.rm = TRUE),\n        max_aod = max(values, na.rm = TRUE),\n        valid_pixels = sum(!is.na(values)),\n        total_pixels = length(values)\n      )\n    } else {\n      stats &lt;- list(\n        mean_aod = NA_real_,\n        max_aod = NA_real_,\n        valid_pixels = 0L,\n        total_pixels = length(aod_local[[1]])\n      )\n    }\n    return(stats)\n  }, error = function(e) {\n    message(\"Error in processing: \", e$message)\n    return(list(\n      mean_aod = NA_real_,\n      max_aod = NA_real_,\n      valid_pixels = 0L,\n      total_pixels = 0L\n    ))\n  })\n}\n\nNow we can define our analysis period and process multiple timestamps:\n\n# Define analysis period (5 days before to 3 days after evacuation)\nanalysis_dates &lt;- seq(\n  as.Date(\"2023-08-11\"),  # 5 days before evacuation\n  as.Date(\"2023-08-19\"),  # 3 days after evacuation\n  by = \"day\"\n)\n\n# Define daily hours to analyze (daylight hours in UTC)\nanalysis_hours &lt;- c(16, 18, 20, 22)  # Approximately 10am-4pm local time\n\n# Create analysis timestamp combinations\nanalysis_times &lt;- expand.grid(\n  date = analysis_dates,\n  hour = analysis_hours\n)\n\n# Process data for all analysis times\naod_results &lt;- lapply(1:nrow(analysis_times), function(i) {\n  date &lt;- analysis_times$date[i]\n  hour &lt;- analysis_times$hour[i]\n  \n  message(\"\\nProcessing: \", date, \" \", hour, \":00 UTC\")\n  \n  # Get AOD data\n  aod_data &lt;- get_aod_data(date, hour)\n  \n  if (is.null(aod_data)) {\n    return(list(\n      datetime = as.POSIXct(\n        paste(date, sprintf(\"%02d:00:00\", hour)),\n        tz = \"UTC\"\n      ),\n      mean_aod = NA_real_,\n      max_aod = NA_real_,\n      valid_pixels = 0L,\n      total_pixels = 0L\n    ))\n  }\n  \n  # Calculate statistics\n  stats &lt;- calculate_local_stats(aod_data, yk_buffer_small)\n  \n  # Add timestamp\n  stats$datetime &lt;- as.POSIXct(\n    paste(date, sprintf(\"%02d:00:00\", hour)),\n    tz = \"UTC\"\n  )\n  \n  return(stats)\n})\n\n# Convert to data frame and handle NaN/Inf values\naod_df &lt;- dplyr::bind_rows(aod_results) |&gt;\n  dplyr::mutate(\n    mean_aod = replace(mean_aod, !is.finite(mean_aod), NA),\n    max_aod = replace(max_aod, !is.finite(max_aod), NA)\n  )\n\nFor context, let’s provide a reference table for interpreting AOD values in terms of air quality provided by the World Health Organization:\n\n# Create WHO threshold reference table\nwho_thresholds &lt;- data.frame(\n  Category = c(\"Good\", \"Moderate\", \"Unhealthy for Sensitive Groups\", \n               \"Unhealthy\", \"Very Unhealthy\", \"Hazardous\"),\n  `PM2.5 (µg/m³)` = c(\"0-12\", \"12.1-35.4\", \"35.5-55.4\", \n                      \"55.5-150.4\", \"150.5-250.4\", \"&gt;250.5\"),\n  `Approximate AOD` = c(\"&lt;0.1\", \"0.1-0.3\", \"0.3-0.5\", \n                       \"0.5-1.0\", \"1.0-2.0\", \"&gt;2.0\"),\n  check.names = FALSE\n)\n\nNow we can create visualizations to show how smoke conditions evolved. First, a time series of AOD values:\n\n# Create time series visualization with WHO thresholds\np1 &lt;- ggplot2::ggplot() +\n  # Add WHO threshold reference lines\n  ggplot2::geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"yellow\", alpha = 0.5) +\n  ggplot2::geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"orange\", alpha = 0.5) +\n  ggplot2::geom_hline(yintercept = 2.0, linetype = \"dashed\", color = \"red\", alpha = 0.5) +\n  # Add evacuation reference line\n  ggplot2::geom_vline(\n    xintercept = as.POSIXct(\"2023-08-16 00:00:00\", tz = \"UTC\"),\n    color = \"red\",\n    linetype = \"dashed\",\n    alpha = 0.5\n  ) +\n  # Add AOD lines\n  ggplot2::geom_line(data = aod_df, \n                     ggplot2::aes(x = datetime, y = mean_aod, color = \"Mean AOD\")) +\n  ggplot2::geom_point(data = aod_df,\n                      ggplot2::aes(x = datetime, y = mean_aod, color = \"Mean AOD\")) +\n  ggplot2::geom_line(data = aod_df,\n                     ggplot2::aes(x = datetime, y = max_aod, color = \"Max AOD\")) +\n  ggplot2::geom_point(data = aod_df,\n                      ggplot2::aes(x = datetime, y = max_aod, color = \"Max AOD\")) +\n  # Customize appearance\n  ggplot2::scale_color_manual(\n    name = \"Metric\",\n    values = c(\"Mean AOD\" = \"blue\", \"Max AOD\" = \"red\")\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Aerosol Optical Depth Around Yellowknife\",\n    subtitle = \"50km radius, GOES-18 observations\",\n    x = \"Date/Time (UTC)\",\n    y = \"Aerosol Optical Depth\",\n    caption = \"Red dashed vertical line: Evacuation date\\nHorizontal lines: WHO thresholds (yellow=unhealthy, orange=very unhealthy, red=hazardous)\"\n  )\n\nprint(p1)\n\n\n\n\n\n\n\n\nFinally, let’s summarize our findings:\n\n# Create summary statistics table\nvalid_coverage &lt;- aod_df |&gt;\n  dplyr::filter(total_pixels &gt; 0) |&gt;\n  dplyr::summarise(\n    coverage = mean(valid_pixels/total_pixels * 100, na.rm = TRUE)\n  ) |&gt;\n  dplyr::pull(coverage)\n\nstats_table &lt;- data.frame(\n  Metric = c(\"Analysis Period\", \"Mean AOD\", \"Maximum AOD\", \"Average Valid Coverage\"),\n  Value = c(\n    paste(format(min(aod_df$datetime), \"%B %d %H:%M UTC\"), \"to\",\n          format(max(aod_df$datetime), \"%B %d %H:%M UTC\")),\n    sprintf(\"%.3f\", mean(aod_df$mean_aod, na.rm = TRUE)),\n    sprintf(\"%.3f\", max(aod_df$max_aod, na.rm = TRUE)),\n    sprintf(\"%.1f%%\", valid_coverage)\n  )\n)\n\n# Display tables\nknitr::kable(stats_table,\n             caption = \"AOD Analysis Summary Statistics\")\n\n\nAOD Analysis Summary Statistics\n\n\nMetric\nValue\n\n\n\n\nAnalysis Period\nAugust 11 16:00 UTC to August 19 22:00 UTC\n\n\nMean AOD\n0.751\n\n\nMaximum AOD\n4.997\n\n\nAverage Valid Coverage\n17.0%\n\n\n\n\nknitr::kable(who_thresholds,\n             caption = \"WHO Air Quality Categories and Approximate AOD Values\")\n\n\nWHO Air Quality Categories and Approximate AOD Values\n\n\nCategory\nPM2.5 (µg/m³)\nApproximate AOD\n\n\n\n\nGood\n0-12\n&lt;0.1\n\n\nModerate\n12.1-35.4\n0.1-0.3\n\n\nUnhealthy for Sensitive Groups\n35.5-55.4\n0.3-0.5\n\n\nUnhealthy\n55.5-150.4\n0.5-1.0\n\n\nVery Unhealthy\n150.5-250.4\n1.0-2.0\n\n\nHazardous\n&gt;250.5\n&gt;2.0\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Temporal AOD Patterns\n\n\n\nWhen interpreting temporal AOD patterns, consider: 1. Diurnal cycles - smoke often settles overnight and lifts during the day 2. Wind patterns - can cause rapid changes in local smoke conditions 3. Cloud interference - can cause gaps in the data 4. Vertical distribution - AOD measures the entire column, not just surface conditions\n\n\n\n\nVisualizing the Maximum Plume Spread\nTo understand the full spatial extent of smoke impacts, we’ll analyze a larger area around Yellowknife during peak smoke conditions. First, let’s create a function to process plume data for specific timestamps:\n\n# Function to process plume data for a specific timestamp\nanalyze_plume &lt;- function(date, hour, plume_buffer) {\n  # Get GOES file\n  files_df &lt;- get_goes_files(\n    date = date,\n    hour = hour,\n    product = \"ABI-L2-AODF\",\n    satellite = \"goes18\"\n  )\n  \n  if (nrow(files_df) == 0) {\n    message(\"No files found for: \", date, \" \", hour, \":00 UTC\")\n    return(NULL)\n  }\n  \n  # Get most recent file for timestamp\n  latest_file &lt;- files_df |&gt;\n    dplyr::arrange(desc(LastModified)) |&gt;\n    dplyr::slice(1)\n  \n  # Download and read data\n  url &lt;- construct_url(latest_file$Key, bucket = \"noaa-goes18\")\n  temp_file &lt;- tempfile(fileext = \".nc\")\n  utils::download.file(url, temp_file, mode = \"wb\", quiet = TRUE)\n  aod_data &lt;- stars::read_stars(temp_file, sub = \"AOD\", quiet = TRUE)\n  unlink(temp_file)\n  \n  # Transform and crop to plume analysis extent\n  plume_goes &lt;- sf::st_transform(plume_buffer, sf::st_crs(aod_data))\n  plume_aod &lt;- sf::st_crop(aod_data, plume_goes)\n  \n  # Calculate plume statistics\n  values &lt;- as.numeric(plume_aod[[1]])\n  stats &lt;- list(\n    datetime = as.POSIXct(\n      paste(date, sprintf(\"%02d:00:00\", hour)),\n      tz = \"UTC\"\n    ),\n    mean_aod = mean(values, na.rm = TRUE),\n    max_aod = max(values, na.rm = TRUE),\n    heavy_smoke_area = sum(values &gt; 2, na.rm = TRUE) * 0.0081,  # Approx km² per pixel\n    moderate_smoke_area = sum(values &gt; 1, na.rm = TRUE) * 0.0081,\n    data = plume_aod\n  )\n  \n  return(stats)\n}\n\n\n\n\n\n\n\nQuantifying Smoke Extent\n\n\n\nWe classify smoke coverage into two categories: - Moderate smoke: AOD &gt; 1 (significant visibility reduction) - Heavy smoke: AOD &gt; 2 (severe air quality impacts)\nThe area calculation (0.0081 km² per pixel) is based on the GOES-R ABI resolution at nadir.\n\n\nNow let’s analyze a focused period around the evacuation:\n\n# Define analysis period (3 days centered on evacuation)\nanalysis_dates &lt;- seq(\n  as.Date(\"2023-08-15\"),  # Day before evacuation\n  as.Date(\"2023-08-17\"),  # Day after evacuation\n  by = \"day\"\n)\n\n# Define daily hours to analyze (focusing on peak hours)\nanalysis_hours &lt;- c(16, 18, 20, 22)  # ~10am-4pm local time\n\n# Create timestamp combinations\nanalysis_times &lt;- expand.grid(\n  date = analysis_dates,\n  hour = analysis_hours\n)\n\n# Define larger analysis extent (500km radius for plume tracking)\nplume_buffer &lt;- sf::st_buffer(yk_boundary, 500000)  # 500km\n\n# Process all timestamps\nplume_results &lt;- lapply(1:nrow(analysis_times), function(i) {\n  date &lt;- analysis_times$date[i]\n  hour &lt;- analysis_times$hour[i]\n  \n  message(\"\\nProcessing: \", date, \" \", hour, \":00 UTC\")\n  analyze_plume(date, hour, plume_buffer)\n})\n\nLet’s extract statistics and create a time series of plume extent:\n\n# Extract statistics into data frame\nplume_stats &lt;- dplyr::bind_rows(lapply(plume_results, function(x) {\n  if (!is.null(x)) {\n    data.frame(\n      datetime = x$datetime,\n      mean_aod = x$mean_aod,\n      max_aod = x$max_aod,\n      heavy_smoke_area = x$heavy_smoke_area,\n      moderate_smoke_area = x$moderate_smoke_area\n    )\n  }\n}))\n\n# Create time series visualization of plume metrics\np1_plume &lt;- ggplot2::ggplot(plume_stats) +\n  ggplot2::geom_vline(\n    xintercept = as.POSIXct(\"2023-08-16 00:00:00\", tz = \"UTC\"),\n    color = \"red\",\n    linetype = \"dashed\",\n    alpha = 0.5\n  ) +\n  ggplot2::geom_line(ggplot2::aes(x = datetime, y = moderate_smoke_area, \n                                  color = \"Moderate Smoke (AOD &gt; 1)\")) +\n  ggplot2::geom_point(ggplot2::aes(x = datetime, y = moderate_smoke_area, \n                                   color = \"Moderate Smoke (AOD &gt; 1)\")) +\n  ggplot2::geom_line(ggplot2::aes(x = datetime, y = heavy_smoke_area, \n                                  color = \"Heavy Smoke (AOD &gt; 2)\")) +\n  ggplot2::geom_point(ggplot2::aes(x = datetime, y = heavy_smoke_area, \n                                   color = \"Heavy Smoke (AOD &gt; 2)\")) +\n  ggplot2::scale_color_manual(\n    name = \"Smoke Category\",\n    values = c(\"Moderate Smoke (AOD &gt; 1)\" = \"orange\", \n               \"Heavy Smoke (AOD &gt; 2)\" = \"red\")\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Smoke Plume Coverage Around Yellowknife\",\n    subtitle = \"500km radius analysis extent\",\n    x = \"Date/Time (UTC)\",\n    y = \"Affected Area (km²)\",\n    caption = \"Red dashed line indicates evacuation date\"\n  )\n\nprint(p1_plume)\n\n\n\n\n\n\n\n\nNow let’s examine conditions during the peak smoke period:\n\n# Identify peak smoke period\npeak_time &lt;- plume_stats |&gt;\n  dplyr::arrange(desc(moderate_smoke_area)) |&gt;\n  dplyr::slice(1)\n\npeak_data &lt;- plume_results[[which(\n  sapply(plume_results, function(x) {\n    !is.null(x) && x$datetime == peak_time$datetime\n  })\n)]]\n\n# Create peak plume visualization\np2_plume &lt;- ggplot2::ggplot() +\n  stars::geom_stars(data = peak_data$data) +\n  ggplot2::geom_sf(data = sf::st_transform(yk_boundary, \n                                           sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"#00BFFF\", linewidth = 1) +\n  ggplot2::geom_sf(data = sf::st_transform(plume_buffer, \n                                           sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"blue\", linetype = \"dashed\") +\n  ggplot2::scale_fill_viridis_c(\n    name = \"AOD\",\n    na.value = NA,\n    option = \"inferno\",\n    limits = c(0, 3)\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Peak Smoke Plume Extent\",\n    subtitle = format(peak_data$datetime, \"%B %d, %Y %H:%M UTC\"),\n    caption = paste(\n      \"Red: City boundary\\nBlue dashed: 500km analysis extent\\n\",\n      \"Affected area (AOD &gt; 1):\", \n      round(peak_time$moderate_smoke_area), \"km²\"\n    )\n  )\n\nprint(p2_plume)\n\n\n\n\n\n\n\n# Create summary statistics table\nstats_summary &lt;- data.frame(\n  Metric = c(\n    \"Analysis Period\",\n    \"Peak AOD Value\",\n    \"Maximum Affected Area (AOD &gt; 1)\",\n    \"Maximum Heavy Smoke Area (AOD &gt; 2)\",\n    \"Average Affected Area (AOD &gt; 1)\",\n    \"Peak Observation Time\"\n  ),\n  Value = c(\n    paste(format(min(plume_stats$datetime), \"%B %d %H:%M UTC\"), \"to\",\n          format(max(plume_stats$datetime), \"%B %d %H:%M UTC\")),\n    sprintf(\"%.2f\", max(plume_stats$max_aod)),\n    sprintf(\"%.0f km²\", max(plume_stats$moderate_smoke_area)),\n    sprintf(\"%.0f km²\", max(plume_stats$heavy_smoke_area)),\n    sprintf(\"%.0f km²\", mean(plume_stats$moderate_smoke_area)),\n    format(peak_time$datetime, \"%B %d %H:%M UTC\")\n  )\n)\n\n# Print summary table\nknitr::kable(stats_summary, \n             caption = \"Smoke Plume Analysis Summary\",\n             format = \"pipe\")\n\n\nSmoke Plume Analysis Summary\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nAnalysis Period\nAugust 15 16:00 UTC to August 17 22:00 UTC\n\n\nPeak AOD Value\n5.00\n\n\nMaximum Affected Area (AOD &gt; 1)\n28 km²\n\n\nMaximum Heavy Smoke Area (AOD &gt; 2)\n5 km²\n\n\nAverage Affected Area (AOD &gt; 1)\n11 km²\n\n\nPeak Observation Time\nAugust 16 16:00 UTC\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Plume Extent\n\n\n\nWhen analyzing smoke plume extent: 1. Consider both intensity (AOD values) and spatial coverage 2. Remember that AOD measures total column aerosol - smoke may be at various heights 3. Cloud cover can create gaps in the data 4. The 500km analysis radius helps capture regional-scale impacts\n\n\n\n\n\nGOES-18 Wind Patterns\nUnderstanding atmospheric transport is crucial for predicting smoke impacts during wildfire events. Wind patterns not only influence fire behavior but also determine how smoke will affect surrounding communities. During the Yellowknife evacuation, wind conditions played a dual role: affecting both fire progression and creating hazardous air quality conditions that complicated evacuation efforts.\nTraditionally, smoke transport has been monitored through a combination of ground-based weather stations and atmospheric models. However, these methods can have significant limitations in remote regions like the Northwest Territories, where weather stations are sparse and model validation data is limited. Satellite-derived wind products help fill these gaps by providing broad spatial coverage and frequent updates. The GOES-18 Derived Motion Winds (DMW) product is particularly valuable as it provides wind measurements at multiple atmospheric levels by tracking the movement of features (such as clouds and water vapor) in successive satellite images.\nFor our analysis, we’ll focus on winds during the period of peak smoke intensity, combining GOES-18 DMW data with our aerosol optical depth measurements. This integration helps us understand: 1. The atmospheric conditions that influenced smoke transport 2. Potential evacuation routes that may have been impacted 3. Areas where convergent winds may have concentrated smoke 4. The broader regional transport patterns that affected air quality across the Northwest Territories\nFirst, let’s create a function to retrieve and process wind data.\n\n# Function to get wind data for a specific timestamp\nget_wind_data &lt;- function(date, hour) {\n  # Get DMW (Derived Motion Winds) file\n  files_df &lt;- get_goes_files(\n    date = date,\n    hour = hour,\n    product = \"ABI-L2-DMWVF\",  # Full disk product\n    satellite = \"goes18\"\n  )\n  \n  if (nrow(files_df) == 0) {\n    message(\"No wind files found for: \", date, \" \", hour, \":00 UTC\")\n    return(NULL)\n  }\n  \n  # Get most recent file\n  latest_file &lt;- files_df |&gt;\n    dplyr::arrange(desc(LastModified)) |&gt;\n    dplyr::slice(1)\n  \n  # Download and read data\n  url &lt;- construct_url(latest_file$Key, bucket = \"noaa-goes18\")\n  temp_file &lt;- tempfile(fileext = \".nc\")\n  utils::download.file(url, temp_file, mode = \"wb\", quiet = TRUE)\n  \n  # Read with ncdf4\n  nc &lt;- ncdf4::nc_open(temp_file)\n  \n  # Read wind data\n  tryCatch({\n    wind_data &lt;- list(\n      speed = ncdf4::ncvar_get(nc, \"wind_speed\"),\n      direction = ncdf4::ncvar_get(nc, \"wind_direction\"),\n      lat = ncdf4::ncvar_get(nc, \"lat\"),\n      lon = ncdf4::ncvar_get(nc, \"lon\"),\n      quality = ncdf4::ncvar_get(nc, \"DQF\")  # Quality flag\n    )\n    \n    # Create data frame of valid winds (where quality == 0)\n    valid_idx &lt;- which(wind_data$quality == 0)\n    wind_df &lt;- data.frame(\n      longitude = wind_data$lon[valid_idx],\n      latitude = wind_data$lat[valid_idx],\n      speed = wind_data$speed[valid_idx],\n      direction = wind_data$direction[valid_idx]\n    )\n    \n    # Remove any NA or invalid values\n    wind_df &lt;- wind_df[complete.cases(wind_df) & \n                      wind_df$speed &gt;= 0 & \n                      wind_df$speed &lt; 150 &  # reasonable wind speed limits\n                      wind_df$direction &gt;= 0 & \n                      wind_df$direction &lt;= 360, ]\n    \n    # Create spatial object\n    wind_sf &lt;- sf::st_as_sf(wind_df, \n                           coords = c(\"longitude\", \"latitude\"), \n                           crs = 4326)\n    \n    # Convert to stars\n    wind_stars &lt;- stars::st_as_stars(wind_sf)\n    \n    ncdf4::nc_close(nc)\n    unlink(temp_file)\n    \n    return(wind_stars)\n    \n  }, error = function(e) {\n    message(\"Error reading wind data: \", e$message)\n    ncdf4::nc_close(nc)\n    unlink(temp_file)\n    return(NULL)\n  })\n}\n\n\n\n\n\n\n\nWind Analysis\n\n\n\nThe GOES-18 Derived Motion Winds (DMW) product: - Tracks features in successive satellite images to derive wind vectors - Provides wind speed and direction at multiple atmospheric levels - Includes quality flags to identify reliable measurements - Helps understand atmospheric transport patterns\n\n\nTo visualize wind vectors, we need to convert speed and direction into vector components:\n\n# Function to convert wind speed and direction to u,v components\ncalculate_wind_components &lt;- function(speed, direction) {\n  # Convert direction from meteorological to mathematical angle\n  math_angle &lt;- (270 - direction) * pi / 180\n  \n  # Calculate u and v components\n  u &lt;- speed * cos(math_angle)\n  v &lt;- speed * sin(math_angle)\n  \n  return(list(u = u, v = v))\n}\n\n# Get wind data for peak plume time\npeak_winds &lt;- get_wind_data(\n  as.Date(peak_time$datetime), \n  lubridate::hour(peak_time$datetime)\n)\n\nNow we’ll process the wind data to match our analysis area:\n\n# Transform wind data to match AOD projection\nwind_transformed &lt;- sf::st_transform(peak_winds, sf::st_crs(peak_data$data))\n\n# Extract wind components\nwind_speed &lt;- as.numeric(wind_transformed[[1]])\nwind_direction &lt;- as.numeric(wind_transformed[[2]])\nwind_components &lt;- calculate_wind_components(wind_speed, wind_direction)\n\n# Convert to spatial features and clip to study area\nwind_sf &lt;- sf::st_as_sf(wind_transformed)\n\n# Properly handle intersections using st_intersects\nplume_goes &lt;- sf::st_transform(plume_buffer, sf::st_crs(peak_data$data))\nintersects &lt;- sf::st_intersects(wind_sf, plume_goes)\nintersecting_indices &lt;- which(lengths(intersects) &gt; 0)\nwind_sf_cropped &lt;- wind_sf[intersecting_indices, ]\n\nTo create a clear visualization, we’ll subsample the wind vectors:\n\n# Get coordinates of cropped winds\nwind_coords &lt;- sf::st_coordinates(wind_sf_cropped)\n\n# Create regular subsample indices (every nth point)\nn_points &lt;- nrow(wind_coords)\nsubsample_interval &lt;- max(1, floor(n_points / 300))  # Aim for ~300 vectors\nsubsample &lt;- seq(1, n_points, by = subsample_interval)\n\n# Create wind dataframe with subsampled points\nwind_df &lt;- data.frame(\n  x = wind_coords[subsample, 1],\n  y = wind_coords[subsample, 2],\n  u = wind_components$u[intersecting_indices][subsample],\n  v = wind_components$v[intersecting_indices][subsample],\n  speed = wind_speed[intersecting_indices][subsample]\n)\n\nFinally, let’s create our integrated visualization of smoke conditions and wind patterns:\n\n# Create the plot with subsampled wind vectors\np3_plume &lt;- ggplot2::ggplot() +\n  ggplot2::geom_sf(data = sf::st_transform(yk_boundary, \n                                          sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"#00BFFF\", linewidth = 0.75) +\n    ggplot2::geom_sf(data = sf::st_transform(plume_buffer, \n                                          sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"blue\", linetype = \"dashed\", linewidth = 2) +\n  # Add wind vectors\n  # The scaling factor of 2500 was chosen to create visible vectors at this map scale:\n  # - Raw u,v components are in m/s\n  # - Map coordinates are in meters (EPSG:3857)\n  # - 2500 means a 1 m/s wind creates a 2.5km vector on the map\n  # - This scaling provides good visibility while avoiding overcrowding\n  # - Adjust this value based on your map extent and wind speeds\n  ggplot2::geom_segment(data = wind_df,\n                        ggplot2::aes(x = x, y = y,\n                                   xend = x + u*2500,  # Scale factor for vector length\n                                   yend = y + v*2500,\n                                   color = speed),\n                        arrow = grid::arrow(length = ggplot2::unit(0.2, \"cm\")),\n                        alpha = 1,\n                        linewidth = 1.25) +\n  ggplot2::scale_color_viridis_c(\n    name = \"Wind Speed\\n(m/s)\",\n    option = \"turbo\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Yellowknife GOES Wind Vectors\",\n    subtitle = format(peak_data$datetime, \"%B %d, %Y %H:%M UTC\"),\n    caption = paste(\n      \"Grey: City boundary\\nBlue dashed: 500km analysis extent\\n\",\n      \"Affected area (AOD &gt; 1):\", \n      round(peak_time$moderate_smoke_area), \"km²\"\n    )\n  ) +\n  # Add coord_sf to ensure proper extent\n  ggplot2::coord_sf(xlim = sf::st_bbox(plume_goes)[c(\"xmin\", \"xmax\")],\n                    ylim = sf::st_bbox(plume_goes)[c(\"ymin\", \"ymax\")])\n\nprint(p3_plume)\n\n\n\n\n\n\n\n# Create wind statistics summary\nwind_stats &lt;- data.frame(\n  Metric = c(\n    \"Mean Wind Speed\",\n    \"Max Wind Speed\",\n    \"Predominant Wind Direction\",\n    \"Wind Speed at Yellowknife\"\n  ),\n  Value = c(\n    sprintf(\"%.1f m/s\", mean(wind_speed, na.rm = TRUE)),\n    sprintf(\"%.1f m/s\", max(wind_speed, na.rm = TRUE)),\n    sprintf(\"%.0f°\", median(wind_direction, na.rm = TRUE)),\n    sprintf(\"%.1f m/s\", wind_speed[which.min(sf::st_distance(\n      sf::st_as_sf(wind_transformed, as_points = TRUE),\n      sf::st_transform(yk_boundary, sf::st_crs(wind_transformed))\n    ))])\n  )\n)\n\n# Print statistics\nknitr::kable(wind_stats, \n             caption = \"Wind Conditions During Peak Plume Extent\",\n             format = \"pipe\")\n\n\nWind Conditions During Peak Plume Extent\n\n\nMetric\nValue\n\n\n\n\nMean Wind Speed\n13.7 m/s\n\n\nMax Wind Speed\n84.2 m/s\n\n\nPredominant Wind Direction\n226°\n\n\nWind Speed at Yellowknife\n23.0 m/s\n\n\n\n\n\n\nInterpreting Wind Patterns\nWhen analyzing wind vectors: 1. Vector length indicates wind speed 2. Vector direction shows where the wind is blowing towards 3. Color scaling highlights areas of stronger winds 4. Pattern variations can indicate: - Convergence zones where smoke may accumulate - Transport corridors for smoke movement - Areas of atmospheric mixing\n\n\n\n\n\n\nKnowledge Check: GOES Wind Analysis\n\n\n\nHow does the GOES-18 Derived Motion Winds (DMW) product determine wind patterns?\n\nUsing ground-based weather stations\nThrough computer modeling\nBy tracking the movement of features like clouds in successive satellite images\nBy measuring atmospheric pressure\n\n\n\n\n\n\nEvacuation Route Analysis\nDuring the Yellowknife evacuation, the majority of residents traveled south along Highway 3 (Yellowknife Highway) to Highway 1 (Mackenzie Highway). These routes were critically important as they represented the only road-based evacuation options for most residents. Analyzing these evacuation corridors in relation to fire and smoke conditions helps us understand the challenges faced during the evacuation.\nOpenStreetMap (OSM) provides a comprehensive source of road network data that is particularly valuable for remote regions like the Northwest Territories. As a collaborative mapping project, OSM relies on contributions from local knowledge, government data imports, and satellite imagery digitization. This crowdsourced approach often results in more up-to-date and detailed road networks than traditional commercial maps, especially in areas where official mapping resources may be limited. The OSM database includes not just road geometries, but also attributes like road classification, names, and reference numbers that help identify major evacuation routes.\nFor northern communities like Yellowknife, OSM’s road network data is crucial for emergency planning because it captures the limited transportation options available. The database accurately reflects the sparse road network characteristic of northern Canada, where communities are often connected by single highways that serve as critical lifelines during emergencies. Through the osmdata R package, we can programmatically access this road network data and focus our analysis on the key evacuation routes.\nFirst, let’s obtain the road network data using OpenStreetMap (OSM) and identify our key evacuation routes:\n\n# Transform plume buffer to WGS84 and get bbox for osm\nplume_buffer_wgs84 &lt;- sf::st_transform(plume_buffer, 4326)\nbbox_wgs84 &lt;- as.vector(sf::st_bbox(plume_buffer_wgs84))\n\n# Get main evacuation route (Highway 3)\nevac_route &lt;- osmdata::opq(bbox = bbox_wgs84) |&gt;\n  osmdata::add_osm_feature(\n    key = \"highway\", \n    value = c(\"primary\", \"trunk\", \"motorway\")\n  ) |&gt;\n  osmdata::osmdata_sf()\n\n:::\n\nOpenStreetMap Road Classifications\n\nmotorway: highest grade highways\ntrunk: major arterial routes\nprimary: major regional routes In the Northwest Territories, most intercity routes are classified as trunk or primary roads.\n\n:::\nNow we’ll filter for our specific evacuation routes:\n\n# Filter for both Highway 3 and Highway 1 segments\nevacuation_route &lt;- evac_route$osm_lines |&gt;\n  sf::st_transform(sf::st_crs(yk_boundary)) |&gt;\n  dplyr::filter(\n    # Highway 3 segments\n    grepl(\"3\", ref, fixed = TRUE) | \n    grepl(\"Highway 3\", name, fixed = TRUE) |\n    grepl(\"Yellowknife Highway\", name, fixed = TRUE) |\n    # Highway 1 segments\n    grepl(\"1\", ref, fixed = TRUE) |\n    grepl(\"Highway 1\", name, fixed = TRUE) |\n    (name == \"Mackenzie Highway\" & highway == \"trunk\")\n  ) |&gt;\n  sf::st_transform(sf::st_crs(peak_data$data))\n\n# Transform evacuation routes if needed\nevacuation_route &lt;- evacuation_route |&gt;\n  sf::st_transform(sf::st_crs(peak_data$data))\n\n# Intersect routes to get those in the plume buffer\nlocal_routes &lt;- sf::st_intersection(evacuation_route, plume_goes)\n\nLet’s create a visualization of the evacuation routes in relation to our study area:\n\n# Create local routes visualization\np4_local &lt;- ggplot2::ggplot() +\n  # Base map with routes\n  ggplot2::geom_sf(data = plume_buffer,\n                   fill = NA, color = \"gray70\", linetype = \"dashed\") +\n  ggplot2::geom_sf(data = local_routes,\n                   color = \"red\",\n                   linewidth = 1.5) +\n  ggplot2::geom_sf(data = sf::st_transform(yk_boundary, \n                                           sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"#00BFFF\", linewidth = 0.75) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Evacuation Routes from Yellowknife\",\n    subtitle = format(peak_data$datetime, \"%B %d, %Y %H:%M UTC\"),\n    caption = \"Red: Primary evacuation routes\\nLight blue: City boundary\\nGray dashed: 50km buffer\"\n  )\n\nprint(p4_local)\n\n\n\n\n\n\n\n\n\n\nEvacuation Route Considerations\nWhen analyzing evacuation routes, several factors are important: 1. Limited Options: In northern communities, there are often few alternate routes 2. Distance: Evacuees may need to travel long distances to reach safety 3. Vulnerability: Routes can be impacted by both fire and smoke 4. Capacity: Limited routes may face congestion during mass evacuation\n\n\n\n\n\n\nKnowledge Check: Evacuation Routes\n\n\n\nWhy is OpenStreetMap particularly valuable for analyzing evacuation routes in remote regions like Northwest Territories?\n\nIt provides real-time traffic updates\nIt includes detailed elevation data\nIt offers more up-to-date and detailed road networks through crowdsourced local knowledge\nIt shows historical road conditions\n\n\n\n\n\n\nIntegrated Evacuation Conditions\nBy combining our analyses of smoke plumes, wind patterns, and evacuation routes, we can better understand the challenging conditions faced by evacuees and emergency managers. During wildfire evacuations, the interaction between smoke transport and evacuation routes is particularly critical - heavy smoke can reduce visibility, impact air quality, and potentially force evacuees to seek alternate routes.\nLet’s start by getting the most relevant FIRMS data for our peak smoke period:\n\n# Get FIRMS data for peak time with larger extent\nbbox &lt;- sf::st_bbox(sf::st_transform(plume_buffer, 4326)) |&gt;\n  round(4)\n\n\nfirms_url &lt;- sprintf(\n  \"https://firms.modaps.eosdis.nasa.gov/api/area/csv/%s/%s/%.4f,%.4f,%.4f,%.4f/%d/%s\",\n  \"API_MAP_KEY\",   # your MAP_KEY\n  \"VIIRS_NOAA20_SP\",                   # product (SOURCE)\n  bbox[\"xmin\"], bbox[\"ymin\"],         # WEST, SOUTH\n  bbox[\"xmax\"], bbox[\"ymax\"],         # EAST, NORTH\n  10,                                   # DAY_RANGE\n  format(peak_time$datetime, \"%Y-%m-%d\")  # DATE\n)\n\n# Get the data\nresp &lt;- httr2::request(firms_url) |&gt;\n  httr2::req_perform()\n\n# Process into spatial data\nfirms_points_peak &lt;- httr2::resp_body_string(resp) |&gt;\n  textConnection() |&gt;\n  utils::read.csv() |&gt;\n  sf::st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  ) |&gt;\n  sf::st_transform(sf::st_crs(peak_data$data))\n\nNow we’ll create fire perimeters to show the extent of active burning:\n\n# Create fire perimeters using our earlier clustering function\nfire_perims &lt;- create_clustered_perimeters(firms_points_peak, \n                                          date = as.Date(peak_time$datetime))\n\n\n\n\n\n\n\nUnderstanding the Integrated Visualization\n\n\n\nThe following visualization combines several key elements: - Smoke plume extent (AOD values) - Active fire perimeters - Wind vectors showing transport patterns - Primary evacuation routes\nThis integration helps visualize the multiple hazards evacuees faced.\n\n\nNow we can create our comprehensive visualization showing the interaction of all these factors:\n\n# Create integrated visualization with larger extent\np4_integrated &lt;- ggplot2::ggplot() +\n  # Smoke plume base\n  stars::geom_stars(data = peak_data$data, alpha = 0.50) +\n  # Evacuation routes\n  ggplot2::geom_sf(data = evacuation_route,\n                   color = \"black\",\n                   linewidth = 1.1) +\n  # Boundaries\n  ggplot2::geom_sf(data = sf::st_transform(yk_boundary, \n                                           sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"#00BFFF\", linewidth = 2) +\n  ggplot2::geom_sf(data = sf::st_transform(plume_buffer, \n                                           sf::st_crs(peak_data$data)),\n                   fill = NA, color = \"grey50\", linetype = \"dashed\", linewidth = 1.5) +\n  # Fire perimeters\n  ggplot2::geom_sf(data = fire_perims,\n                   fill = \"orange\",\n                   alpha = 0.75,\n                   color = \"red\",\n                   linewidth = 0.5) +\n  # Wind vectors\n  ggplot2::geom_segment(data = wind_df,\n                        ggplot2::aes(x = x, y = y,\n                                     xend = x + u*2500,\n                                     yend = y + v*2500,\n                                     color = speed),\n                        arrow = grid::arrow(length = ggplot2::unit(0.2, \"cm\")),\n                        alpha = 0.65,\n                        linewidth = 1.1) +\n  # Scales\n  ggplot2::scale_fill_viridis_c(\n    name = \"AOD (Plume Hazard)\",\n    na.value = NA,\n    option = \"inferno\",\n    limits = c(0, 3)\n  ) +\n  ggplot2::scale_color_gradientn(\n    name = \"Wind Speed (m/s)\",\n    colors = c(\"blue\", \"purple\")\n  )+\n  ggplot2::theme_minimal() +\n  ggplot2::labs(\n    title = \"Yellowknife, CA Wildfire Crisis: Evacuation Conditions\",\n    subtitle = format(peak_data$datetime, \"%B %d, %Y %H:%M UTC\"),\n    caption = paste(\n      \"Black: Primary evacuation routes (OpenStreetMap) | Light blue: City boundary\\n\",\n      \"Orange areas: Active fire complexes (NASA FIRMS VIIRS 375m)\\n\",\n      \"Wind vectors: GOES-18 Derived Motion Winds | Background: GOES-18 Aerosol Optical Depth\\n\",\n      \"Gray dashed: 500km analysis extent\"\n    )\n  )+\n  ggplot2::coord_sf(xlim = sf::st_bbox(plume_goes)[c(\"xmin\", \"xmax\")],\n                    ylim = sf::st_bbox(plume_goes)[c(\"ymin\", \"ymax\")])\n\nprint(p4_integrated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Evacuation Conditions\n\n\n\nWhen interpreting this integrated view, consider:\n\nRoute Vulnerability:\n\nProximity of fires to evacuation routes\nSmoke intensity along evacuation corridors\nWind patterns that could drive fire or smoke toward routes\n\nTemporal Factors:\n\nThis represents conditions at peak smoke intensity\nConditions likely varied throughout the evacuation period\nWind patterns influenced both fire spread and smoke transport\n\nSpatial Relationships:\n\nLimited evacuation options (single main route)\nRegional extent of smoke impacts\nPotential for route closures due to fire proximity\n\nEvacuation Management Implications:\n\nNeed for real-time monitoring of multiple hazards\nImportance of alternative evacuation plans\nValue of integrated spatial analysis for decision support\n\n\n\n\nThis comprehensive visualization demonstrates the complex challenges faced during the Yellowknife evacuation. The combination of active fires, extensive smoke plumes, and limited evacuation routes created a particularly challenging scenario for both emergency managers and evacuees. The wind patterns during this period were especially critical, as they influenced both the movement of smoke and the potential for fire spread toward evacuation routes.\n\n\n\n\n\n\nKnowledge Check: Comprehensive Analysis\n\n\n\nWhich combination of data sources provides the most complete understanding of evacuation conditions during a wildfire event?\n\nFIRMS data and weather forecasts only\nWind patterns and smoke plumes only\nIntegration of fire detections, smoke plumes, wind patterns, and evacuation routes\nBurned area analysis only"
  },
  {
    "objectID": "m302-wildfire-assessment.html#conclusion",
    "href": "m302-wildfire-assessment.html#conclusion",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThis lesson demonstrated how multiple remote sensing products can be integrated to analyze wildfire impacts, taking the 2023 Yellowknife evacuation as a case study. Through hands-on analysis of NASA’s FIRMS active fire detections, GOES-18 aerosol optical depth and wind data, and VIIRS burned area products, we gained insights into both the technical aspects of geospatial analysis and the real-world applications for emergency management.\nThis case study illustrates the vital role of Earth observation data in:\n\nSupporting emergency management decisions\nDocumenting and analyzing extreme events\nUnderstanding the progression of natural disasters\nValidating ground-based observations with satellite data\n\nThe methods demonstrated here can be applied to other wildfire events or adapted for different types of natural hazards where multiple spatial datasets need to be integrated for comprehensive analysis.\n\nFuture Directions\nWhile this analysis focused on basic integration of available datasets, future work could:\n\nInclude additional data sources such as weather conditions and fuel moisture\nDevelop automated processing chains for near-real-time analysis\nIncorporate ground validation data\nAnalyze longer time series to understand historical context\nIntegrate socioeconomic data to assess community impacts"
  },
  {
    "objectID": "m302-wildfire-assessment.html#footnotes",
    "href": "m302-wildfire-assessment.html#footnotes",
    "title": "Geospatial Analysis of Canadian Wildfire Impacts: A Multi-sensor Approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPhoto Credit: NASA Earth Observatory↩︎\nPhoto Credt: Detroit Free Press, Alberta Wildfire Firefighters↩︎\nPhoto Credit: Statistics Canada↩︎\nPhoto Credit: City of Yellowknife↩︎\nPhoto Credit: Global News↩︎\nPhoto Credit: NWT↩︎"
  },
  {
    "objectID": "m304b-arctic-climate.html",
    "href": "m304b-arctic-climate.html",
    "title": "Community-Generated Lesson: The Major Effects of Climate Change Towards the Arctic Ecosystem",
    "section": "",
    "text": "from IPython.display import display, Image\nfrom IPython.display import HTML\nfrom IPython.display import display ,Markdown \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nWe will be focusing on is data analysis (management and analysis) and how climate change is affecting the ecosystem in the Arctic. We can collect data by looking into the rising temperatures over the years, satellite heat map of the arctic, the melting ice, and how the Arctic’s species ecosystem is being affected. All the data that is being collected is going to connect with each other by showing the impact that climate change is doing to the Arctic.”\n\n#display image from URL\nimage_url = \"https://www.theglobeandmail.com/resizer/v2/XOHD6ZNGBZEQ7NKJY2D32TULAE?auth=23fab6336f88534f8370296fb915d7f81eef7e904a3c8bc968e98274374e95eb&width=1200&quality=80\"\nprint(Image(url=image_url))\n\nlink_html = '&lt;a href=\"https://www.theglobeandmail.com/technology/science/shrinking-polar-bears-a-barometer-for-the-climate-sensitive-north/article20904215/\" target=_blank\"&gt;How the effects of climate change in Arctic Canada are shrinking polar bears&lt;/a&gt;'\nprint(HTML(link_html))\n\n&lt;IPython.core.display.Image object&gt;\n&lt;IPython.core.display.HTML object&gt;\n\n\n\nKey Takeaways\n\nTemperature rising\nIce melting\nData and Habitat loss\nImpact of climate change\n\n\n\nWhat Is Climate Change?\nClimate change is a long-term change based on the earth’s temperature patters and atmospheric conditions. The increasing temperate of the planet due to the increased of greenhouse gases in the atmosphere. The gases trap heat in the atmosphere which leads to global warming and can affect the planet’s climate systems. Black Carbon is an air pollutant with bad health effects, and it can impact the environment. It can absorb the heat which can lead to the ice and snow to melt, also mention by APA(Environmental Protection Agency. (n.d.). Drivers of Climate Change in the Arctic. EPA.). Black carbon decreases the ability of snow and ice to reflect heat from the sun.\n\n\nIce Melting\nCurrently the Arctics sea ice has been showing a decrease and making the ice thinner and vulnerable. The ice is slowly going away as it ages and that can make the sea level rise. NSIDC ( National Snow and Ice data Center)gather data to provides an animation with sea ice animations for the arctic showing you how from 1979 to 2024 the Artic Sea ice is becoming smaller as time goes by. Not only the ice is melting over the years, but the sea levels are rising, and it is causing an effect on the habitat species like polar bears, seals, and walruses.\n\n\n\n\nArctic sea ice animations, 1979 to 2024\n\n\n\n\nSea Ice\nSea ice extent is the total area of the ocean’s surface that is covered by sea ice, and it has been declining at an alarming rate. Climate.gov provided a map and a graph to show us how the ice extent has been getting smaller over the years. The median extends from 1981-2010 compared to the median extent of 1991- 2020 was smaller and you can see the outline of the ice extent shrinking. The bottom graph displays the ice extend of each September from 1979-2024. From 1979 to 1998 you can see a small increase of the sea ice extent, but it starts to drop from 2000 to 2024. This shows how the ice keeps decreasing over time because the temperature of the Arctic keeps on increasing as time goes by.\n \n\n\nMy Example Using a Graph Bar:\nI create a graph bar to show an example of visual representation on how we are losing sea ice over the years. I wanted to show an example with the code that I made, to show how every year there is a change on how much sea ice we end up losing. Sometimes Arctic temperature data can be confusing, so providing an example can help break the information to clarify the data.\n\ndata = {\n    'Year': [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020],\n    'Sea Ice Loss': [1.0, 1.2, 1.5, 1.3, 1.4, 1.6, 1.8, 1.7, 1.9, 2.0, 2.1]  # Example loss values\n}\n\n\ndf = pd.DataFrame(data)\n\n# Plotting the bar graph\nplt.figure(figsize=(10, 6))\nplt.bar(df['Year'], df['Sea Ice Loss'], color='skyblue')\n\n# Adding labels and title\nplt.xlabel('Year')\nplt.ylabel('Sea Ice Loss (Million Square Kilometers)')\nplt.title('Sea Ice Loss in the Arctic Over Time')\nplt.xticks(df['Year'], rotation=45)\n\n# Display \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "m304a-viirs-wildfires.html",
    "href": "m304a-viirs-wildfires.html",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "",
    "text": "In this lesson, you will learn to access, preprocess, and analyze AVIRIS-3 data to map burned areas from the Palisades Fire in Los Angeles. Using satellite imagery obtained through the VIIRS tool, you will examine pre- and post-fire conditions, assess the fire’s impact on vegetation and infrastructure.\n\n\nBy the end of this lesson, you should be able to:\n\nAccess and download AVIRIS-3 and VIIRS Burn Area data for wildfire analysis.\nPreprocess and clean AVIRIS-3 spectral radiance data for visualization.\nCompare pre- and post-fire imagery to assess vegetation and burned area changes.\n\nOverlay VIIRS Burn Area data with AVIRIS-3 images using GIS tools to map burned areas.\nEvaluate wildfire impact by analyzing the overlap of burn data with infrastructure (e.g., buildings, population, critical infrastructure).\nQuantify damage and assess risk to urban zones and infrastructure."
  },
  {
    "objectID": "m304a-viirs-wildfires.html#learning-objectives",
    "href": "m304a-viirs-wildfires.html#learning-objectives",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nAccess and download AVIRIS-3 and VIIRS Burn Area data for wildfire analysis.\nPreprocess and clean AVIRIS-3 spectral radiance data for visualization.\nCompare pre- and post-fire imagery to assess vegetation and burned area changes.\n\nOverlay VIIRS Burn Area data with AVIRIS-3 images using GIS tools to map burned areas.\nEvaluate wildfire impact by analyzing the overlap of burn data with infrastructure (e.g., buildings, population, critical infrastructure).\nQuantify damage and assess risk to urban zones and infrastructure."
  },
  {
    "objectID": "m304a-viirs-wildfires.html#palisades-fire-overview",
    "href": "m304a-viirs-wildfires.html#palisades-fire-overview",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Palisades Fire Overview",
    "text": "Palisades Fire Overview\nThe Palisades Fire affected large parts of the Los Angeles region, particularly residential areas and natural vegetation. Satellite imagery from AVIRIS-3 and VIIRS provides us with valuable data to assess the extent of the fire’s impact. By overlaying burn area data with infrastructure maps, we can determine how much of the urban and natural landscapes were affected. The data allows us to quantify the impact and identify areas at high risk."
  },
  {
    "objectID": "m304a-viirs-wildfires.html#understanding-aviris-3-data",
    "href": "m304a-viirs-wildfires.html#understanding-aviris-3-data",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Understanding AVIRIS-3 Data",
    "text": "Understanding AVIRIS-3 Data\nAVIRIS-3 provides high-resolution hyperspectral imagery across a wide range of wavelengths, allowing for detailed analysis of land surface properties.\n\nimport os\nimport earthaccess\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom scipy.interpolate import griddata\n\nfrom osgeo import gdal, osr\n\nA NASA Earthdata account is needed to download NASA Earthdata files.\n\n# Authenticate with NASA Earthdata (you need an account)\n\nAuth = earthaccess.login()\n# Search for VIIRS Burned Area data\nviirs_data = earthaccess.search_data(\n    short_name=\"AV3_L1B_RDN_2356\", \n    temporal=('2025-01-16', '2025-01-16'),  # Palisades's wildfire (January 6 - January 17 2025)\n    bounding_box=(-118.8, 33.9, -118.46, 34.28)  # Bounding box for Palisades California\n)\n\nprint(f\"Found {len(viirs_data)} files.\")"
  },
  {
    "objectID": "m304a-viirs-wildfires.html#processing-and-visualizing-satellite-images",
    "href": "m304a-viirs-wildfires.html#processing-and-visualizing-satellite-images",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Processing and Visualizing Satellite Images",
    "text": "Processing and Visualizing Satellite Images\n\n# Extract RDN.nc file URLs from VIIRS data\ngranule_urls = [\n    [file_url for file_url in file.data_links() if file_url.endswith('RDN.nc')]\n    for file in viirs_data[-6:]\n]\n\nfor granule in granule_urls:\n    # Download files\n    earthaccess.download(granule, 'data/granule_files')\n            \nprint(\"Processing complete for all granules.\")"
  },
  {
    "objectID": "m304a-viirs-wildfires.html#plotting-data",
    "href": "m304a-viirs-wildfires.html#plotting-data",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Plotting Data",
    "text": "Plotting Data\n\n# Plot first granule - Frist Flight\n\nfile_path = \"data/granule_files/AV320250116t193840_005_L1B_RDN_3f4aef90_RDN.nc\"\n\n# Open the file and plot radiance with geolocation\nwith h5py.File(file_path, 'r') as f:\n    radiance = f['radiance']['radiance']\n    lat = f['lat'][:]\n    lon = f['lon'][:]\n    \n    # Take a specific band (like band 25)\n    data = radiance[25, :, :]\n    \n    # Create cell edges\n    lon_edges = (lon[:-1, :-1] + lon[1:, 1:]) / 2\n    lat_edges = (lat[:-1, :-1] + lat[1:, 1:]) / 2\n    \n    # Sort to fix the monotonic issue\n    lon_edges = np.sort(lon_edges, axis=1)\n    lat_edges = np.sort(lat_edges, axis=0)\n\n    plt.figure(figsize=(12, 4))\n    plt.pcolormesh(lon_edges, lat_edges, data[:-1, :-1], shading='auto', cmap='viridis')\n    plt.colorbar(label='Radiance')\n    plt.title('Radiance (Band 25) with Geolocation')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.show()\n\n\n\n\n\n\n\n\n\n# Plot first bacth of granules - First Flight\ngranule_paths = glob.glob('data/*/*40_*RDN.nc')\n\nfig, axes = plt.subplots(1, 6, figsize=(16, 2))  # 1 row, 6 columns\n\nfor i, path in enumerate(granule_paths):\n    with h5py.File(path, 'r') as f:\n        radiance = f['radiance']['radiance']\n        lat = f['lat'][:]\n        lon = f['lon'][:]\n        data = radiance[25, :, :]\n        lon_edges = (lon[:-1, :-1] + lon[1:, 1:]) / 2\n        lat_edges = (lat[:-1, :-1] + lat[1:, 1:]) / 2\n        lon_edges = np.sort(lon_edges, axis=1)\n        lat_edges = np.sort(lat_edges, axis=0)\n        pcm = axes[i].pcolormesh(lon_edges, lat_edges, data[:-1, :-1], shading='auto', cmap='viridis')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nflights = ['01', '03', '05']\n\nfor flight in flights:\n    # List of file paths (ordered west to east)\n    file_paths = sorted(glob.glob(f'data/granule_files/*{flight}_*RDN.nc'))\n\n    # Band to extract\n    band = 25\n\n    # Define resolution for the common grid\n    num_points = 1000\n\n    # Initialize common grid bounds\n    lon_min, lon_max = float('inf'), float('-inf')\n    lat_min, lat_max = float('inf'), float('-inf')\n\n    # First loop to define grid bounds\n    for file_path in file_paths:\n        with h5py.File(file_path, 'r') as f:\n            lat = f['lat'][:]\n            lon = f['lon'][:]\n            lon_min = min(lon_min, lon.min())\n            lon_max = max(lon_max, lon.max())\n            lat_min = min(lat_min, lat.min())\n            lat_max = max(lat_max, lat.max())\n\n    # Create common grid\n    grid_lon = np.linspace(lon_min, lon_max, num_points)\n    grid_lat = np.linspace(lat_min, lat_max, num_points)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    # Initialize container for merged data\n    merged_radiance_list = []\n\n    file = 1\n    # Second loop to process each granule and interpolate to the grid\n    for file_path in file_paths:\n        print(f'--&gt; Flight: {flight}, File {file} out of {len(file_paths)}')\n        with h5py.File(file_path, 'r') as f:\n            radiance = f['radiance']['radiance'][band, :, :]\n            lat = f['lat'][:]\n            lon = f['lon'][:]\n            \n            # Flatten arrays for interpolation\n            points = np.array([lon.flatten(), lat.flatten()]).T\n            radiance_flat = radiance.flatten()\n            \n            # Interpolate to common grid\n            interpolated_radiance = griddata(\n                points, radiance_flat, (grid_lon, grid_lat), method='linear'\n            )\n            \n            # Store result for averaging\n            merged_radiance_list.append(interpolated_radiance)\n\n    # Average the overlapping areas across all granules\n    merged_radiance = np.nanmean(merged_radiance_list, axis=0)\n\n    # Plot the merged result\n    plt.figure(figsize=(25, 3))\n    plt.pcolormesh(grid_lon, grid_lat, merged_radiance, shading='auto', cmap='viridis')\n    plt.colorbar(label='Radiance (uW nm-1 cm-2 sr-1)')\n    plt.title(f'Merged Radiance (Band {band}) with Geolocation - Fixed')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.show()\n\n--&gt; Flight: 01, File 1 out of 1\n\n\n\n\n\n\n\n\n\n--&gt; Flight: 03, File 1 out of 1\n\n\n\n\n\n\n\n\n\n--&gt; Flight: 05, File 1 out of 1\n\n\n\n\n\n\n\n\n\n\n# Output file path\n    output_file = f'data/granule_files/merged_radiance_{flight}.tif'\n\n    # Create a new GeoTIFF file\n    driver = gdal.GetDriverByName('GTiff')\n    rows, cols = merged_radiance.shape\n    dataset = driver.Create(output_file, cols, rows, 1, gdal.GDT_Float32)\n\n    # Set geotransform and projection\n    xmin, xmax = grid_lon.min(), grid_lon.max()\n    ymin, ymax = grid_lat.min(), grid_lat.max()\n    xres = (xmax - xmin) / cols\n    yres = (ymax - ymin) / rows\n\n    geotransform = (xmin, xres, 0, ymax, 0, -yres)\n    dataset.SetGeoTransform(geotransform)\n\n    # Set projection to WGS84\n    srs = osr.SpatialReference()\n    srs.ImportFromEPSG(4326)  # WGS84\n    dataset.SetProjection(srs.ExportToWkt())\n\n    # Write data to the file\n    dataset.GetRasterBand(1).WriteArray(merged_radiance)\n    dataset.GetRasterBand(1).SetNoDataValue(np.nan)\n\n    # Close the dataset\n    dataset = None\n\n    print(f\"✅ GeoTIFF saved to: {output_file}\")"
  },
  {
    "objectID": "m304a-viirs-wildfires.html#radiance-band-25-interpretation",
    "href": "m304a-viirs-wildfires.html#radiance-band-25-interpretation",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Radiance (Band 25) Interpretation",
    "text": "Radiance (Band 25) Interpretation\n\nThe left image shows the radiance in the near-infrared (NIR) spectrum, which highlights vegetation and surface reflectance.\nIn healthy, undisturbed vegetation, NIR reflectance tends to be higher because of the internal leaf structure.\nBurned or damaged areas typically show reduced NIR reflectance since the leaf structure is destroyed or altered."
  },
  {
    "objectID": "m304a-viirs-wildfires.html#normalized-burn-ratio-nbr-interpretation",
    "href": "m304a-viirs-wildfires.html#normalized-burn-ratio-nbr-interpretation",
    "title": "Community-Generated Lesson: Mapping Wildfire Burned Areas Using VIIRS/AVIRIS-3 Data (Python)",
    "section": "Normalized Burn Ratio (NBR) Interpretation",
    "text": "Normalized Burn Ratio (NBR) Interpretation\n\nThe NBR image (right) measures fire severity by comparing the difference between NIR (healthy vegetation) and SWIR (sensitive to moisture loss and charring).\n\nHigh NBR values (close to +1, shown in red) indicate healthy vegetation.\nLow NBR values (close to -1, shown in blue) indicate burned or damaged areas.\n\n\nEvidence of Fire Impact in NBR:\n\nLarge patches of light red/orange color over the coastal and mountainous areas indicate healthy vegetation.\nHowever, the areas closer to the coastline and lowlands show lower NBR values (closer to 0 or negative), suggesting:\n\nLoss of vegetation\nScorched or charred areas\nFire impact on the ground cover and tree canopy\n\nThe smooth red area along the coast suggests widespread burn scars or barren ground post-fire."
  },
  {
    "objectID": "docs/m304c-california-wildfires.html",
    "href": "docs/m304c-california-wildfires.html",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "",
    "text": "This lesson will focus on the environmental impact of California Wildfires, focusing on changes in land cover, vegetation loss, and air quality before, during, and after major wildfire events. Using NASA datasets such as MODIS and VIIRS, users will learn how to access, visualize, and interpret satellite imagery to assess wildfire severity and its broader implications for ecosystems, air quality, and human health. In addition to analyzing spatial data, the lesson will integrate external datasets, such as air pollution measurements, to provide a more comprehensive understanding of wildfire impacts.\nWildfires in Los Angeles, California"
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#explanation-of-the-content",
    "href": "docs/m304c-california-wildfires.html#explanation-of-the-content",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Explanation of the Content:",
    "text": "Explanation of the Content:\nThis passage describes the conditions and factors contributing to a wildfire in Los Angeles County in January 2025, including:\n\nMeteorological conditions: High winds (Santa Ana winds) and dry weather made the region highly susceptible to fire.\n\nVegetation buildup: Due to the wet years of 2022-2024, vegetation in the hills and foothills of California built up significantly, providing ample fuel for the fire.\n\nHydroclimate whiplash: A rapid transition from wet to dry conditions (hydroclimate whiplash) contributed to the fire risk by creating an environment where vegetation grew abundantly but was later dried out.\n\nSatellite data (NDVI): The Normalized Difference Vegetation Index (NDVI), which uses satellite imagery from Landsat, shows that vegetation in the area was 30% greener than average. This indicates that a large amount of fuel (plants) was available to burn.\n\nSoil moisture: Despite the vegetation growth from the wet years, the soil moisture was extremely low (in the bottom 2% historically) by January 7, 2025. Low soil moisture is often associated with dry conditions that contribute to more intense fires.\n\n\nAir Quality in California Before and After Recent Wildfires\n-The January 2025 wildfires in California, particularly around Los Angeles, had a profound impact on air quality, with significant variations observed before, during, and after the events.\n\n\nBefore\nPrior to the wildfires, air quality in the greater Los Angeles region was generally within acceptable standards. Measurements of fine particulate matter (PM2.5) typically remained below the U.S. Environmental Protection Agency’s (EPA) safety threshold of 35 micrograms per cubic meter (µg/m³), indicating healthy air conditions.\n\n\nDuring\nThe onset of the wildfires on January 7, 2025, led to a rapid and severe deterioration in air quality:\nPM2.5 Levels: Monitoring stations recorded alarming spikes in PM2.5 concentrations. For instance, the Harrison Elementary School station reported levels reaching 184.1 µg/m³, approximately 36.8 times the World Health Organization’s annual guideline value. Other stations across Los Angeles County registered one-hour peaks as high as 483.7 µg/m³, categorizing the air quality as hazardous.\nHealth Advisories: Authorities issued warnings urging residents, especially those with respiratory conditions, to remain indoors, utilize air purifiers, and wear N95 masks when necessary. Symptoms such as burning eyes, throat irritation, and exacerbated respiratory issues were commonly reported.\n\n\nAfter\nFollowing the containment of the fires, air quality began to improve; however, certain concerns persisted:\nResidual Pollution: While PM2.5 levels gradually declined to safer ranges, some areas continued to experience elevated concentrations due to lingering smoke and resuspended ash particles. Residents were advised to continue monitoring air quality reports and take precautions during outdoor activities.\nSoil and Water Contamination: The combustion of urban structures released hazardous substances, including heavy metals like lead and zinc, into the environment. These contaminants posed risks of soil and water pollution, necessitating ongoing environmental assessments and remediation efforts.\n\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \n\n\n# Air Quality Data Visualization\n\ndays = np.array([\"Before (Jan 1-6)\", \"During (Jan 7-12)\", \"After (Jan 13-18)\"])\npm25_levels = np.array([12, 184, 45])  # values in µg/m³\n\nplt.figure(figsize=(8, 5))\nplt.bar(days, pm25_levels, color=[\"green\", \"red\", \"orange\"])\nplt.xlabel(\"Time Period\")\nplt.ylabel(\"PM2.5 Concentration (µg/m³)\")\nplt.title(\"Air Quality Before, During, and After the 2025 California Wildfires\")\nplt.ylim(0, 200)\nplt.show()"
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#before-the-wildfire-jan-1-6-safe-air-quality",
    "href": "docs/m304c-california-wildfires.html#before-the-wildfire-jan-1-6-safe-air-quality",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Before the Wildfire (Jan 1-6) – Safe Air Quality",
    "text": "Before the Wildfire (Jan 1-6) – Safe Air Quality\n\nPM2.5 Level: 12 µg/m³ (green bar)\nAir quality was within EPA’s safe standard (below 35 µg/m³), meaning most people could breathe easily without health risks.\nConditions consisted of Clear skies, good visibility, and minimal respiratory risks for sensitive groups (children, elderly, and those with asthma or heart disease)."
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#during-the-wildfire-jan-7-12-hazardous-air-quality",
    "href": "docs/m304c-california-wildfires.html#during-the-wildfire-jan-7-12-hazardous-air-quality",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "During the Wildfire (Jan 7-12) – Hazardous Air Quality",
    "text": "During the Wildfire (Jan 7-12) – Hazardous Air Quality\n\nPM2.5 Level: 184 µg/m³ (red bar)\nA dramatic spike in air pollution due to massive smoke, ash, and airborne particles.\n\n\nHealth Impacts:\nSevere health risks for everyone, not just sensitive groups. Increased respiratory issues, eye irritation, throat discomfort, and aggravated heart conditions. Authorities issued health warnings, urging people to stay indoors, wear N95 masks, and use air purifiers.\n\n\nEnvironmental Impact:\nSmoke spread across cities like Los Angeles, Sacramento, and San Francisco, causing poor visibility. Nearby vegetation and wildlife suffered from the intense particulate matter."
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#after-the-wildfire-jan-13-18-lingering-pollution",
    "href": "docs/m304c-california-wildfires.html#after-the-wildfire-jan-13-18-lingering-pollution",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "After the Wildfire (Jan 13-18) – Lingering Pollution",
    "text": "After the Wildfire (Jan 13-18) – Lingering Pollution\n\nPM2.5 Level: 45 µg/m³ (orange bar)\nThough air quality improved as fires were contained, it remained above safe levels due to residual smoke and ash particles still circulating."
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#continued-risks",
    "href": "docs/m304c-california-wildfires.html#continued-risks",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Continued Risks:",
    "text": "Continued Risks:\nRespiratory irritation and discomfort persisted, especially in affected regions. Air quality fluctuated based on wind patterns and weather conditions. Some areas required weeks or months to return to pre-wildfire conditions."
  },
  {
    "objectID": "docs/m304c-california-wildfires.html#long-term-effects",
    "href": "docs/m304c-california-wildfires.html#long-term-effects",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Long-term Effects:",
    "text": "Long-term Effects:\nPossible soil and water contamination from the burnt debris. Risk of secondary pollution as wind carries ash and fine particulates over time.\n\n## soil moisture data (values in %)\ndates = pd.date_range(start=\"2024-12-20\", periods=10, freq='D')\nsoil_moisture_before = np.array([35, 34, 36, 33, 31, 30, 28, 27, 25, 24])  # Before wildfire\ndates_after = pd.date_range(start=\"2025-01-10\", periods=10, freq='D')\nsoil_moisture_after = np.array([20, 19, 18, 16, 15, 14, 13, 12, 11, 10])  # After wildfire\n\n# Creates DataFrames\ndf_before = pd.DataFrame({'Date': dates, 'Soil Moisture (%)': soil_moisture_before})\ndf_after = pd.DataFrame({'Date': dates_after, 'Soil Moisture (%)': soil_moisture_after})\n\n# Print summary statistics\nprint(\"Soil Moisture Data Before the Wildfire:\\n\", df_before.describe(), \"\\n\")\nprint(\"Soil Moisture Data After the Wildfire:\\n\", df_after.describe(), \"\\n\")\n\n# Plotting the data\nplt.figure(figsize=(10, 5))\nplt.plot(df_before['Date'], df_before['Soil Moisture (%)'], marker='o', linestyle='-', color='blue', label=\"Before Wildfire\")\nplt.plot(df_after['Date'], df_after['Soil Moisture (%)'], marker='s', linestyle='-', color='red', label=\"After Wildfire\")\n\nplt.xlabel(\"Date\")\nplt.ylabel(\"Soil Moisture (%)\")\nplt.title(\"Soil Moisture Levels Before and After the 2025 California Wildfire\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.grid()\nplt.show()\n\nSoil Moisture Data Before the Wildfire:\n                       Date  Soil Moisture (%)\ncount                   10          10.000000\nmean   2024-12-24 12:00:00          30.300000\nmin    2024-12-20 00:00:00          24.000000\n25%    2024-12-22 06:00:00          27.250000\n50%    2024-12-24 12:00:00          30.500000\n75%    2024-12-26 18:00:00          33.750000\nmax    2024-12-29 00:00:00          36.000000\nstd                    NaN           4.217688 \n\nSoil Moisture Data After the Wildfire:\n                       Date  Soil Moisture (%)\ncount                   10          10.000000\nmean   2025-01-14 12:00:00          14.800000\nmin    2025-01-10 00:00:00          10.000000\n25%    2025-01-12 06:00:00          12.250000\n50%    2025-01-14 12:00:00          14.500000\n75%    2025-01-16 18:00:00          17.500000\nmax    2025-01-19 00:00:00          20.000000\nstd                    NaN           3.425395"
  },
  {
    "objectID": "m304c-california-wildfires.html",
    "href": "m304c-california-wildfires.html",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "",
    "text": "This lesson will focus on the environmental impact of California Wildfires, focusing on changes in land cover, vegetation loss, and air quality before, during, and after major wildfire events. Using NASA datasets such as MODIS and VIIRS, users will learn how to access, visualize, and interpret satellite imagery to assess wildfire severity and its broader implications for ecosystems, air quality, and human health. In addition to analyzing spatial data, the lesson will integrate external datasets, such as air pollution measurements, to provide a more comprehensive understanding of wildfire impacts.\nWildfires in Los Angeles, California"
  },
  {
    "objectID": "m304c-california-wildfires.html#explanation-of-the-content",
    "href": "m304c-california-wildfires.html#explanation-of-the-content",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Explanation of the Content:",
    "text": "Explanation of the Content:\nThis passage describes the conditions and factors contributing to a wildfire in Los Angeles County in January 2025, including:\n\nMeteorological conditions: High winds (Santa Ana winds) and dry weather made the region highly susceptible to fire.\n\nVegetation buildup: Due to the wet years of 2022-2024, vegetation in the hills and foothills of California built up significantly, providing ample fuel for the fire.\n\nHydroclimate whiplash: A rapid transition from wet to dry conditions (hydroclimate whiplash) contributed to the fire risk by creating an environment where vegetation grew abundantly but was later dried out.\n\nSatellite data (NDVI): The Normalized Difference Vegetation Index (NDVI), which uses satellite imagery from Landsat, shows that vegetation in the area was 30% greener than average. This indicates that a large amount of fuel (plants) was available to burn.\n\nSoil moisture: Despite the vegetation growth from the wet years, the soil moisture was extremely low (in the bottom 2% historically) by January 7, 2025. Low soil moisture is often associated with dry conditions that contribute to more intense fires.\n\n\nAir Quality in California Before and After Recent Wildfires\n-The January 2025 wildfires in California, particularly around Los Angeles, had a profound impact on air quality, with significant variations observed before, during, and after the events.\n\n\nBefore\nPrior to the wildfires, air quality in the greater Los Angeles region was generally within acceptable standards. Measurements of fine particulate matter (PM2.5) typically remained below the U.S. Environmental Protection Agency’s (EPA) safety threshold of 35 micrograms per cubic meter (µg/m³), indicating healthy air conditions.\n\n\nDuring\nThe onset of the wildfires on January 7, 2025, led to a rapid and severe deterioration in air quality:\nPM2.5 Levels: Monitoring stations recorded alarming spikes in PM2.5 concentrations. For instance, the Harrison Elementary School station reported levels reaching 184.1 µg/m³, approximately 36.8 times the World Health Organization’s annual guideline value. Other stations across Los Angeles County registered one-hour peaks as high as 483.7 µg/m³, categorizing the air quality as hazardous.\nHealth Advisories: Authorities issued warnings urging residents, especially those with respiratory conditions, to remain indoors, utilize air purifiers, and wear N95 masks when necessary. Symptoms such as burning eyes, throat irritation, and exacerbated respiratory issues were commonly reported.\n\n\nAfter\nFollowing the containment of the fires, air quality began to improve; however, certain concerns persisted:\nResidual Pollution: While PM2.5 levels gradually declined to safer ranges, some areas continued to experience elevated concentrations due to lingering smoke and resuspended ash particles. Residents were advised to continue monitoring air quality reports and take precautions during outdoor activities.\nSoil and Water Contamination: The combustion of urban structures released hazardous substances, including heavy metals like lead and zinc, into the environment. These contaminants posed risks of soil and water pollution, necessitating ongoing environmental assessments and remediation efforts.\n\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \n\n\n# Air Quality Data Visualization\n\ndays = np.array([\"Before (Jan 1-6)\", \"During (Jan 7-12)\", \"After (Jan 13-18)\"])\npm25_levels = np.array([12, 184, 45])  # values in µg/m³\n\nplt.figure(figsize=(8, 5))\nplt.bar(days, pm25_levels, color=[\"green\", \"red\", \"orange\"])\nplt.xlabel(\"Time Period\")\nplt.ylabel(\"PM2.5 Concentration (µg/m³)\")\nplt.title(\"Air Quality Before, During, and After the 2025 California Wildfires\")\nplt.ylim(0, 200)\nplt.show()"
  },
  {
    "objectID": "m304c-california-wildfires.html#before-the-wildfire-jan-1-6-safe-air-quality",
    "href": "m304c-california-wildfires.html#before-the-wildfire-jan-1-6-safe-air-quality",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Before the Wildfire (Jan 1-6) – Safe Air Quality",
    "text": "Before the Wildfire (Jan 1-6) – Safe Air Quality\n\nPM2.5 Level: 12 µg/m³ (green bar)\nAir quality was within EPA’s safe standard (below 35 µg/m³), meaning most people could breathe easily without health risks.\nConditions consisted of Clear skies, good visibility, and minimal respiratory risks for sensitive groups (children, elderly, and those with asthma or heart disease)."
  },
  {
    "objectID": "m304c-california-wildfires.html#during-the-wildfire-jan-7-12-hazardous-air-quality",
    "href": "m304c-california-wildfires.html#during-the-wildfire-jan-7-12-hazardous-air-quality",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "During the Wildfire (Jan 7-12) – Hazardous Air Quality",
    "text": "During the Wildfire (Jan 7-12) – Hazardous Air Quality\n\nPM2.5 Level: 184 µg/m³ (red bar)\nA dramatic spike in air pollution due to massive smoke, ash, and airborne particles.\n\n\nHealth Impacts:\nSevere health risks for everyone, not just sensitive groups. Increased respiratory issues, eye irritation, throat discomfort, and aggravated heart conditions. Authorities issued health warnings, urging people to stay indoors, wear N95 masks, and use air purifiers.\n\n\nEnvironmental Impact:\nSmoke spread across cities like Los Angeles, Sacramento, and San Francisco, causing poor visibility. Nearby vegetation and wildlife suffered from the intense particulate matter."
  },
  {
    "objectID": "m304c-california-wildfires.html#after-the-wildfire-jan-13-18-lingering-pollution",
    "href": "m304c-california-wildfires.html#after-the-wildfire-jan-13-18-lingering-pollution",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "After the Wildfire (Jan 13-18) – Lingering Pollution",
    "text": "After the Wildfire (Jan 13-18) – Lingering Pollution\n\nPM2.5 Level: 45 µg/m³ (orange bar)\nThough air quality improved as fires were contained, it remained above safe levels due to residual smoke and ash particles still circulating."
  },
  {
    "objectID": "m304c-california-wildfires.html#continued-risks",
    "href": "m304c-california-wildfires.html#continued-risks",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Continued Risks:",
    "text": "Continued Risks:\nRespiratory irritation and discomfort persisted, especially in affected regions. Air quality fluctuated based on wind patterns and weather conditions. Some areas required weeks or months to return to pre-wildfire conditions."
  },
  {
    "objectID": "m304c-california-wildfires.html#long-term-effects",
    "href": "m304c-california-wildfires.html#long-term-effects",
    "title": "Community-Generated Lesson: Data Analysis of California Wildfires",
    "section": "Long-term Effects:",
    "text": "Long-term Effects:\nPossible soil and water contamination from the burnt debris. Risk of secondary pollution as wind carries ash and fine particulates over time.\n\n## soil moisture data (values in %)\ndates = pd.date_range(start=\"2024-12-20\", periods=10, freq='D')\nsoil_moisture_before = np.array([35, 34, 36, 33, 31, 30, 28, 27, 25, 24])  # Before wildfire\ndates_after = pd.date_range(start=\"2025-01-10\", periods=10, freq='D')\nsoil_moisture_after = np.array([20, 19, 18, 16, 15, 14, 13, 12, 11, 10])  # After wildfire\n\n# Creates DataFrames\ndf_before = pd.DataFrame({'Date': dates, 'Soil Moisture (%)': soil_moisture_before})\ndf_after = pd.DataFrame({'Date': dates_after, 'Soil Moisture (%)': soil_moisture_after})\n\n# Print summary statistics\nprint(\"Soil Moisture Data Before the Wildfire:\\n\", df_before.describe(), \"\\n\")\nprint(\"Soil Moisture Data After the Wildfire:\\n\", df_after.describe(), \"\\n\")\n\n# Plotting the data\nplt.figure(figsize=(10, 5))\nplt.plot(df_before['Date'], df_before['Soil Moisture (%)'], marker='o', linestyle='-', color='blue', label=\"Before Wildfire\")\nplt.plot(df_after['Date'], df_after['Soil Moisture (%)'], marker='s', linestyle='-', color='red', label=\"After Wildfire\")\n\nplt.xlabel(\"Date\")\nplt.ylabel(\"Soil Moisture (%)\")\nplt.title(\"Soil Moisture Levels Before and After the 2025 California Wildfire\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.grid()\nplt.show()\n\nSoil Moisture Data Before the Wildfire:\n                       Date  Soil Moisture (%)\ncount                   10          10.000000\nmean   2024-12-24 12:00:00          30.300000\nmin    2024-12-20 00:00:00          24.000000\n25%    2024-12-22 06:00:00          27.250000\n50%    2024-12-24 12:00:00          30.500000\n75%    2024-12-26 18:00:00          33.750000\nmax    2024-12-29 00:00:00          36.000000\nstd                    NaN           4.217688 \n\nSoil Moisture Data After the Wildfire:\n                       Date  Soil Moisture (%)\ncount                   10          10.000000\nmean   2025-01-14 12:00:00          14.800000\nmin    2025-01-10 00:00:00          10.000000\n25%    2025-01-12 06:00:00          12.250000\n50%    2025-01-14 12:00:00          14.500000\n75%    2025-01-16 18:00:00          17.500000\nmax    2025-01-19 00:00:00          20.000000\nstd                    NaN           3.425395"
  },
  {
    "objectID": "m304d-snow-storms.html",
    "href": "m304d-snow-storms.html",
    "title": "Snow Storms 1990",
    "section": "",
    "text": "import geopandas as gpd\n\n\ngdf = gpd.read_file(\"data/snow-storms/Snow_19000226_19000303_1.shp\") # Display the first few rows of the GeoDataFrame print(gdf.head())\nprint(gdf.head())\n\n  Net    Lat    Lon                  Name State  Elevation   d1   d2   d3  \\\n0   C  35.45 -92.40            BEE BRANCH    AR      198.1  0.0  0.0  4.0   \n1   C  35.08 -92.43                CONWAY    AR       96.0  0.0  0.0  2.0   \n2   C  36.08 -94.17          FAYETTEVILLE    AR      418.0  0.0  0.0  2.5   \n3   C  36.10 -94.17  FAYETTEVILLE EXP STN    AR      387.0  0.0  0.0  2.5   \n4   C  34.48 -91.53             STUTTGART    AR       65.0  0.0  0.0  0.2   \n\n    d4   d5   d6  snowfall     GHCND_ID             STORM_ID START_DATE  \\\n0  0.0  0.0  0.0       4.0  USC00030528  19000226_19000303_1 1900-02-26   \n1  0.0  0.0  0.0       2.0  USC00031596  19000226_19000303_1 1900-02-26   \n2  0.0  0.0  0.0       2.5  USC00032442  19000226_19000303_1 1900-02-26   \n3  0.0  0.0  0.0       2.5  USC00032444  19000226_19000303_1 1900-02-26   \n4  0.0  0.0  0.0       0.2  USC00036918  19000226_19000303_1 1900-02-26   \n\n    END_DATE                        geometry  \n0 1900-03-03  POINT (323876.643 -223490.835)  \n1 1900-03-03  POINT (322734.114 -264971.665)  \n2 1900-03-03  POINT (163308.307 -157517.397)  \n3 1900-03-03  POINT (163265.179 -155278.009)  \n4 1900-03-03  POINT (407197.398 -328571.755)"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html",
    "href": "m301-coastal-hazards-PR.html",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "",
    "text": "VIIRS Nighttime Lights\nLECZ\nNHGIS\nLiDAR SAR"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#datasets",
    "href": "m301-coastal-hazards-PR.html#datasets",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "",
    "text": "VIIRS Nighttime Lights\nLECZ\nNHGIS\nLiDAR SAR"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#areas-of-interest-aois",
    "href": "m301-coastal-hazards-PR.html#areas-of-interest-aois",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Areas of Interest (AOIs):",
    "text": "Areas of Interest (AOIs):\n\nPuerto Rico (PRI)"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#functions",
    "href": "m301-coastal-hazards-PR.html#functions",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Functions:",
    "text": "Functions:\n\nImage segmentation\nValidation with SAR or LiDAR"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#low-elevation-coastal-zones-leczs",
    "href": "m301-coastal-hazards-PR.html#low-elevation-coastal-zones-leczs",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Low Elevation Coastal Zones (LECZs)",
    "text": "Low Elevation Coastal Zones (LECZs)\nLow Elevation Coastal Zones (LECZs) have been defined globally, with population estimates for areas below 5m and 10m elevations (McGranahan, Balk, and Anderson 2007; MacManus et al. 2021).\nIn the continental U.S. (CONUS), 1 in 10 people live in the 10m LECZ, and studies highlight that urban residents, people of color, and older adults are disproportionately exposed. For instance, about 1 in 5 urban Black residents live in this zone (Tagtachian and Balk 2023; Hauer et al. 2020).\nThis is the sneak peak of what LECZ looks like: \nYou may wonder why studies of the “entire” US often restrict themselves to the CONUS? The simplest answer is limitations either data or computational power. For example, this happens because of incomplete coverage in one data set or another. Some US territories may not collect the full suite of census variables that are collected in CONUS.\nFor example the detail on housing characteristics is limited in Guam, Northern Mariana Islands, US Virgin Islands and American Samoa, and the Census’ American Community Survey is not conducted in any of the territories, though Puerto Rico conducts its own Community Survey. In some other cases, data collected from satellites (such as SRTM) have variable accuracy toward polar regions U.S Census Bureau.\nAnother possible reason for omission outside of CONUS could be computational challenges or limitations. For instance US territories are subject to different map projections, which implies the need for additional functions in processing algorithms to account for spatial variations and to unify spatial structures.\n\n\n\n\n\n\nProgramming Reminder\n\n\n\n“What is a Map Projection?”\nMap projections have existed for thousands of years. They help map makers render the spherical surface of the Earth flat – so it can be seen on a piece of paper or computer screen, and so that the unit of measure is uniform throughout the area of interest. As a result, map projections distort something – area, shape, distance, direction – to make this possible.\nHere are some resources to learn more about map projections:\nA brief video explainer\nA brief guide from USGS\nThere are many resources to guide a new learner, so enjoy learning!"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#using-ipums-api-to-pull-u.s-census-data-for-puerto-rico",
    "href": "m301-coastal-hazards-PR.html#using-ipums-api-to-pull-u.s-census-data-for-puerto-rico",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Using IPUMS API to pull U.S Census Data for Puerto Rico",
    "text": "Using IPUMS API to pull U.S Census Data for Puerto Rico\n\nRegistering to IPUMS and the National Historical Geographic Information System (NHGIS)\nIn order to retrieve an IPUMS API Key, you will have to register for an account for IPUMS and request your API Key.\nAdditionally register, to The National Historical Geographic Information System (NHGIS)\nAfter you requested your IPUMS API, to the NHGIS, store it in os.env format. You will need your registration email and the API Key:\n\n# Step 1: Load API key from .env file\nload_dotenv()\nIPUMS_API_KEY = os.getenv(\"EMAIL\", \"API_KEY\")\n\nif IPUMS_API_KEY is None:\n    raise ValueError(\"API key not found. Make sure IPUMS_API_KEY is defined in your .env file.\")\n\n# Step 2: Initialize IPUMS client\nipums = IpumsApiClient(IPUMS_API_KEY)\n\nGetting shapefile metadata in order to get the filename for downloading the shapefile in the below chunk.\nThis block will be returning the shapefile name of interest so that we can download it in the next block\n\n# After registering to NHGIS, please run this code\n\nfor page in ipums.get_metadata_catalog(\"nhgis\", metadata_type=\"shapefiles\"):\n    for shapefile in page[\"data\"]:\n        if shapefile[\"extent\"] == \"Puerto Rico\":\n            if shapefile[\"geographicLevel\"] == \"Block Group\" and shapefile[\"year\"] == \"2010\":\n                print( \"Name: \" + shapefile[\"name\"] + \" | Year: \" + shapefile[\"year\"])\n\nName: 720_blck_grp_2010_tl2010 | Year: 2010\nName: 720_blck_grp_2010_tl2020 | Year: 2010\n\n\n\n\nDownlaoding shapefiles from IPUMS\nWith this API key, we can extract geospatial data from the IPUMS API. Using the geo level blck_grp, we can speicify that we want to extract data at the Block Group level.\nThe AggregateDataExtract function specifies the collection to use, in this case NHGIS, give it a human-readable label for the extract request, and requests the 2010 and 2020 Summary File 1 (SF1a) dataset with tables P12 (sex by age) and H3 (vacancy status) at the block group geographic level. It also limits the extract to geographic extent code “720” (Puerto Rico).\nGetting data from 2010 and 2020 with the variables for 5-year Age Groups (P12) and the Housing (H3).\n\n# Submit extraction data to IPUMS portal\nextract = AggregateDataExtract(\n    collection=\"nhgis\",  # Use NHGIS collection\n    description=\"Puerto Rico 2010–2020 vacancy\",  # Extract label\n    datasets=[\n        NhgisDataset(\n            name=\"2010_SF1a\",  # 2010 dataset\n            data_tables=[\"P12\", \"H3\"],  # Tables: sex by age, vacancy\n            geog_levels=[\"blck_grp\"]  # At block group level\n        ),\n        NhgisDataset(\n            name=\"2020_DHCa\",  # 2020 dataset\n            data_tables=[\"P12\", \"H3\"],  # Same tables\n            geog_levels=[\"blck_grp\"]  # Same level\n        ),\n    ],\n    geographic_extents=[\"720\"]  # Puerto Rico only\n    # shapefiles=[\"720_blck_grp_2020_tl2020\"]  # Optional: include shapefile\n)\n\nThis code sends the extract request to IPUMS, prints the unique extract ID so you can track it, and sets up to wait until the extract is finished.\n\n# Submit the extract request\nipums.submit_extract(extract)  # Send request to IPUMS\nprint(f\"Extract ID: {extract.extract_id}\")  # Print the extract ID\n\n# Wait for the extract to finish\n\nThis code sets up the folder where the extract will be saved, creates it if it doesn’t already exist, and downloads the extract from IPUMS to that location:\n\n# Download the extract\ncurrent = os.getcwd()  # Get current working directory\nDOWNLOAD_DIR = os.path.join(f\"{current}/data/ipums/block\")  # Set download path\n\nos.makedirs(DOWNLOAD_DIR, exist_ok=True)  # Create folder if needed\n\nipums.download_extract(extract, download_dir=DOWNLOAD_DIR)  # Download files to folder\n\nAfter downloading the extract, this code navigates to the download directory, identifies the ZIP file containing the CSV data, and inspects its contents. It then locates the specific CSV files for the years 2010 and 2020 using filename patterns, and reads them directly from the ZIP archive into pandas DataFrames—no need to manually unzip anything!\n\ncurrent = os.getcwd()  # Get current working directory\nDOWNLOAD_DIR = os.path.join(f\"{current}/data/ipums/block\")  # Set path to downloaded extract\nfile_list = os.listdir(DOWNLOAD_DIR)  # List files in download folder\ncsv_zip = [f for f in file_list if f.endswith('_csv.zip')]  # Find ZIP with CSVs\ncsv = f\"{DOWNLOAD_DIR}/{csv_zip[0]}\"  # Get full path to ZIP\n\n# Read zip data file in the extract\nwith ZipFile(csv) as z:\n    csv_data = z.namelist()  # List files inside ZIP\n    print(\"Contents of zip: \", csv_data)\n    \n    # Find the correct CSVs using filename patterns\n    file_2020 = next(f for f in csv_data if '2020' in f and f.endswith('.csv'))  # 2020 file\n    file_2010 = next(f for f in csv_data if '2010' in f and f.endswith('.csv'))  # 2010 file\n\n    # Read CSVs into DataFrames\n    with z.open(file_2020) as f:\n        df_2020 = pd.read_csv(f)  # Load 2020 data\n\n    with z.open(file_2010) as f:\n        df_2010 = pd.read_csv(f)  # Load 2010 data\n\nContents of zip:  ['nhgis0012_csv/nhgis0012_ds258_2020_blck_grp_codebook.txt', 'nhgis0012_csv/nhgis0012_ds258_2020_blck_grp.csv', 'nhgis0012_csv/nhgis0012_ds172_2010_blck_grp.csv', 'nhgis0012_csv/nhgis0012_ds172_2010_blck_grp_codebook.txt']\n\n\nThis section uses NHGIS Codebook file(s) that were automatically included in your data extract to rename cryptic column codes in the 2010 and 2020 datasets to human-readable labels. These codes correspond to census tables on sex by age and housing occupancy. Renaming makes analysis and visualization much easier later in your workflow.\nLook for the .txt file(s) in the zipped file you downloaded, and they will shed some light on your data.\n\n# The NHGIS codes are as follows in the documentation which is downloaded from the IPUMS API \n\n# Rename columns for dataframe 2020\n\n'''    Table 1:     Sex by Age for Selected Age Categories\n    Universe:    Total population\n    Source code: P12\n    NHGIS code:  U7S\n        U7S001:      Total\n        U7S002:      Male\n        U7S003:      Male: Under 5 years\n        U7S004:      Male: 5 to 9 years\n        U7S005:      Male: 10 to 14 years\n        U7S006:      Male: 15 to 17 years\n        U7S007:      Male: 18 and 19 years\n        U7S008:      Male: 20 years\n        U7S009:      Male: 21 years\n        U7S010:      Male: 22 to 24 years\n        U7S011:      Male: 25 to 29 years\n        U7S012:      Male: 30 to 34 years\n        U7S013:      Male: 35 to 39 years\n        U7S014:      Male: 40 to 44 years\n        U7S015:      Male: 45 to 49 years\n        U7S016:      Male: 50 to 54 years\n        U7S017:      Male: 55 to 59 years\n        U7S018:      Male: 60 and 61 years\n        U7S019:      Male: 62 to 64 years\n        U7S020:      Male: 65 and 66 years\n        U7S021:      Male: 67 to 69 years\n        U7S022:      Male: 70 to 74 years\n        U7S023:      Male: 75 to 79 years\n        U7S024:      Male: 80 to 84 years\n        U7S025:      Male: 85 years and over\n        U7S026:      Female\n        U7S027:      Female: Under 5 years\n        U7S028:      Female: 5 to 9 years\n        U7S029:      Female: 10 to 14 years\n        U7S030:      Female: 15 to 17 years\n        U7S031:      Female: 18 and 19 years\n        U7S032:      Female: 20 years\n        U7S033:      Female: 21 years\n        U7S034:      Female: 22 to 24 years\n        U7S035:      Female: 25 to 29 years\n        U7S036:      Female: 30 to 34 years\n        U7S037:      Female: 35 to 39 years\n        U7S038:      Female: 40 to 44 years\n        U7S039:      Female: 45 to 49 years\n        U7S040:      Female: 50 to 54 years\n        U7S041:      Female: 55 to 59 years\n        U7S042:      Female: 60 and 61 years\n        U7S043:      Female: 62 to 64 years\n        U7S044:      Female: 65 and 66 years\n        U7S045:      Female: 67 to 69 years\n        U7S046:      Female: 70 to 74 years\n        U7S047:      Female: 75 to 79 years\n        U7S048:      Female: 80 to 84 years\n        U7S049:      Female: 85 years and over\n \n    Table 2:     Occupancy Status\n    Universe:    Housing units\n    Source code: H3\n    NHGIS code:  U9X\n        U9X001:      Total\n        U9X002:      Occupied\n        U9X003:      Vacant\n'''\n\n\nrename_2020 = {\n    \"U7S001\": \"Total_Population\",\n    \"U7S002\": \"Male\",\n    \"U7S003\": \"Male: Under 5 years\",\n    \"U7S004\": \"Male: 5 to 9 years\",\n    \"U7S005\":      \"Male: 10 to 14 years\",\n    \"U7S006\":      \"Male: 15 to 17 years\",\n    \"U7S007\":      \"Male: 18 and 19 years\",\n    \"U7S008\":      \"Male: 20 years\",\n    \"U7S009\":      \"Male: 21 years\",\n    \"U7S010\":      \"Male: 22 to 24 years\",\n    \"U7S011\":      \"Male: 25 to 29 years\",\n    \"U7S012\":      \"Male: 30 to 34 years\",\n    \"U7S013\":      \"Male: 35 to 39 years\",\n    \"U7S014\":      \"Male: 40 to 44 years\",\n    \"U7S015\":      \"Male: 45 to 49 years\",\n    \"U7S016\":      \"Male: 50 to 54 years\",\n    \"U7S017\":      \"Male: 55 to 59 years\",\n    \"U7S018\":      \"Male: 60 and 61 years\",\n    \"U7S019\":      \"Male: 62 to 64 years\",\n    \"U7S020\":      \"Male: 65 and 66 years\",\n    \"U7S021\":      \"Male: 67 to 69 years\",\n    \"U7S022\":      \"Male: 70 to 74 years\",\n    \"U7S023\":      \"Male: 75 to 79 years\",\n    \"U7S024\":      \"Male: 80 to 84 years\",\n    \"U7S025\":      \"Male: 85 years and over\",\n    \"U7S026\":      \"Female\",\n    \"U7S027\":      \"Female: Under 5 years\",\n    \"U7S028\":      \"Female: 5 to 9 years\",\n    \"U7S029\":      \"Female: 10 to 14 years\",\n    \"U7S030\":      \"Female: 15 to 17 years\",\n    \"U7S031\":      \"Female: 18 and 19 years\",\n    \"U7S032\":      \"Female: 20 years\",\n    \"U7S033\":      \"Female: 21 years\",\n    \"U7S034\":      \"Female: 22 to 24 years\",\n    \"U7S035\":      \"Female: 25 to 29 years\",\n    \"U7S036\":      \"Female: 30 to 34 years\",\n    \"U7S037\":      \"Female: 35 to 39 years\",\n    \"U7S038\":      \"Female: 40 to 44 years\",\n    \"U7S039\":      \"Female: 45 to 49 years\",\n    \"U7S040\":      \"Female: 50 to 54 years\",\n    \"U7S041\":      \"Female: 55 to 59 years\",\n    \"U7S042\":      \"Female: 60 and 61 years\",\n    \"U7S043\":      \"Female: 62 to 64 years\",\n    \"U7S044\":      \"Female: 65 and 66 years\",\n    \"U7S045\":      \"Female: 67 to 69 years\",\n    \"U7S046\":      \"Female: 70 to 74 years\",\n    \"U7S047\":      \"Female: 75 to 79 years\",\n    \"U7S048\":      \"Female: 80 to 84 years\",\n    \"U7S049\":      \"Female: 85 years and over\",\n    \"U9X001\": \"Total_Housing_Units\",\n    \"U9X002\": \"Occupied\",\n    \"U9X003\": \"Vacant\"\n}\n\n#Rename columns for dataframe 2010\n'''    Table 1:     Housing Units\n    Universe:    Housing units\n    Source code: H1\n    NHGIS code:  IFC\n        IFC001:      Total\n \n    Table 2:     Occupancy Status\n    Universe:    Housing units\n    Source code: H3\n    NHGIS code:  IFE\n        IFE001:      Total\n        IFE002:      Occupied\n        IFE003:      Vacant'''\n\nrename_2010 = {\n    \"H76001\": \"Total_Population\",\n    \"H76002\": \"Male\",\n    \"H76003\": \"Male: Under 5 years\",\n    \"H76004\": \"Male: 5 to 9 years\",\n    \"H76005\":      \"Male: 10 to 14 years\",\n    \"H76006\":      \"Male: 15 to 17 years\",\n    \"H76007\":      \"Male: 18 and 19 years\",\n    \"H76008\":      \"Male: 20 years\",\n    \"H76009\":      \"Male: 21 years\",\n    \"H76010\":      \"Male: 22 to 24 years\",\n    \"H76011\":      \"Male: 25 to 29 years\",\n    \"H76012\":      \"Male: 30 to 34 years\",\n    \"H76013\":      \"Male: 35 to 39 years\",\n    \"H76014\":      \"Male: 40 to 44 years\",\n    \"H76015\":      \"Male: 45 to 49 years\",\n    \"H76016\":      \"Male: 50 to 54 years\",\n    \"H76017\":      \"Male: 55 to 59 years\",\n    \"H76018\":      \"Male: 60 and 61 years\",\n    \"H76019\":      \"Male: 62 to 64 years\",\n    \"H76020\":      \"Male: 65 and 66 years\",\n    \"H76021\":      \"Male: 67 to 69 years\",\n    \"H76022\":      \"Male: 70 to 74 years\",\n    \"H76023\":      \"Male: 75 to 79 years\",\n    \"H76024\":      \"Male: 80 to 84 years\",\n    \"H76025\":      \"Male: 85 years and over\",\n    \"H76026\":      \"Female\",\n    \"H76027\":      \"Female: Under 5 years\",\n    \"H76028\":      \"Female: 5 to 9 years\",\n    \"H76029\":      \"Female: 10 to 14 years\",\n    \"H76030\":      \"Female: 15 to 17 years\",\n    \"H76031\":      \"Female: 18 and 19 years\",\n    \"H76032\":      \"Female: 20 years\",\n    \"H76033\":      \"Female: 21 years\",\n    \"H76034\":      \"Female: 22 to 24 years\",\n    \"H76035\":      \"Female: 25 to 29 years\",\n    \"H76036\":      \"Female: 30 to 34 years\",\n    \"H76037\":      \"Female: 35 to 39 years\",\n    \"H76038\":      \"Female: 40 to 44 years\",\n    \"H76039\":      \"Female: 45 to 49 years\",\n    \"H76040\":      \"Female: 50 to 54 years\",\n    \"H76041\":      \"Female: 55 to 59 years\",\n    \"H76042\":      \"Female: 60 and 61 years\",\n    \"H76043\":      \"Female: 62 to 64 years\",\n    \"H76044\":      \"Female: 65 and 66 years\",\n    \"H76045\":      \"Female: 67 to 69 years\",\n    \"H76046\":      \"Female: 70 to 74 years\",\n    \"H76047\":      \"Female: 75 to 79 years\",\n    \"H76048\":      \"Female: 80 to 84 years\",\n    \"H76049\":      \"Female: 85 years and over\",\n    \"IFC001\": \"Total_Housing\",\n    \"IFE001\": \"Total_Housing_Units\",\n    \"IFE002\": \"Occupied\",\n    \"IFE003\": \"Vacant\"\n}\n\n\n\n# Apply renaming to both datasets\ndf_2010.rename(columns=rename_2010, inplace=True)  # Rename 2010 columns\ndf_2020.rename(columns=rename_2020, inplace=True)  # Rename 2020 columns\n\nThis step filters both datasets to include only records from Puerto Rico, which is identified in the IPUMS data by STATEA == 72. It also removes any columns that are completely empty (all values are NaN), which helps clean up the data for analysis.\n\n# Subset Puerto Rico (STATEA code 72)\npr_df_2010 = df_2010[df_2010[\"STATEA\"] == 72]  # Filter 2010 data for Puerto Rico\npr_df_2020 = df_2020[df_2020[\"STATEA\"] == 72]  # Filter 2020 data for Puerto Rico\n\n# Drop columns with all missing values\npr_df_2010 = pr_df_2010.dropna(axis=1, how='all')  # Clean 2010 data\npr_df_2020 = pr_df_2020.dropna(axis=1, how='all')  # Clean 2020 data\n\nThis step calculates the total population aged 60 and over by summing the relevant male and female age group columns. It then computes two new indicators for both 2010 and 2020:\n\nAgedRatio: the share of the total population that is 60+\nVacantRatio: the share of housing units that are vacant\n\nThese ratios help measure aging and housing vacancy patterns in Puerto Rico over time.\n\n# Define age columns for population 60+\npop60plus_cols = [\n    \"Female: 60 and 61 years\",\n    \"Female: 62 to 64 years\",\n    \"Female: 65 and 66 years\",\n    \"Female: 67 to 69 years\",\n    \"Female: 70 to 74 years\",\n    \"Female: 75 to 79 years\",\n    \"Female: 80 to 84 years\",\n    \"Female: 85 years and over\",\n    \"Male: 60 and 61 years\",\n    \"Male: 62 to 64 years\",\n    \"Male: 65 and 66 years\",\n    \"Male: 67 to 69 years\",\n    \"Male: 70 to 74 years\",\n    \"Male: 75 to 79 years\",\n    \"Male: 80 to 84 years\",\n    \"Male: 85 years and over\"\n]\n\n# Calculate totals and ratios for 2010\npr_df_2010[\"Pop60plus_total\"] = pr_df_2010[pop60plus_cols].sum(axis=1)  # Sum 60+ pop\npr_df_2010[\"AgedRatio\"] = pr_df_2010[\"Pop60plus_total\"] / pr_df_2010[\"Total_Population\"]  # 60+ share\npr_df_2010[\"VacantRatio\"] = pr_df_2010[\"Vacant\"] / pr_df_2010[\"Total_Housing_Units\"]  # Vacancy share\n\n# Calculate totals and ratios for 2020\npr_df_2020[\"Pop60plus_total\"] = pr_df_2020[pop60plus_cols].sum(axis=1)\npr_df_2020[\"AgedRatio\"] = pr_df_2020[\"Pop60plus_total\"] / pr_df_2020[\"Total_Population\"]\npr_df_2020[\"VacantRatio\"] = pr_df_2020[\"Vacant\"] / pr_df_2020[\"Total_Housing_Units\"]\n\nLets create a scatter plot to explore the relationship between aging and housing vacancy in Puerto Rico in 2010. Each point represents a geographic unit (e.g., block group), where the x-axis shows the Vacant Ratio (share of housing units that are vacant), and the y-axis shows the Aged Ratio (share of population aged 60+).\nThis kind of visualization helps reveal spatial patterns or clusters related to population aging and housing dynamics.\n\n# Plot Aged Ratio vs. Vacant Ratio for 2010\nplt.scatter(pr_df_2010[\"VacantRatio\"], pr_df_2010[\"AgedRatio\"], alpha=0.3)  # Transparent points\n\nplt.title(\"Aged Ratio (60+) over Vacancy Ratio\")  # Plot title\nplt.xlabel(\"Vacant Ratio\")  # X-axis label\nplt.ylabel(\"Aged Ratio\")  # Y-axis label\nplt.grid(True)  # Show grid\nplt.tight_layout()  # Adjust layout\nplt.show()  # Display plot\n\n\n\n\n\n\n\n\nThis step prepares for comparison between the 2010 and 2020 datasets.\nIt selects key columns (like total population, aged population, and vacancy) from the 2010 data and merges them with the full 2020 data using GISJOIN as a unique geographic identifier.\nThe resulting merged_df contains side-by-side values for both years, using suffixes to distinguish 2010 and 2020 columns.\n\n# Key columns to compare between years\ncolumns_used = [\n    \"Total_Population\",  \"Pop60plus_total\", \"AgedRatio\",\n    \"Total_Housing_Units\", \"Occupied\", \"Vacant\", \"VacantRatio\"\n]\n\n# Merge 2020 data with selected 2010 columns using GISJOIN\nmerged_df = pr_df_2020.merge(\n    pr_df_2010[[\"GISJOIN\"] + columns_used],  # Keep GISJOIN + key columns from 2010\n    on=\"GISJOIN\",  # Join on geographic ID\n    how=\"inner\",  # Keep only matching rows\n    suffixes=(\"_2020\", \"_2010\")  # Label columns by year\n)\n\n# View merged dataset\nmerged_df\n\n\n\n\n\n\n\n\nGISJOIN\nYEAR\nSTUSAB\nGEOID\nGEOCODE\nREGIONA\nDIVISIONA\nSTATE\nSTATEA\nCOUNTY\n...\nPop60plus_total_2020\nAgedRatio_2020\nVacantRatio_2020\nTotal_Population_2010\nPop60plus_total_2010\nAgedRatio_2010\nTotal_Housing_Units_2010\nOccupied_2010\nVacant_2010\nVacantRatio_2010\n\n\n\n\n0\nG72000109563001\n2020\nPR\n1500000US720019563001\n720019563001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n562\n0.298301\n0.102815\n2171\n363\n0.167204\n854\n737\n117\n0.137002\n\n\n1\nG72000109563002\n2020\nPR\n1500000US720019563002\n720019563002\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n568\n0.286290\n0.070263\n2165\n438\n0.202309\n881\n751\n130\n0.147560\n\n\n2\nG72000109564001\n2020\nPR\n1500000US720019564001\n720019564001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n483\n0.295956\n0.103064\n1852\n322\n0.173866\n801\n677\n124\n0.154806\n\n\n3\nG72000109564002\n2020\nPR\n1500000US720019564002\n720019564002\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n370\n0.365252\n0.092593\n1143\n277\n0.242345\n493\n418\n75\n0.152130\n\n\n4\nG72000109565001\n2020\nPR\n1500000US720019565001\n720019565001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n523\n0.278191\n0.138142\n2100\n346\n0.164762\n827\n682\n145\n0.175333\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2244\nG72015307506011\n2020\nPR\n1500000US721537506011\n721537506011\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n487\n0.243988\n0.167658\n1335\n298\n0.223221\n565\n475\n90\n0.159292\n\n\n2245\nG72015307506012\n2020\nPR\n1500000US721537506012\n721537506012\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n284\n0.294912\n0.083146\n2986\n481\n0.161085\n1201\n1058\n143\n0.119067\n\n\n2246\nG72015307506013\n2020\nPR\n1500000US721537506013\n721537506013\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n360\n0.255500\n0.184638\n994\n69\n0.069416\n340\n322\n18\n0.052941\n\n\n2247\nG72015307506021\n2020\nPR\n1500000US721537506021\n721537506021\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n481\n0.353937\n0.221906\n1872\n436\n0.232906\n813\n681\n132\n0.162362\n\n\n2248\nG72015307506022\n2020\nPR\n1500000US721537506022\n721537506022\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n342\n0.278502\n0.187500\n1269\n267\n0.210402\n637\n463\n174\n0.273155\n\n\n\n\n2249 rows × 88 columns"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#comparing-time-series",
    "href": "m301-coastal-hazards-PR.html#comparing-time-series",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Comparing Time Series",
    "text": "Comparing Time Series\nTo understand demographic and housing trends over time, we compare key indicators from different time periods. By calculating the absolute changes we can assess patterns of population decline, aging, and housing dynamics at the local level.\nThis step calculates absolute changes between 2010 and 2020 for several key indicators: total population, population aged 60+, vacant housing units, and the ratios of aged population and vacancy. These new columns will help identify trends across geographic areas. Missing values in the AgedRatio_Change column are filled with 0 to ensure clean outputs.\n\n# Calculate absolute changes between 2010 and 2020\nmerged_df[\"Total_Pop_Change\"] = merged_df[\"Total_Population_2020\"] - merged_df[\"Total_Population_2010\"]  # Change in total population\nmerged_df[\"Pop60plus_Change\"] = merged_df[\"Pop60plus_total_2020\"] - merged_df[\"Pop60plus_total_2010\"]  # Change in 60+ population\nmerged_df[\"Vacant_Change\"] = merged_df[\"Vacant_2020\"] - merged_df[\"Vacant_2010\"]  # Change in vacant units\nmerged_df[\"AgedRatio_Change\"] = merged_df[\"AgedRatio_2020\"] - merged_df[\"AgedRatio_2010\"]  # Change in aged ratio\nmerged_df[\"AgedRatio_Change\"].fillna(0, inplace=True)  # Fill missing with 0\nmerged_df[\"VacantRatio_Change\"] = merged_df[\"VacantRatio_2020\"] - merged_df[\"VacantRatio_2010\"]  # Change in vacancy ratio\n\n# View updated DataFrame\nmerged_df\n\n\n\n\n\n\n\n\nGISJOIN\nYEAR\nSTUSAB\nGEOID\nGEOCODE\nREGIONA\nDIVISIONA\nSTATE\nSTATEA\nCOUNTY\n...\nAgedRatio_2010\nTotal_Housing_Units_2010\nOccupied_2010\nVacant_2010\nVacantRatio_2010\nTotal_Pop_Change\nPop60plus_Change\nVacant_Change\nAgedRatio_Change\nVacantRatio_Change\n\n\n\n\n0\nG72000109563001\n2020\nPR\n1500000US720019563001\n720019563001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n0.167204\n854\n737\n117\n0.137002\n-287\n199\n-33\n0.131097\n-0.034187\n\n\n1\nG72000109563002\n2020\nPR\n1500000US720019563002\n720019563002\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n0.202309\n881\n751\n130\n0.147560\n-181\n130\n-74\n0.083981\n-0.077296\n\n\n2\nG72000109564001\n2020\nPR\n1500000US720019564001\n720019564001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n0.173866\n801\n677\n124\n0.154806\n-220\n161\n-50\n0.122090\n-0.051742\n\n\n3\nG72000109564002\n2020\nPR\n1500000US720019564002\n720019564002\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n0.242345\n493\n418\n75\n0.152130\n-130\n93\n-35\n0.122907\n-0.059537\n\n\n4\nG72000109565001\n2020\nPR\n1500000US720019565001\n720019565001\n9\n0\nPuerto Rico\n72\nAdjuntas Municipio\n...\n0.164762\n827\n682\n145\n0.175333\n-220\n177\n-32\n0.113430\n-0.037191\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2244\nG72015307506011\n2020\nPR\n1500000US721537506011\n721537506011\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n0.223221\n565\n475\n90\n0.159292\n661\n189\n51\n0.020767\n0.008366\n\n\n2245\nG72015307506012\n2020\nPR\n1500000US721537506012\n721537506012\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n0.161085\n1201\n1058\n143\n0.119067\n-2023\n-197\n-106\n0.133827\n-0.035921\n\n\n2246\nG72015307506013\n2020\nPR\n1500000US721537506013\n721537506013\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n0.069416\n340\n322\n18\n0.052941\n415\n291\n107\n0.186084\n0.131697\n\n\n2247\nG72015307506021\n2020\nPR\n1500000US721537506021\n721537506021\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n0.232906\n813\n681\n132\n0.162362\n-513\n45\n24\n0.121031\n0.059544\n\n\n2248\nG72015307506022\n2020\nPR\n1500000US721537506022\n721537506022\n9\n0\nPuerto Rico\n72\nYauco Municipio\n...\n0.210402\n637\n463\n174\n0.273155\n-41\n75\n-66\n0.068100\n-0.085655\n\n\n\n\n2249 rows × 93 columns"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#generating-age-pyramids",
    "href": "m301-coastal-hazards-PR.html#generating-age-pyramids",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Generating Age Pyramids",
    "text": "Generating Age Pyramids\nAnother useful way to analyze popylation is with population pyramids for Puerto Rico using mean percentages of male and female population by age group, based on census block-level data. The pyramid compares the age structure visually between sexes, helping identify trends like population aging or gender imbalances in specific cohorts.\nKey Steps in the Code:\nFirst, Extract age-by-sex columns from the dataset.\n\n# Define the relevant columns\njoined_columns = [col for col in merged_df.columns if col.startswith(\"Male:\") or col.startswith(\"Female:\")]\n\n# Create DataFrame for percentage values\npct_df = merged_df[[\"Total_Population_2020\"] + joined_columns].copy()\n\nConvert raw population counts into percentage values by age group, making them easier to compare across geographic units regardless of their total population size. Then, it separates the age group columns into male and female categories to prepare for a population pyramid:\n\n# 4. Compute percentage for each age group column\nfor col in joined_columns:\n    pct_df[f\"{col}_pct\"] = pct_df[col] / pct_df[\"Total_Population_2020\"] * 100\n\n# 5. Drop original count columns, keep only percent columns\npct_df = pct_df[[col for col in pct_df.columns if col.endswith(\"_pct\")]]\n\n# 6. Separate by sex\nmale_age_percentages = []\nfemale_age_percentages = []\n\nThis following part calculates the average percent of total population for each age group, separately for males and females, across all geographic units (e.g., blocks).\nIt also creates a clean DataFrame that aligns age group labels with their corresponding male and female percentages, setting the foundation for the population pyramid.\n\nfor col in pct_df.columns:\n    mean_pct = round(pct_df[col].mean(), 2)\n    if col.startswith(\"Male:\"):\n        male_age_percentages.append(mean_pct)\n    elif col.startswith(\"Female:\"):\n        female_age_percentages.append(mean_pct)\n\n# 7. Extract age labels\nnew_age_columns = [col.split(\": \")[1].replace(\"_pct\", \"\") for col in pct_df.columns if col.startswith(\"Male:\")]\n\n# 8. Create the base DataFrame\npopulation_df = pd.DataFrame({\n    \"Age\": new_age_columns,\n    \"Male\": male_age_percentages,\n    \"Female\": female_age_percentages\n})\npopulation = population_df.copy()\n\nWe can cambine narrow age bands into broader, more interpretable age groups (like 15–19 or 20–24) to improve the readability of the population pyramid.\nIn the plot, we manually add percentages for adjacent rows as the age group label.\nAfter combining, it prepares the data for bidirectional horizontal bar plotting by making male values negative (so they show on the left), keeping female values positive (on the right), and creating helper columns for width and placement of bars.\n\n# Combine select age groups\n# Combine 15-17 and 18-19 into 15-19\npopulation.iloc[3, 0] = \"15 to 19 years\"\npopulation.iloc[3, 1] += population.iloc[4, 1]\npopulation.iloc[3, 2] += population.iloc[4, 2]\npopulation = population.drop(index=4).reset_index(drop=True)\n\n# Combine 20, 21, 22–24 into 20–24\npopulation.iloc[4, 0] = \"20 to 24 years\"\npopulation.iloc[4, 1] += population.iloc[5, 1] + population.iloc[6, 1]\npopulation.iloc[4, 2] += population.iloc[5, 2] + population.iloc[6, 2]\npopulation = population.drop(index=[5, 6]).reset_index(drop=True)\n\n# Combine 60–61 and 62–64 into 60–64\npopulation.iloc[12, 0] = \"60 to 64 years\"\npopulation.iloc[12, 1] += population.iloc[13, 1]\npopulation.iloc[12, 2] += population.iloc[13, 2]\npopulation = population.drop(index=13).reset_index(drop=True)\n\n# Combine 65–66 and 67–69 into 65–69\npopulation.iloc[13, 0] = \"65 to 69 years\"\npopulation.iloc[13, 1] += population.iloc[14, 1]\npopulation.iloc[13, 2] += population.iloc[14, 2]\npopulation = population.drop(index=14).reset_index(drop=True)\n\n# Prepare for plotting\npopulation[\"Female_Left\"] = 0\npopulation[\"Female_Width\"] = population[\"Female\"]\npopulation[\"Male_Left\"] = -population[\"Male\"]\npopulation[\"Male_Width\"] = population[\"Male\"]\n\n# Round for labels\npopulation[\"Male\"] = population[\"Male\"].round(2)\npopulation[\"Female\"] = population[\"Female\"].round(2)\n\nThe horizontal population pyramid displays the average percent of Puerto Rico’s population in each age group, broken down by sex. Male bars extend left using negative values while Female bars extend right using positive values.\nA custom title, axis formatting, and color scheme are applied for clarity and polish.\n\n# 11. Plot the population pyramid\nfig = plt.figure(figsize=(15, 10))  # Set figure size\n\n# Plot female bars to the right\nplt.barh(\n    y=population[\"Age\"],\n    width=population[\"Female_Width\"],\n    color=\"#ee7a87\",\n    label=\"Female\"\n)\n\n# Plot male bars to the left\nplt.barh(\n    y=population[\"Age\"],\n    width=population[\"Male_Width\"],\n    left=population[\"Male_Left\"],\n    color=\"#4682b4\",\n    label=\"Male\"\n)\n\n# Add gender labels to plot\nplt.text(-5, len(population) - 1, \"Male\", fontsize=25, fontweight=\"bold\")\nplt.text(4, len(population) - 1, \"Female\", fontsize=25, fontweight=\"bold\")\n\n# Add percentage labels to each bar\nfor idx in range(len(population)):\n    # Male label (on the left)\n    plt.text(\n        x=population[\"Male_Left\"][idx] - 0.1,\n        y=idx,\n        s=\"{}%\".format(population[\"Male\"][idx]),\n        ha=\"right\", va=\"center\",\n        fontsize=15, color=\"#4682b4\"\n    )\n    # Female label (on the right)\n    plt.text(\n        x=population[\"Female_Width\"][idx] + 0.1,\n        y=idx,\n        s=\"{}%\".format(population[\"Female\"][idx]),\n        ha=\"left\", va=\"center\",\n        fontsize=15, color=\"#ee7a87\"\n    )\n\n# Format axes and labels\nplt.xlim(-7, 7)  # Set x-axis limits\nplt.xticks(range(-7, 8), [\"{}%\".format(i) for i in range(-7, 8)])  # Format tick labels as percents\nplt.legend(loc=\"best\")  # Add legend\nplt.xlabel(\"Percent (%)\", fontsize=16, fontweight=\"bold\")\nplt.ylabel(\"Age Range\", fontsize=16, fontweight=\"bold\")\n\n# Add title\nplt.title(\n    \"Puerto Rico Mean Age Distribution (Census Blocks) 2020\",\n    loc=\"left\", pad=20,\n    fontsize=25, fontweight=\"bold\"\n)\n\nplt.tight_layout()  # Optimize spacing\nplt.show()  # Display the plot\n\n\n\n\n\n\n\n\n\nSelect an individual census block\nWe can subset the dataframe to select a single census block and generate an age pyramid.\n\n# --- STEP 1: Select a census block by GEOID ---\nblock_id = \"G72002100310212\"  # Replace with any valid GEOID\nblock_df = merged_df[merged_df[\"GISJOIN\"] == block_id].copy()\n\nassert len(block_df) == 1, f\"Expected one row for GEOID {block_id}, but got {len(block_df)}\"\n\n# --- STEP 2: Identify male and female age columns ---\nmale_cols = [col for col in block_df.columns if col.startswith(\"Male:\")]\nfemale_cols = [col for col in block_df.columns if col.startswith(\"Female:\")]\nage_labels = [col.split(\": \")[1] for col in male_cols]  # Extract age ranges\n\n# --- STEP 3: Get values (choose percent or raw counts) ---\nuse_percent = True  # Set to False if you want raw counts\n\nif use_percent:\n    total_pop = block_df[male_cols + female_cols].sum(axis=1).values[0]\n    male_vals = (block_df[male_cols].values[0] / total_pop) * 100\n    female_vals = (block_df[female_cols].values[0] / total_pop) * 100\n    value_label = \"Percent (%)\"\nelse:\n    male_vals = block_df[male_cols].values[0]\n    female_vals = block_df[female_cols].values[0]\n    value_label = \"Population Count\"\n\n# --- STEP 4: Build plotting dataframe ---\npopulation = pd.DataFrame({\n    \"Age\": age_labels,\n    \"Male\": male_vals,\n    \"Female\": female_vals\n})\n\npopulation[\"Female_Left\"] = 0\npopulation[\"Female_Width\"] = population[\"Female\"]\npopulation[\"Male_Left\"] = -population[\"Male\"]\npopulation[\"Male_Width\"] = population[\"Male\"]\n\n# --- STEP 5: Plot the pyramid ---\nfig = plt.figure(figsize=(12, 8))\n\nplt.barh(y=population[\"Age\"], width=population[\"Female_Width\"], color=\"#ee7a87\", label=\"Female\")\nplt.barh(y=population[\"Age\"], width=population[\"Male_Width\"], left=population[\"Male_Left\"],\n         color=\"#4682b4\", label=\"Male\")\n\nplt.text(-5, len(population) - 1, \"Male\", fontsize=20, fontweight=\"bold\")\nplt.text(4, len(population) - 1, \"Female\", fontsize=20, fontweight=\"bold\")\n\nfor idx in range(len(population)):\n    plt.text(x=population[\"Male_Left\"][idx] - 0.1, y=idx, s=f\"{round(population['Male'][idx], 1)}\",\n             ha=\"right\", va=\"center\", fontsize=12, color=\"#4682b4\")\n    plt.text(x=population[\"Female_Width\"][idx] + 0.1, y=idx, s=f\"{round(population['Female'][idx], 1)}\",\n             ha=\"left\", va=\"center\", fontsize=12, color=\"#ee7a87\")\n\nxlim_val = max(population[[\"Male\", \"Female\"]].max()) * 1.2\nplt.xlim(-xlim_val, xlim_val)\nplt.xticks([])\nplt.legend(loc=\"best\")\n\nplt.xlabel(value_label, fontsize=14)\nplt.ylabel(\"Age Range\", fontsize=14)\nplt.title(f\"Census Block {block_id} Age Distribution 2020\", fontsize=18, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSelect a large Block Group\nAge pyramids can also be generated by subsetting a Census Block Group by name, for example NAME == \"Block Group 1\" or “Block Group 3”:\n\nbg_name = \"Block Group 3\"\n\ndf_gb = merged_df[merged_df[\"NAME\"] == bg_name ]\n\n\n# 1. Define the relevant columns\njoined_columns = [col for col in df_gb.columns if col.startswith(\"Male:\") or col.startswith(\"Female:\")]\n\n# 2. Ensure Total_Population exists (if not, create it)\nif \"Total_Population\" not in df_gb.columns:\n    df_gb[\"Total_Population\"] = df_gb[joined_columns].sum(axis=1)\n\n# 3. Create DataFrame for percentage values\npct_df = df_gb[[\"Total_Population\"] + joined_columns].copy()\n\n# 4. Compute percentage for each age group column\nfor col in joined_columns:\n    pct_df[f\"{col}_pct\"] = pct_df[col] / pct_df[\"Total_Population\"] * 100\n\n# 5. Drop original count columns, keep only percent columns\npct_df = pct_df[[col for col in pct_df.columns if col.endswith(\"_pct\")]]\n\n# 6. Separate by sex\nmale_age_percentages = []\nfemale_age_percentages = []\n\nfor col in pct_df.columns:\n    mean_pct = round(pct_df[col].mean(), 2)\n    if col.startswith(\"Male:\"):\n        male_age_percentages.append(mean_pct)\n    elif col.startswith(\"Female:\"):\n        female_age_percentages.append(mean_pct)\n\n# 7. Extract age labels\nnew_age_columns = [col.split(\": \")[1].replace(\"_pct\", \"\") for col in pct_df.columns if col.startswith(\"Male:\")]\n\n# 8. Create the base DataFrame\npopulation_df = pd.DataFrame({\n    \"Age\": new_age_columns,\n    \"Male\": male_age_percentages,\n    \"Female\": female_age_percentages\n})\npopulation = population_df.copy()\n\n# 9. Combine select age groups\n# Combine 15-17 and 18-19 into 15-19\npopulation.iloc[3, 0] = \"15 to 19 years\"\npopulation.iloc[3, 1] += population.iloc[4, 1]\npopulation.iloc[3, 2] += population.iloc[4, 2]\npopulation = population.drop(index=4).reset_index(drop=True)\n\n# Combine 20, 21, 22–24 into 20–24\npopulation.iloc[4, 0] = \"20 to 24 years\"\npopulation.iloc[4, 1] += population.iloc[5, 1] + population.iloc[6, 1]\npopulation.iloc[4, 2] += population.iloc[5, 2] + population.iloc[6, 2]\npopulation = population.drop(index=[5, 6]).reset_index(drop=True)\n\n# Combine 60–61 and 62–64 into 60–64\npopulation.iloc[12, 0] = \"60 to 64 years\"\npopulation.iloc[12, 1] += population.iloc[13, 1]\npopulation.iloc[12, 2] += population.iloc[13, 2]\npopulation = population.drop(index=13).reset_index(drop=True)\n\n# Combine 65–66 and 67–69 into 65–69\npopulation.iloc[13, 0] = \"65 to 69 years\"\npopulation.iloc[13, 1] += population.iloc[14, 1]\npopulation.iloc[13, 2] += population.iloc[14, 2]\npopulation = population.drop(index=14).reset_index(drop=True)\n\n# 10. Prepare for plotting\npopulation[\"Female_Left\"] = 0\npopulation[\"Female_Width\"] = population[\"Female\"]\npopulation[\"Male_Left\"] = -population[\"Male\"]\npopulation[\"Male_Width\"] = population[\"Male\"]\n\n# Round for labels\npopulation[\"Male\"] = population[\"Male\"].round(2)\npopulation[\"Female\"] = population[\"Female\"].round(2)\n\n# 11. Plot the population pyramid\nfig = plt.figure(figsize=(15, 10))\n\nplt.barh(y=population[\"Age\"], width=population[\"Female_Width\"], color=\"#ee7a87\", label=\"Female\")\nplt.barh(y=population[\"Age\"], width=population[\"Male_Width\"], left=population[\"Male_Left\"],\n         color=\"#4682b4\", label=\"Male\")\n\nplt.text(-5, len(population) - 1, \"Male\", fontsize=25, fontweight=\"bold\")\nplt.text(4, len(population) - 1, \"Female\", fontsize=25, fontweight=\"bold\")\n\n# Add labels\nfor idx in range(len(population)):\n    plt.text(x=population[\"Male_Left\"][idx] - 0.1, y=idx, s=\"{}%\".format(population[\"Male\"][idx]),\n             ha=\"right\", va=\"center\", fontsize=15, color=\"#4682b4\")\n    plt.text(x=population[\"Female_Width\"][idx] + 0.1, y=idx, s=\"{}%\".format(population[\"Female\"][idx]),\n             ha=\"left\", va=\"center\", fontsize=15, color=\"#ee7a87\")\n\nplt.xlim(-7, 7)\nplt.xticks(range(-7, 8), [\"{}%\".format(i) for i in range(-7, 8)])\nplt.legend(loc=\"best\")\nplt.xlabel(\"Percent (%)\", fontsize=16, fontweight=\"bold\")\nplt.ylabel(\"Age Range\", fontsize=16, fontweight=\"bold\")\nplt.title(f\"Puerto Rico Mean Age Distribution ({bg_name}) 2020\", loc=\"left\", pad=20, fontsize=25, fontweight=\"bold\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#interactinv-with-arcgis-online-portal",
    "href": "m301-coastal-hazards-PR.html#interactinv-with-arcgis-online-portal",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Interactinv with ArcGIS Online Portal",
    "text": "Interactinv with ArcGIS Online Portal\nArcGIS Online or Enterprise portal are tools that retrieve a specific hosted feature layers. We use the arcgis package to find features of U.S. counties (2020), and filter it to include only features where STATEFP = ‘72’, which corresponds to Puerto Rico. The resulting spatial data is converted into a Spatially Enabled DataFrame (sedf) using ArcGIS’ Python API.\n\n# Get the layer from the published data\n\n# Connect to ArcGIS Online (anonymous session)\ngis = GIS()\n\n# Access a specific hosted feature layer by its Item ID\nlayer = gis.content.get(\"3132216944b249a08d13b1aa0ee6fda2\").layers[0]  # PR counties 2020 layer\n\n# Query Puerto Rico counties using the state FIPS code '72'\nsedf = layer.query(where=\"STATEFP = '72'\").sdf  # Convert to Spatially Enabled DataFrame\n\nWe can merge the spatial county feature layer for Puerto Rico (2020) with your demographic data (2010–2020) using the GISJOIN field as a common geographic identifier. The result, pr_sedf_ipums, is a Spatially Enabled DataFrame that contains both geometry and attribute data—making it ready for mapping or spatial analysis!\n\n#Merge the feature to the merged 2010-2020 df\npr_sedf_ipums = sedf.merge(merged_df, on = \"GISJOIN\", how = \"inner\")"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#summarizing-and-plotting-data",
    "href": "m301-coastal-hazards-PR.html#summarizing-and-plotting-data",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Summarizing and Plotting DAta",
    "text": "Summarizing and Plotting DAta\nLet’s combine the Block Groups into Counties and Map the results."
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#hieu-summarize-merged_df-to-county-pd_county_df-using-sum-to-add-total_population-male-female-pop60plus_total-total_housing_units-occupied-vacant-for-all-have-_2010-and-_2020-at-the-end.",
    "href": "m301-coastal-hazards-PR.html#hieu-summarize-merged_df-to-county-pd_county_df-using-sum-to-add-total_population-male-female-pop60plus_total-total_housing_units-occupied-vacant-for-all-have-_2010-and-_2020-at-the-end.",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "HIEU SUMMARIZE merged_df TO COUNTY (pd_county_df) using “sum” to add: “Total_Population”, “Male”, “Female”, “Pop60plus_total”, “Total_Housing_Units”, “Occupied”, “Vacant”, for all have _2010 and _2020 at the end.",
    "text": "HIEU SUMMARIZE merged_df TO COUNTY (pd_county_df) using “sum” to add: “Total_Population”, “Male”, “Female”, “Pop60plus_total”, “Total_Housing_Units”, “Occupied”, “Vacant”, for all have _2010 and _2020 at the end.\n\nHieu Recalculate CHange\n\n# pr_county_df[\"AgedRatio_2010\"] = pr_county_df[\"Pop60plus_total_2010\"] /  pr_county_df[\"Total_Population_2010\"]\n# pr_county_df[\"VacantRatio_2010\"] = pr_county_df[\"Vacant_2010\"] /  pr_county_df[\"Total_Housing_Units_2010\"]\n\n\n# pr_county_df[\"AgedRatio_2020\"] = pr_county_df[\"Pop60plus_total_2020\"] /  pr_county_df[\"Total_Population_2020\"]\n# pr_county_df[\"VacantRatio_2020\"] = pr_county_df[\"Vacant_2020\"] /  pr_county_df[\"Total_Housing_Units_2020\"]\n\n\n# pr_county_df[\"Total_Pop_Change\"] = pr_county_df[\"Total_Population_2020\"] - pr_county_df[\"Total_Population_2010\"]\n# pr_county_df[\"Pop60plus_Change\"] = pr_county_df[\"Pop60plus_total_2020\"] - pr_county_df[\"Pop60plus_total_2010\"]\n# pr_county_df[\"Vacant_Change\"] = (pr_county_df[\"Vacant_2020\"] - pr_county_df[\"Vacant_2010\"])\n\n# pr_county_df[\"VacantRatio_Change\"] = pr_county_df[\"VacantRatio_2020\"] - pr_county_df[\"VacantRatio_2010\"]\n\n# pr_county_df[\"AgedRatio_Change\"] = (pr_county_df[\"AgedRatio_2020\"] - pr_county_df[\"AgedRatio_2010\"])\n\n\n# pr_county_df\n\n\n# # Compute means and standard deviations\n# means = pr_county_df[vars_to_plot].mean()\n# stds = pr_county_df[vars_to_plot].std()\n\n# # Create z-score columns\n# for var in vars_to_plot:\n#     z_col = var.replace(\"_Change\", \"_dz\")\n#     pr_county_df[z_col] = (pr_county_df[var] - means[var]) / stds[var]"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#section",
    "href": "m301-coastal-hazards-PR.html#section",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "",
    "text": "# Filter for lowest and highest vacancy quantile groups\n# lowest = pr_county_df[pr_county_df[\"AgedRatio_dz\"] == 1]\n# highest = pr_county_df[pr_county_df[\"AgedRatio_dz\"] == 5]\n\n\n# # Use COUNTYA for grouping (could be COUNTY if preferred)\n# # Step 1: Find common counties between the two groups\n# common_counties = sorted(\n#     set(lowest[\"COUNTY\"]).intersection(set(highest[\"COUNTY\"]))\n# )\n\n# # Step 2: Group and sum vacant housing units for 2010 and 2020, reindexed by common counties\n# vacant_low_2010 = lowest[lowest[\"COUNTY\"].isin(common_counties)] \\\n#     .groupby(\"COUNTY\")[\"VacantRatio_2010\"].sum().reindex(common_counties)\n# vacant_low_2020 = lowest[lowest[\"COUNTYA\"].isin(common_counties)] \\\n#     .groupby(\"COUNTY\")[\"VacantRatio_2020\"].sum().reindex(common_counties)\n\n# vacant_high_2010 = highest[highest[\"COUNTY\"].isin(common_counties)] \\\n#     .groupby(\"COUNTY\")[\"VacantRatio_2010\"].sum().reindex(common_counties)\n# vacant_high_2020 = highest[highest[\"COUNTY\"].isin(common_counties)] \\\n#     .groupby(\"COUNTY\")[\"VacantRatio_2020\"].sum().reindex(common_counties)\n\n# # Step 3: Plot\n# x = np.arange(len(common_counties))  # consistent X positions\n\n# fig, ax = plt.subplots(figsize=(12, 6))\n\n# ax.scatter(x, vacant_low_2010.values, label='Lowest Age Ratio 2010', color='skyblue', marker='o', s=60)\n# ax.scatter(x, vacant_low_2020.values, label='Lowest Age Ratio 2020', color='blue', marker='o', s=60)\n# ax.scatter(x, vacant_high_2010.values, label='Highest Age Ratio 2010', color='orange', marker='^', s=60)\n# ax.scatter(x, vacant_high_2020.values, label='Highest Age Ratio 2020', color='darkred', marker='^', s=60)\n\n# # Formatting\n# ax.set_ylabel('Vacant Housing Ratio')\n# ax.set_xlabel('County (COUNTYA)')\n# ax.set_title('Vacant Housing Units by County: Lowest vs Highest Age Ratio Change Quantile (2010 & 2020)')\n# ax.set_xticks(x)\n# ax.set_xticklabels(common_counties, rotation=90)\n# ax.legend()\n# ax.grid(True, axis='y')\n\n# plt.tight_layout()\n# plt.show()"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#start-preprocessing-the-census-data-downloaded-from-ipums-api",
    "href": "m301-coastal-hazards-PR.html#start-preprocessing-the-census-data-downloaded-from-ipums-api",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Start preprocessing the Census data downloaded from IPUMS API",
    "text": "Start preprocessing the Census data downloaded from IPUMS API\nTo map the shapefile to the map, we have to publish the shapefile as feature layer on MapViewer on ArcGIS Online by adding layer from your shapefile zipped folder.\nThis lesson is public facing so we already published the layers for the learners.\n\n# # Explicit interactive authentication\n# gis = GIS(\"https://www.arcgis.com\", authenticate='interactive')\n\n\n# # Get the layer from the published data\n\n# results = gis.content.search(\"Census County Esri_US_Federal_Data\", item_type=\"Feature Layer\")\n# for r in results:\n#     print(r)\n\nSelect the position of the “Cansus Zip Code Tabulation Areas” layer starting with position the first position as 0.\n\n# #select the appropiate item\n# zip_item = results[3]\n# item_id = zip_item.id\n# print(f\"Title: {zip_item.title}\")\n# print(f\"ID: {item_id}\")\n\nWith the item’s ID (ccfe5a9e3eea4ec68b7945b610422b0a) from ablove, we can use the layers for analysis\n\n# from arcgis.features import FeatureLayer\n# # Authenticate interactively\n# gis = GIS(\"https://www.arcgis.com\", authenticate='interactive')\n# layer = gis.content.get(item_id).layers[0] # The layer of PR county 2020 published to the portal\n# # Print all field names\n# field_names = [field['name'] for field in layer.properties.fields]\n# print(field_names)\n# print(layer.url)\n# layer = FeatureLayer(layer.url)"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#hieu-change-this-for-change-this-for-county-to-display-the-county-name-in-the-label-and-either-vacancyratio_dz",
    "href": "m301-coastal-hazards-PR.html#hieu-change-this-for-change-this-for-county-to-display-the-county-name-in-the-label-and-either-vacancyratio_dz",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Hieu Change this for Change this for County to display the “COUNTY” name in the label and either VacancyRatio_dz",
    "text": "Hieu Change this for Change this for County to display the “COUNTY” name in the label and either VacancyRatio_dz\n\n# # Build unique value info entries\n# unique_value_infos = []\n# for i in range(5):\n#     unique_value_infos.append({\n#         \"value\": i + 1,\n#         \"label\": quantile_labels[i],\n#         \"symbol\": symbols[i]\n#     })\n\n# # Define the unique value renderer\n# pr_uvr = {\n#     \"type\": \"uniqueValue\",\n#     \"field1\": \"Total_Population\",\n#     \"uniqueValueInfos\": unique_value_infos\n# }\n\n# # Define labeling info\n# pr_labeling_info = [\n#     {\n#         \"labelExpression\": \"[NAME_x]\",\n#         \"labelPlacement\": \"esriServerPolygonPlacementAlwaysHorizontal\",\n#         \"repeatLabel\": True,\n#         \"symbol\": {\n#             \"type\": \"esriTS\",\n#             \"color\": [0, 0, 0, 255],\n#             \"font\": {\n#                 \"family\": \"Arial\",\n#                 \"size\": 12\n#             },\n#             \"horizontalAlignment\": \"center\",\n#             \"kerning\": True\n#         }\n#     }\n# ]\n\n# # Layer display options\n# pr_options_dict = {\n#     \"showLabels\": True,\n#     \"layerDefinition\": {\n#         \"drawingInfo\": {\n#             \"labelingInfo\": pr_labeling_info,\n#             \"renderer\": pr_uvr\n#         }\n#     },\n#     \"opacity\": 0.5,\n#     \"title\": \"Vacant Housing Units Decadal Change\"\n# }\n\n\n\n# gis = GIS()\n# m1 = gis.map(\"Puerto Rico\") \n \n# m1.content.add(pr_sedf_ipums, options = pr_options_dict) \n# # update the legend label for LECZ layer in Notebooks\n \n# m1.legend.enabled = True\n\n# m1"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#hieu-overlay-lecz-polygons-and-add-a-classification-column-of-1-or-0-to-the-counties-dataframe",
    "href": "m301-coastal-hazards-PR.html#hieu-overlay-lecz-polygons-and-add-a-classification-column-of-1-or-0-to-the-counties-dataframe",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Hieu Overlay LECZ polygons and add a classification column of 1 or 0 to the counties dataframe",
    "text": "Hieu Overlay LECZ polygons and add a classification column of 1 or 0 to the counties dataframe"
  },
  {
    "objectID": "m301-coastal-hazards-PR.html#plot-bloxplot-showing-differences-in-vacant-and-aged-ratios-in-the-two-groups-in-lecz-not-in-lecz",
    "href": "m301-coastal-hazards-PR.html#plot-bloxplot-showing-differences-in-vacant-and-aged-ratios-in-the-two-groups-in-lecz-not-in-lecz",
    "title": "Who is Exposed to Coastal Hazards in Puerto Rico?",
    "section": "Plot bloxplot showing differences in Vacant and Aged Ratios in the two groups (in lecz, not in lecz)",
    "text": "Plot bloxplot showing differences in Vacant and Aged Ratios in the two groups (in lecz, not in lecz)"
  }
]