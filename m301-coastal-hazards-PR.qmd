---
title: "Who is Exposed to Coastal Hazards in Puerto Rico?"
author: 
   - Deborah Balk 
   - Kytt MacManus
   - Hieu Tran 
   - Camilla Green 
   - Shemontee Chowdhury
   - Juan F. Martinez
format: 
    html
bibliography: lecz-references.bib
---

## Datasets:
- VIIRS Nighttime Lights
- LECZ
- NHGIS
- LiDAR SAR

## Areas of Interest (AOIs):
- Puerto Rico (PRI)

## Functions:
- Image segmentation
- Validation with SAR or LiDAR

# Overview

In this lesson, you will use a dataset delineating [Low Elevation Coastal Zones](https://sedac.ciesin.columbia.edu/data/set/lecz-low-elevation-coastal-zones) from the NASA Socioeconomic Data and Applications Center (SEDAC) website along with census data from the [US Census Bureau](https://www.census.gov/data.html) from the [IPUMS](https://www.ipums.org/) National Historical Geographic Information System (NHGIS). 

You will perform various preprocessing tasks to combina tabular and spatial boundary data for analysis. These steps include exploring the dataset with visualizations and thematic mapping tools. You will then learn how to generate summary statistics based on combinations of these two layers, over two different time periods. 

You will learn how to perform numerous data manipulations, create statistical summaries of population-at-risk (and related housing characteristics), and examine a decade of change throughout this lesson.

# Learning Objectives

- Become familiar with Census Block and County census data for 5-year age groups from the U.S. Census.
- Become familiar with Low Elevation Coastal Zones (LECZs) in Puerto Rico and explain their significance (as well as limitations) in assessing coastal hazard exposure.
- Access, integrate, explore, and use LECZ data from NASA SEDAC and demographic data from the US Census Bureau for Puerto Rico.
- Assess decadal changes (2010–2020) in population and housing characteristics in coastal versus non-coastal zones in Puerto Rico.
- Create regional and local scale maps and statistical figures of exposure and decadal change.
- Identify venues for sharing output (for example, discussion board associated with data, policy briefs, op-eds).

# Introduction

## Low Elevation Coastal Zones (LECZs)

Low Elevation Coastal Zones (LECZs) have been defined globally, with population estimates for areas below 5m and 10m elevations [@mcgranahan2007; @macmanus2021]. 

In the continental U.S. (CONUS), 1 in 10 people live in the 10m LECZ, and studies highlight that urban residents, people of color, and older adults are disproportionately exposed. For instance, about 1 in 5 urban Black residents live in this zone [@tagtachian2023; @hauer2020sea].

This is the sneak peak of what LECZ looks like:
![](data/lecz_pr/lecz_satellite.png)

You may wonder why studies of the “entire” US often restrict themselves to the CONUS? The simplest answer is limitations either data or computational power. For example, this happens because of incomplete coverage in one data set or another. Some US territories may not collect the full suite of census variables that are collected in CONUS. 

For example the detail on housing characteristics is limited in Guam, Northern Mariana Islands, US Virgin Islands and American Samoa, and the Census’ American Community Survey is not conducted in any of the territories, though Puerto Rico conducts its own Community Survey. In some other cases, data collected from satellites (such as SRTM) have variable accuracy toward polar regions [U.S Census Bureau](https://docs.google.com/document/d/1bKYopZoMLCD2djl2vc3pwNSKwGDk6TKRI79swefIvKY/edit?tab=t.0). 

Another possible reason for omission outside of CONUS could be computational challenges or limitations. For instance US territories are subject to different map projections, which implies the need for additional functions in processing algorithms to account for spatial variations and to unify spatial structures.

 
::: {.callout-tip}

## Programming Reminder


### “What is a Map Projection?”

Map projections have existed for thousands of years. They help map makers render the spherical surface of the Earth flat -- so it can be seen on a piece of paper or computer screen, and so that the unit of measure is uniform throughout the area of interest. As a result, map projections distort something -- area, shape, distance, direction -- to make this possible. 

Here are some resources to learn more about map projections:

[A brief video explainer](https://www.youtube.com/watch?v=wlfLW1j05Dg)

[A brief guide from USGS](https://pubs.usgs.gov/gip/70047422/report.pdf)

There are many resources to guide a new learner, so enjoy learning! 



:::


# Accessing Data


This lesson uses the Python language. These are the equired packages to run this lesson `impumspy`, `pandas`, `arcgis`, `zipfile`, and `numpy`. Optional packages include `glob`, `geopandas`, and `earthaccess`.

Use `import` to import the necessary packages:

```{python}
#| results: hide


 
import arcgis
from arcgis.gis import GIS


# Standard libraries
import os
import glob
import re
from zipfile import ZipFile
from dotenv import load_dotenv
import pprint
import requests

# Data manipulation and plotting
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from matplotlib.ticker import FuncFormatter

# Geospatial libraries
import geopandas as gpd
import arcgis
from arcgis.gis import GIS
from arcgis.features import GeoAccessor, GeoSeriesAccessor
from arcgis.raster import Raster
import arcgis.mapping  # For using WebMap, MapView, etc.

# Earthdata access
import earthaccess as ea

# IPUMS API and DDI access
from ipumspy.api import IpumsApiClient
from ipumspy import AggregateDataExtract, readers, ddi

from ipumspy import readers, ddi
from ipumspy.api import IpumsApiClient
from ipumspy import AggregateDataExtract

from ipumspy.api.extract import NhgisDataset

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter


from shapely.geometry import Polygon

import folium 
  
 


```


This lesson uses arcgis version of 2.4.0 or higher:

```{python}
# Check the arcgis version for mapping properly 
arcgis.__version__
```

If arcgis version is lower, use `pip install`:

```{python}
#| eval: false


pip install arcgis==2.4.1.1

```



## Using IPUMS API to pull U.S Census Data for Puerto Rico


### Registering to IPUMS and the National Historical Geographic Information System (NHGIS) 

In order to retrieve an IPUMS API Key, you will have to register for an account for IPUMS and request your [API Key](https://account.ipums.org/api_keys).

Additionally register, to [The National Historical Geographic Information System (NHGIS)](https://uma.pop.umn.edu/nhgis/registration)

After you requested your IPUMS API, to the NHGIS, store it in `os.env` format. You will need your registration email and the API Key:


```{python} 
#| eval: false

 
load_dotenv()
IPUMS_API_KEY = os.getenv("EMAIL", "API_KEY")

if IPUMS_API_KEY is None:
    raise ValueError("API key not found. Make sure IPUMS_API_KEY is defined in your .env file.")

# Step 2: Initialize IPUMS client
ipums = IpumsApiClient(IPUMS_API_KEY)


```

```{python} 
#| echo: false


# Step 1: Load API key from .env file
load_dotenv()
IPUMS_API_KEY = os.getenv("jfm2205@columbia.edu", "59cba10d8a5da536fc06b59d0f22aac2b5f64a819df686fd2d47d0e4")

if IPUMS_API_KEY is None:
    raise ValueError("API key not found. Make sure IPUMS_API_KEY is defined in your .env file.")

# Step 2: Initialize IPUMS client
ipums = IpumsApiClient(IPUMS_API_KEY)


```




Getting shapefile metadata in order to get the filename for downloading the shapefile in the below chunk.

This block will be returning the shapefile name of interest so that we can download it in the next block


```{python}

# After registering to NHGIS, please run this code

for page in ipums.get_metadata_catalog("nhgis", metadata_type="shapefiles"):
    for shapefile in page["data"]:
        if shapefile["extent"] == "Puerto Rico":
            if shapefile["geographicLevel"] == "Block Group" and shapefile["year"] == "2010":
                print( "Name: " + shapefile["name"] + " | Year: " + shapefile["year"])
```
### Downlaoding shapefiles from IPUMS

With this API key, we can extract geospatial data from the IPUMS API. Using the geo level `blck_grp`, we can speicify that we want to extract data at the Block Group level.


The `AggregateDataExtract` function specifies the collection to use, in this case NHGIS, give it a human-readable label for the extract request, and requests the 2010 and 2020 Summary File 1 (SF1a) dataset with tables P12 (sex by age) and H3 (vacancy status) at the block group geographic level. It also limits the extract to geographic extent code "720" (Puerto Rico).

Getting data from 2010 and 2020 with the variables for 5-year Age Groups (P12) and the Housing (H3). 


```{python}
#| eval: false


# Submit extraction data to IPUMS portal
extract = AggregateDataExtract(
    collection="nhgis",  # Use NHGIS collection
    description="Puerto Rico 2010–2020 vacancy",  # Extract label
    datasets=[
        NhgisDataset(
            name="2010_SF1a",  # 2010 dataset
            data_tables=["P12", "H3"],  # Tables: sex by age, vacancy
            geog_levels=["blck_grp"]  # At block group level
        ),
        NhgisDataset(
            name="2020_DHCa",  # 2020 dataset
            data_tables=["P12", "H3"],  # Same tables
            geog_levels=["blck_grp"]  # Same level
        ),
    ],
    geographic_extents=["720"]  # Puerto Rico only
    # shapefiles=["720_blck_grp_2020_tl2020"]  # Optional: include shapefile
)




```




This code sends the extract request to IPUMS, prints the unique extract ID so you can track it, and sets up to wait until the extract is finished.
 
```{python}
#| eval: false


# Submit the extract request
ipums.submit_extract(extract)  # Send request to IPUMS
print(f"Extract ID: {extract.extract_id}")  # Print the extract ID

# Wait for the extract to finish

```

This code sets up the folder where the extract will be saved, creates it if it doesn't already exist, and downloads the extract from IPUMS to that location:

```{python}
#| eval: false


# Download the extract
current = os.getcwd()  # Get current working directory
DOWNLOAD_DIR = os.path.join(f"{current}/data/ipums/block")  # Set download path

os.makedirs(DOWNLOAD_DIR, exist_ok=True)  # Create folder if needed

ipums.download_extract(extract, download_dir=DOWNLOAD_DIR)  # Download files to folder

```

After downloading the extract, this code navigates to the download directory, identifies the ZIP file containing the CSV data, and inspects its contents. It then locates the specific CSV files for the years 2010 and 2020 using filename patterns, and reads them directly from the ZIP archive into pandas DataFrames—no need to manually unzip anything!
```{python}

current = os.getcwd()  # Get current working directory
DOWNLOAD_DIR = os.path.join(f"{current}/data/ipums/block")  # Set path to downloaded extract
file_list = os.listdir(DOWNLOAD_DIR)  # List files in download folder
csv_zip = [f for f in file_list if f.endswith('_csv.zip')]  # Find ZIP with CSVs
csv = f"{DOWNLOAD_DIR}/{csv_zip[0]}"  # Get full path to ZIP

# Read zip data file in the extract
with ZipFile(csv) as z:
    csv_data = z.namelist()  # List files inside ZIP
    print("Contents of zip: ", csv_data)
    
    # Find the correct CSVs using filename patterns
    file_2020 = next(f for f in csv_data if '2020' in f and f.endswith('.csv'))  # 2020 file
    file_2010 = next(f for f in csv_data if '2010' in f and f.endswith('.csv'))  # 2010 file

    # Read CSVs into DataFrames
    with z.open(file_2020) as f:
        df_2020 = pd.read_csv(f)  # Load 2020 data

    with z.open(file_2010) as f:
        df_2010 = pd.read_csv(f)  # Load 2010 data


```


This section uses NHGIS Codebook file(s) that were automatically included in your data extract to rename cryptic column codes in the 2010 and 2020 datasets to human-readable labels. These codes correspond to census tables on sex by age and housing occupancy. Renaming makes analysis and visualization much easier later in your workflow.

Look for the .txt file(s) in the zipped file you downloaded, and they will shed some light on your data.

### Cleaning and processing data

```{python}

# The NHGIS codes are as follows in the documentation which is downloaded from the IPUMS API 

# Rename columns for dataframe 2020

'''    Table 1:     Sex by Age for Selected Age Categories
    Universe:    Total population
    Source code: P12
    NHGIS code:  U7S
        U7S001:      Total
        U7S002:      Male
        U7S003:      Male: Under 5 years
        U7S004:      Male: 5 to 9 years
        U7S005:      Male: 10 to 14 years
        U7S006:      Male: 15 to 17 years
        U7S007:      Male: 18 and 19 years
        U7S008:      Male: 20 years
        U7S009:      Male: 21 years
        U7S010:      Male: 22 to 24 years
        U7S011:      Male: 25 to 29 years
        U7S012:      Male: 30 to 34 years
        U7S013:      Male: 35 to 39 years
        U7S014:      Male: 40 to 44 years
        U7S015:      Male: 45 to 49 years
        U7S016:      Male: 50 to 54 years
        U7S017:      Male: 55 to 59 years
        U7S018:      Male: 60 and 61 years
        U7S019:      Male: 62 to 64 years
        U7S020:      Male: 65 and 66 years
        U7S021:      Male: 67 to 69 years
        U7S022:      Male: 70 to 74 years
        U7S023:      Male: 75 to 79 years
        U7S024:      Male: 80 to 84 years
        U7S025:      Male: 85 years and over
        U7S026:      Female
        U7S027:      Female: Under 5 years
        U7S028:      Female: 5 to 9 years
        U7S029:      Female: 10 to 14 years
        U7S030:      Female: 15 to 17 years
        U7S031:      Female: 18 and 19 years
        U7S032:      Female: 20 years
        U7S033:      Female: 21 years
        U7S034:      Female: 22 to 24 years
        U7S035:      Female: 25 to 29 years
        U7S036:      Female: 30 to 34 years
        U7S037:      Female: 35 to 39 years
        U7S038:      Female: 40 to 44 years
        U7S039:      Female: 45 to 49 years
        U7S040:      Female: 50 to 54 years
        U7S041:      Female: 55 to 59 years
        U7S042:      Female: 60 and 61 years
        U7S043:      Female: 62 to 64 years
        U7S044:      Female: 65 and 66 years
        U7S045:      Female: 67 to 69 years
        U7S046:      Female: 70 to 74 years
        U7S047:      Female: 75 to 79 years
        U7S048:      Female: 80 to 84 years
        U7S049:      Female: 85 years and over
 
    Table 2:     Occupancy Status
    Universe:    Housing units
    Source code: H3
    NHGIS code:  U9X
        U9X001:      Total
        U9X002:      Occupied
        U9X003:      Vacant
'''


rename_2020 = {
    "U7S001": "Total_Population",
    "U7S002": "Male",
    "U7S003": "Male: Under 5 years",
    "U7S004": "Male: 5 to 9 years",
    "U7S005":      "Male: 10 to 14 years",
    "U7S006":      "Male: 15 to 17 years",
    "U7S007":      "Male: 18 and 19 years",
    "U7S008":      "Male: 20 years",
    "U7S009":      "Male: 21 years",
    "U7S010":      "Male: 22 to 24 years",
    "U7S011":      "Male: 25 to 29 years",
    "U7S012":      "Male: 30 to 34 years",
    "U7S013":      "Male: 35 to 39 years",
    "U7S014":      "Male: 40 to 44 years",
    "U7S015":      "Male: 45 to 49 years",
    "U7S016":      "Male: 50 to 54 years",
    "U7S017":      "Male: 55 to 59 years",
    "U7S018":      "Male: 60 and 61 years",
    "U7S019":      "Male: 62 to 64 years",
    "U7S020":      "Male: 65 and 66 years",
    "U7S021":      "Male: 67 to 69 years",
    "U7S022":      "Male: 70 to 74 years",
    "U7S023":      "Male: 75 to 79 years",
    "U7S024":      "Male: 80 to 84 years",
    "U7S025":      "Male: 85 years and over",
    "U7S026":      "Female",
    "U7S027":      "Female: Under 5 years",
    "U7S028":      "Female: 5 to 9 years",
    "U7S029":      "Female: 10 to 14 years",
    "U7S030":      "Female: 15 to 17 years",
    "U7S031":      "Female: 18 and 19 years",
    "U7S032":      "Female: 20 years",
    "U7S033":      "Female: 21 years",
    "U7S034":      "Female: 22 to 24 years",
    "U7S035":      "Female: 25 to 29 years",
    "U7S036":      "Female: 30 to 34 years",
    "U7S037":      "Female: 35 to 39 years",
    "U7S038":      "Female: 40 to 44 years",
    "U7S039":      "Female: 45 to 49 years",
    "U7S040":      "Female: 50 to 54 years",
    "U7S041":      "Female: 55 to 59 years",
    "U7S042":      "Female: 60 and 61 years",
    "U7S043":      "Female: 62 to 64 years",
    "U7S044":      "Female: 65 and 66 years",
    "U7S045":      "Female: 67 to 69 years",
    "U7S046":      "Female: 70 to 74 years",
    "U7S047":      "Female: 75 to 79 years",
    "U7S048":      "Female: 80 to 84 years",
    "U7S049":      "Female: 85 years and over",
    "U9X001": "Total_Housing_Units",
    "U9X002": "Occupied",
    "U9X003": "Vacant"
}

#Rename columns for dataframe 2010
'''    Table 1:     Housing Units
    Universe:    Housing units
    Source code: H1
    NHGIS code:  IFC
        IFC001:      Total
 
    Table 2:     Occupancy Status
    Universe:    Housing units
    Source code: H3
    NHGIS code:  IFE
        IFE001:      Total
        IFE002:      Occupied
        IFE003:      Vacant'''

rename_2010 = {
    "H76001": "Total_Population",
    "H76002": "Male",
    "H76003": "Male: Under 5 years",
    "H76004": "Male: 5 to 9 years",
    "H76005":      "Male: 10 to 14 years",
    "H76006":      "Male: 15 to 17 years",
    "H76007":      "Male: 18 and 19 years",
    "H76008":      "Male: 20 years",
    "H76009":      "Male: 21 years",
    "H76010":      "Male: 22 to 24 years",
    "H76011":      "Male: 25 to 29 years",
    "H76012":      "Male: 30 to 34 years",
    "H76013":      "Male: 35 to 39 years",
    "H76014":      "Male: 40 to 44 years",
    "H76015":      "Male: 45 to 49 years",
    "H76016":      "Male: 50 to 54 years",
    "H76017":      "Male: 55 to 59 years",
    "H76018":      "Male: 60 and 61 years",
    "H76019":      "Male: 62 to 64 years",
    "H76020":      "Male: 65 and 66 years",
    "H76021":      "Male: 67 to 69 years",
    "H76022":      "Male: 70 to 74 years",
    "H76023":      "Male: 75 to 79 years",
    "H76024":      "Male: 80 to 84 years",
    "H76025":      "Male: 85 years and over",
    "H76026":      "Female",
    "H76027":      "Female: Under 5 years",
    "H76028":      "Female: 5 to 9 years",
    "H76029":      "Female: 10 to 14 years",
    "H76030":      "Female: 15 to 17 years",
    "H76031":      "Female: 18 and 19 years",
    "H76032":      "Female: 20 years",
    "H76033":      "Female: 21 years",
    "H76034":      "Female: 22 to 24 years",
    "H76035":      "Female: 25 to 29 years",
    "H76036":      "Female: 30 to 34 years",
    "H76037":      "Female: 35 to 39 years",
    "H76038":      "Female: 40 to 44 years",
    "H76039":      "Female: 45 to 49 years",
    "H76040":      "Female: 50 to 54 years",
    "H76041":      "Female: 55 to 59 years",
    "H76042":      "Female: 60 and 61 years",
    "H76043":      "Female: 62 to 64 years",
    "H76044":      "Female: 65 and 66 years",
    "H76045":      "Female: 67 to 69 years",
    "H76046":      "Female: 70 to 74 years",
    "H76047":      "Female: 75 to 79 years",
    "H76048":      "Female: 80 to 84 years",
    "H76049":      "Female: 85 years and over",
    "IFC001": "Total_Housing",
    "IFE001": "Total_Housing_Units",
    "IFE002": "Occupied",
    "IFE003": "Vacant"
}



# Apply renaming to both datasets
df_2010.rename(columns=rename_2010, inplace=True)  # Rename 2010 columns
df_2020.rename(columns=rename_2020, inplace=True)  # Rename 2020 columns

```

This step filters both datasets to include only records from Puerto Rico, which is identified in the IPUMS data by `STATEA == 72`. It also removes any columns that are completely empty (all values are NaN), which helps clean up the data for analysis.

```{python}

# Subset Puerto Rico (STATEA code 72)
pr_df_2010 = df_2010[df_2010["STATEA"] == 72]  # Filter 2010 data for Puerto Rico
pr_df_2020 = df_2020[df_2020["STATEA"] == 72]  # Filter 2020 data for Puerto Rico

# Drop columns with all missing values
pr_df_2010 = pr_df_2010.dropna(axis=1, how='all')  # Clean 2010 data
pr_df_2020 = pr_df_2020.dropna(axis=1, how='all')  # Clean 2020 data

```


This step calculates the total population aged 60 and over by summing the relevant male and female age group columns. It then computes two new indicators for both 2010 and 2020:

-   AgedRatio: the share of the total population that is 60+

-   VacantRatio: the share of housing units that are vacant

These ratios help measure aging and housing vacancy patterns in Puerto Rico over time.

```{python}


# Define age columns for population 60+
pop60plus_cols = [
    "Female: 60 and 61 years",
    "Female: 62 to 64 years",
    "Female: 65 and 66 years",
    "Female: 67 to 69 years",
    "Female: 70 to 74 years",
    "Female: 75 to 79 years",
    "Female: 80 to 84 years",
    "Female: 85 years and over",
    "Male: 60 and 61 years",
    "Male: 62 to 64 years",
    "Male: 65 and 66 years",
    "Male: 67 to 69 years",
    "Male: 70 to 74 years",
    "Male: 75 to 79 years",
    "Male: 80 to 84 years",
    "Male: 85 years and over"
]

# Calculate totals and ratios for 2010
pr_df_2010["Pop60plus_total"] = pr_df_2010[pop60plus_cols].sum(axis=1)  # Sum 60+ pop
pr_df_2010["AgedRatio"] = pr_df_2010["Pop60plus_total"] / pr_df_2010["Total_Population"]  # 60+ share
pr_df_2010["VacantRatio"] = pr_df_2010["Vacant"] / pr_df_2010["Total_Housing_Units"]  # Vacancy share

# Calculate totals and ratios for 2020
pr_df_2020["Pop60plus_total"] = pr_df_2020[pop60plus_cols].sum(axis=1)
pr_df_2020["AgedRatio"] = pr_df_2020["Pop60plus_total"] / pr_df_2020["Total_Population"]
pr_df_2020["VacantRatio"] = pr_df_2020["Vacant"] / pr_df_2020["Total_Housing_Units"]


```

Lets create a scatter plot to explore the relationship between aging and housing vacancy in Puerto Rico in 2010. Each point represents a geographic unit (e.g., block group), where the x-axis shows the Vacant Ratio (share of housing units that are vacant), and the y-axis shows the Aged Ratio (share of population aged 60+).

This kind of visualization helps reveal spatial patterns or clusters related to population aging and housing dynamics.

```{python}

# Plot Aged Ratio vs. Vacant Ratio for 2010
plt.scatter(pr_df_2010["VacantRatio"], pr_df_2010["AgedRatio"], alpha=0.3)  # Transparent points

plt.title("Puero Rico Aged Ratio (60+) over Vacancy Ratio, 2010")  # Plot title
plt.xlabel("Vacant Ratio")  # X-axis label
plt.ylabel("Aged Ratio")  # Y-axis label
plt.grid(True)  # Show grid
plt.tight_layout()  # Adjust layout
plt.show()  # Display plot

```



## Generating Age Pyramids

Another useful way to analyze popylation is with population pyramids for Puerto Rico using male and female population by age group, based on census block-level data. The pyramid compares the age structure visually between sexes, helping identify trends like population aging or gender imbalances in specific cohorts.

Key Steps in the Code:

First, Extract age-by-sex columns from the dataset.

```{python}


# Define the relevant columns
pyramid_columns_2010 = [col for col in pr_df_2010.columns if col.startswith("Male:") or col.startswith("Female:")]
pyramid_columns_2020 = [col for col in pr_df_2020.columns if col.startswith("Male:") or col.startswith("Female:")]


```
 
 

We can cambine narrow age bands into broader, more interpretable age groups (like 15–19 or 20–24) to improve the readability of the population pyramid. 

In the plot, we manually add percentages for adjacent rows as the age group label.

After combining, it prepares the data for bidirectional horizontal bar plotting by making male values negative (so they show on the left), keeping female values positive (on the right), and creating helper columns for width and placement of bars.

```{python}



def combine_age_groups(df, male_prefix="Male: ", female_prefix="Female: "):
    # Define the custom age groupings
    age_groups = {
        "Under 5 years": ["Under 5 years"],
        "5 to 9 years": ["5 to 9 years"],
        "10 to 14 years": ["10 to 14 years"],
        "15 to 19 years": ["15 to 17 years", "18 and 19 years"],
        "20 to 24 years": ["20 years", "21 years", "22 to 24 years"],
        "25 to 29 years": ["25 to 29 years"],
        "30 to 34 years": ["30 to 34 years"],
        "35 to 39 years": ["35 to 39 years"],
        "40 to 44 years": ["40 to 44 years"],
        "45 to 49 years": ["45 to 49 years"],
        "50 to 54 years": ["50 to 54 years"],
        "55 to 59 years": ["55 to 59 years"],
        "60 to 64 years": ["60 and 61 years", "62 to 64 years"],
        "65 to 69 years": ["65 and 66 years", "67 to 69 years"],
        "70 to 74 years": ["70 to 74 years"],
        "75 to 79 years": ["75 to 79 years"],
        "80 to 84 years": ["80 to 84 years"],
        "85 years and over": ["85 years and over"]
    }

    # Aggregate by age groups
    rows = []
    for label, group in age_groups.items():
        male_cols = [male_prefix + g for g in group if male_prefix + g in df.columns]
        female_cols = [female_prefix + g for g in group if female_prefix + g in df.columns]

        male_total = df[male_cols].sum().sum() if male_cols else 0
        female_total = df[female_cols].sum().sum() if female_cols else 0

        rows.append({
            "Age": label,
            "Male": male_total,
            "Female": female_total
        })

    pyramid = pd.DataFrame(rows)

    # Prepare for horizontal bar plotting
    pyramid["Female_Left"] = 0
    pyramid["Female_Width"] = pyramid["Female"]
    pyramid["Male_Left"] = -pyramid["Male"]
    pyramid["Male_Width"] = pyramid["Male"]

    return pyramid
 




```


```{python}

pyramid_2010 = combine_age_groups(pr_df_2010)
pyramid_2020 = combine_age_groups(pr_df_2020)

```


The horizontal population pyramid displays the average percent of Puerto Rico’s population in each age group, broken down by sex. Male bars extend left using negative values while Female bars extend right using positive values.

A custom title, axis formatting, and color scheme are applied for clarity and polish.

```{python}
def plot_pyramid_2010_vs_2020(pyramid_2010, pyramid_2020, title="Puerto Rico Population Pyramid (2010 vs 2020)"):
    def abs_tick(x, pos):
        return f"{abs(int(x)):,}"

    age_labels = pyramid_2010["Age"]
    y = np.arange(len(age_labels))
    bar_height = 0.35

    fig, ax = plt.subplots(figsize=(12, 10))

    # Colors: 2010 (light), 2020 (dark)
    male_colors = ["#c6dbef", "#2171b5"]
    female_colors = ["#fdd0a2", "#e6550d"]

    # Plot Male bars (left)
    ax.barh(y - bar_height, -pyramid_2010["Male"], height=bar_height, color=male_colors[0], label="Male 2010")
    ax.barh(y,              -pyramid_2020["Male"], height=bar_height, color=male_colors[1], label="Male 2020")

    # Plot Female bars (right)
    ax.barh(y - bar_height, pyramid_2010["Female"], height=bar_height, color=female_colors[0], label="Female 2010")
    ax.barh(y,              pyramid_2020["Female"], height=bar_height, color=female_colors[1], label="Female 2020")

    # Format axes
    ax.set_yticks(y - bar_height / 2)
    ax.set_yticklabels(age_labels)
    ax.axvline(0, color="gray", lw=0.8)
    ax.xaxis.set_major_formatter(FuncFormatter(abs_tick))
    ax.grid(axis='x', linestyle='--', linewidth=0.5, alpha=0.5)

    max_val = max(
        pyramid_2010[["Male", "Female"]].values.max(),
        pyramid_2020[["Male", "Female"]].values.max()
    )
    ax.set_xlim(-max_val * 1.1, max_val * 1.1)

    ax.set_title(title)
    ax.set_xlabel("Population")
    ax.set_ylabel("Age Group")
    ax.legend(loc="lower right")

    plt.tight_layout()
    plt.show()


```


```{python}

plot_pyramid_2010_vs_2020(pyramid_2010, pyramid_2020, title="Puerto Rico")

```


### Age Pyrmid of a single County

```{python}
print(pr_df_2010["COUNTY"].unique())
```

```{python}
county_df_2010 = pr_df_2010[pr_df_2010["COUNTY"] == "Arecibo Municipio"]
county_df_2020 = pr_df_2020[pr_df_2020["COUNTY"] == "Arecibo Municipio"]

county_pyr_2010 = combine_age_groups(county_df_2010)
county_pyr_2020 = combine_age_groups(county_df_2020)

```


```{python}

plot_pyramid_2010_vs_2020(county_pyr_2010, county_pyr_2020, "Arecibo")

```


## Municipio (County) analysis

```{python}
def aggregate_to_county(df, county_col="COUNTY", male_prefix="Male", female_prefix="Female",  additional_cols=None):
    if additional_cols is None:
        additional_cols = [
            'Total_Population', 'Total_Housing_Units', 
            'Occupied', 'Vacant',  'Pop60plus_total'
        ]
    
    # Identify all relevant columns to aggregate
    population_cols = [col for col in df.columns if col.startswith(male_prefix) or col.startswith(female_prefix)]
    
    # Combine with additional explicitly named columns (if they exist in df)
    valid_additional_cols = [col for col in additional_cols if col in df.columns]
    group_cols = [county_col] + valid_additional_cols + population_cols

    # Group by county and sum
    df_aggregated = df[group_cols].groupby(county_col, as_index=False).sum()

    return df_aggregated

```

```{python}
county_df_2010 = aggregate_to_county(pr_df_2010)

county_df_2020 = aggregate_to_county(pr_df_2020)

```




### Merginig Datasets

This step prepares for comparison between the 2010 and 2020 datasets. 

It selects key columns (like total population, aged population, and vacancy) from the 2010 data and merges them with the full 2020 data using GISJOIN as a unique geographic identifier. 

The resulting `merged_df` DataFrame contains side-by-side values for both years, using suffixes to distinguish 2010 and 2020 columns.
```{python}
  


# Merge 2020 data with selected 2010 columns using GISJOIN
merged_df = county_df_2020.merge(
    county_df_2010,  
    on="COUNTY",  # Join on geographic ID
    how="inner",  # Keep only matching rows
    suffixes=("_2020", "_2010")  # Label columns by year
)


```

## Comparing Time Series 

To understand demographic and housing trends over time, we compare key indicators from different time periods. By calculating the absolute changes we can assess patterns of population decline, aging, and housing dynamics at the local level.

This step calculates absolute changes between 2010 and 2020 for several key indicators: total population, population aged 60+, vacant housing units, and the ratios of aged population and vacancy. These new columns will help identify trends across geographic areas. Missing values in the AgedRatio_Change column are filled with 0 to ensure clean outputs.

```{python}
 
# Calculate absolute changes between 2010 and 2020
merged_df["Total_Pop_Change"] = merged_df["Total_Population_2020"] - merged_df["Total_Population_2010"]  # Change in total population
merged_df["Pop60plus_Change"] = merged_df["Pop60plus_total_2020"] - merged_df["Pop60plus_total_2010"]  # Change in 60+ population
merged_df["Vacant_Change"] = merged_df["Vacant_2020"] - merged_df["Vacant_2010"]  # Change in vacant units


# Avoid division by zero using .where()
merged_df["Total_Pop_Change_Pct"] = (
    (merged_df["Total_Population_2020"] - merged_df["Total_Population_2010"]) /
    merged_df["Total_Population_2010"].where(merged_df["Total_Population_2010"] != 0)
) * 100

merged_df["Pop60plus_Change_Pct"] = (
    (merged_df["Pop60plus_total_2020"] - merged_df["Pop60plus_total_2010"]) /
    merged_df["Pop60plus_total_2010"].where(merged_df["Pop60plus_total_2010"] != 0)
) * 100

merged_df["Vacant_Change_Pct"] = (
    (merged_df["Vacant_2020"] - merged_df["Vacant_2010"]) /
    merged_df["Vacant_2010"].where(merged_df["Vacant_2010"] != 0)
) * 100



```


# Plotting Graphs from IPUMS data:

This visualization step uses a `Seaborn pairplot` to explore the relationships between key change variables from 2010 to 2020. 

By plotting scatterplots for every pair of variables and histograms along the diagonal we can quickly spot correlations or patterns in population decline, aging, and housing vacancy across Puerto Rico’s geographic units.

```{python}
 
 

# Create pairwise plots with histograms on the diagonal
sns.pairplot(merged_df[["Total_Pop_Change_Pct", "Pop60plus_Change_Pct", "Vacant_Change_Pct", "Total_Population_2020", "Total_Population_2010"]], diag_kind="hist",  corner=True, plot_kws= dict(marker="o",  facecolors='none', edgecolors = 'blue'  ))

plt.suptitle("Pairwise Scatterplots and Histograms", y=1.02)  # Set overall title
plt.tight_layout()
plt.show()  # Display the plot


```





 

By looking more closely at the histogram and comparing vacancy differences between 2010 and 2020, it becomes evident that quantile-based classification may not accurately reflect true differences across areas. In cases with extreme outliers, quantile breaks can distort interpretation.

## Comparing Counties

To better evaluate how unusual each area's change is, we compute z-scores for each variable. This standardizes the values by subtracting the mean and dividing by the standard deviation, allowing for comparisons across indicators on a consistent scale.


::: {.callout-tip }

## Math Refresher

A z-score tells us how many standard deviations a value is from the mean. A positive z-score means the value is above the mean. A negative z-score means it's below the mean. Z-scores help us compare variables on different scales and identify outliers—typically anything above +2 or below –2 is considered unusually high or low.

:::


```{python}

# Compute means and standard deviations
means = merged_df[["Pop60plus_Change_Pct", "Vacant_Change_Pct" ]].mean()
stds = merged_df[["Pop60plus_Change_Pct", "Vacant_Change_Pct" ]].std()

# Create z-score columns
for var in ["Pop60plus_Change_Pct", "Vacant_Change_Pct" ]:
    z_col = var.replace("_Change_Pct", "_dz")
    merged_df[z_col] = (merged_df[var] - means[var]) / stds[var]

```


This `pairplot` function uses the z-score standardized versions of the change variables to compare their relative behavior across Block Groups in a **scatterplot matrix**. 

By putting all variables on the same scale (mean = 0, std = 1), we can asily spot strong linear relationships, identify clusters or outliers, and ompare change intensity across different domains (e.g., aging vs. vacancy).

This is especially useful when your original variables had different units or ranges.


Can you spot any linear trends in the scatterplots?




This scatterplot shows the relationship between changes in vacancy rates and aging ratios (ages 60+) using their z-scores, allowing for meaningful comparison on a standardized scale for the two variables.

This type of scaling:

- Keeps the area near zero linear (for interpretability)

- Compresses extreme values

- Handles both positive and negative changes symmetrically.

This visualization helps identify whether increases in older populations are associated with changes in housing vacancy—and how extreme those shifts are across Puerto Rico.

## Interact with ArcGIS Online Portal

ArcGIS Online or Enterprise portal are tools that retrieve a specific hosted feature layers. We use the `arcgis` package to find features of U.S. counties (2020), and filter it to include only features where STATEFP = '72', which corresponds to Puerto Rico. The resulting spatial data is converted into a Spatially Enabled DataFrame (sedf) using ArcGIS' Python API.

```{python}

# Get the layer from the published data

# Connect to ArcGIS Online (anonymous session)
gis = GIS()

# Access a specific hosted feature layer by its Item ID
layer = gis.content.get("3132216944b249a08d13b1aa0ee6fda2").layers[0]  # PR counties 2020 layer

# Query Puerto Rico counties using the state FIPS code '72'
sedf = layer.query(where="STATEFP = '72'").sdf  # Convert to Spatially Enabled DataFrame

```

We can merge the spatial county feature layer for Puerto Rico (2020) with your  demographic data (2010–2020) using the `GISJOIN` field as a common geographic identifier. The result, `pr_sedf_ipums`, is a Spatially Enabled DataFrame that contains both geometry and attribute data—making it ready for mapping or spatial analysis!

```{python}

# merged_df_cleaned = merged_df[['COUNTY', 'Total_Population_2020',  'Total_Pop_Change_Pct',   'Pop60plus_dz', 'Vacant_dz']]

#Merge the feature to the merged 2010-2020 df
print("`merged_df` DataFrame row count before table join: ", len(merged_df))
pr_sedf_ipums = sedf.merge(merged_df, left_on="NAMELSAD", right_on="COUNTY", how = "inner")

print("`pr_sedf_ipums` DataFrame results after table join: ", len(pr_sedf_ipums))
 

# # # Function to safely convert Esri-style rings to shapely geometry
# def esri_to_shapely(geom):
#     try:
#         rings = geom["rings"]
#         if len(rings) == 1:
#             return Polygon(rings[0])
#         else:
#             return MultiPolygon([Polygon(r) for r in rings])
#     except Exception as e:
#         print(f"Error in geometry: {e}")
#         return None

# # Apply the function to create a shapely geometry column
# pr_sedf_ipums["geometry"] = pr_sedf_ipums["SHAPE"].apply(esri_to_shapely)

# Create a GeoDataFrame with correct CRS (Esri Web Mercator)
# pr_sedf_ipums = gpd.GeoDataFrame(pr_sedf_ipums, geometry="geometry", crs="EPSG:3857")

# pr_sedf_ipums = GeoAccessor.from_geodataframe(pr_sedf_ipums)


```



```{python}
 

# Set up map
gis = GIS()
m = gis.map("Puerto Rico")

# Classify values manually (e.g., quantiles)
num_classes = 5
field = "Pop60plus_dz"
column_values = pr_sedf_ipums[field].dropna()

# Create class breaks
breaks = list(np.quantile(column_values, np.linspace(0, 1, num_classes + 1)))
colors = ["#d73027", "#fc8d59", "#fee090", "#91bfdb", "#4575b4"]  # RdBu reversed

# Define classBreaksRenderer
renderer = {
    "type": "classBreaks",
    "field": field,
    "classificationMethod": "esriClassifyQuantile", 
    "minValue": float(min(column_values)),
    "classBreakInfos": [
        {
            "classMaxValue": float(breaks[i + 1]),
            "label": f"{breaks[i]:.2f} – {breaks[i+1]:.2f}",
            "description": "",
            "symbol": {
                "type": "esriSFS",
                "style": "esriSFSSolid",
                "color": list(int(c.lstrip("#")[i:i+2], 16) for i in (0, 2, 4)) + [180],
                "outline": {
                    "color": [0, 0, 0, 40],
                    "width": 0.4,
                    "type": "esriSLS",
                    "style": "esriSLSSolid"
                }
            }
        }
        for i, c in enumerate(colors)
    ]
}

# Plot with renderer
pr_sedf_ipums.spatial.plot(
    map_widget=m,
    renderer=renderer,
    legend=True
)

m.legend.enabled = True
m


```

Red representing high values and blue representing low values

 

# Search for LECZ MERIT-DEM layer on ArcGIS Online's Living Atlas portal.

With the arcgis package, we can search online for the polygons of the LECZ areas in Puerto Rico


```{python}
lecz = gis.content.search("title:Low Elevation Coastal Zone (LECZ) for Puerto Rico", item_type="Feature")

for item in lecz:
    display(item)


lecz_layer = lecz[0].layers[0]  # Assuming the first layer is the one you want




```

```{python}

lecz_sdf = lecz_layer.query(where="lecz_zone > 0").sdf

``` 

## Examine which counties are exposed to LECZ visually


```{python}


# Set up map
gis = GIS()
m = gis.map("Puerto Rico")

# Classify values manually (e.g., quantiles)
num_classes = 5
field = "Pop60plus_dz"
column_values = pr_sedf_ipums[field].dropna()

# Create class breaks
breaks = list(np.quantile(column_values, np.linspace(0, 1, num_classes + 1)))
colors = ["#d73027", "#fc8d59", "#fee090", "#91bfdb", "#4575b4"]  # RdBu reversed

# Define classBreaksRenderer
renderer = {
    "type": "classBreaks",
    "field": "Pop60plus_dz",
    "classificationMethod": "esriClassifyQuantile", 
    "minValue": float(min(column_values)),
    "classBreakInfos": [
        {
            "classMaxValue": float(breaks[i + 1]),
            "label": f"{breaks[i]:.2f} – {breaks[i+1]:.2f}",
            "description": "",
            "symbol": {
                "type": "esriSFS",
                "style": "esriSFSSolid",
                "color": list(int(c.lstrip("#")[i:i+2], 16) for i in (0, 2, 4)) + [180],
                "outline": {
                    "color": [0, 0, 0, 40],
                    "width": 0.4,
                    "type": "esriSLS",
                    "style": "esriSLSSolid"
                }
            }
        }
        for i, c in enumerate(colors)
    ]
}

# Plot with renderer
pr_sedf_ipums.spatial.plot(
    map_widget=m,
    renderer=renderer,
    legend=True
)

lecz_sdf.spatial.plot(
    map_widget=m
) 
m.legend.enabled = True

m


```



 
##  Overlay Analysis



```{python}

 
 
# 2. Apply to create valid Shapely geometry column

lecz_sdf['geometry'] = lecz_sdf['SHAPE']  

pr_sedf_ipums['geometry'] = pr_sedf_ipums['SHAPE']  

 

lecz_gdf = gpd.GeoDataFrame(lecz_sdf, geometry='geometry')


pr_ipums_gdf = gpd.GeoDataFrame(pr_sedf_ipums, geometry="geometry")



lecz_gdf.set_crs(epsg=3857, inplace=True)

pr_ipums_gdf.set_crs(epsg=3857, inplace=True)


 
lecz_gdf["dissolve_id"] = 1

# Dissolve all rows into one MultiPolygon
lecz_union_gdf = lecz_gdf.dissolve(by=None)
 


```




```{python}

pr_counties_lecz_intersect = gpd.sjoin(pr_ipums_gdf, lecz_union_gdf, how="left", predicate="intersects")

```



```{python}
 
 
# Rename the column
pr_counties_lecz_intersect = pr_counties_lecz_intersect.rename(columns={"dissolve_id": "lecz"})

# Replace NaN with 0 and 1.0 with 1
pr_counties_lecz_intersect["lecz"] = pr_counties_lecz_intersect["lecz"].fillna(0)  # Replace NaN with 0
pr_counties_lecz_intersect["lecz"] = pr_counties_lecz_intersect["lecz"].replace(1.0, 1)  # Replace 1.0 with 1
pr_counties_lecz_intersect["lecz"] = pr_counties_lecz_intersect["lecz"].astype(int)



```


```{python}

 
gdf = pr_counties_lecz_intersect.to_crs(epsg=4326)

# Ensure 'lecz' is binary int and 'id' is string
gdf["lecz"] = gdf["lecz"].fillna(0).astype(int)


# Add unique ID column for choropleth
gdf = gdf.reset_index(drop=True)
gdf["id"] = gdf.index.astype(str)


# Get bounds
minx, miny, maxx, maxy = gdf.total_bounds

# Compute center of bounding box
center = [(miny + maxy) / 2, (minx + maxx) / 2]

```

```{python}



m = folium.Map(location=center, zoom_start=8)


# Choropleth layer for one variable (e.g., 'TotalPopulation_2020')
folium.Choropleth(
    geo_data=gdf,
    data=gdf,
    columns=["id", "lecz"],
    key_on="feature.properties.id",
    fill_color="PuOr",
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name="LECZ zone (1 = Inside, 0 = Outside)"
).add_to(m)

# Show map
m

```



# Comparing populaitons in and out of the LECZ

```{python}

 


cols_to_include = ["lecz", "Total_Population_2020"] + [
    col for col in gdf.columns if col.endswith("_Pct") or col.endswith("_dz")
]

df = gdf[cols_to_include].dropna()
df["lecz"] = df["lecz"].astype(int)

# Step 2: Build aggregation dictionary
agg_dict = {
    "Total_Population_2020": ("Total_Population_2020", "mean"),
}
for col in df.columns:
    if col.endswith("_Pct") or col.endswith("_dz"):
        agg_dict[col] = (col, "mean")

# Add count separately
agg_dict["count"] = ("Total_Population_2020", "count")

# Step 3: Group and aggregate
summary = df.groupby("lecz").agg(**agg_dict)

# Step 4: Rename index for readability
summary.index = summary.index.map({0: "Outside LECZ Mean", 1: "Inside LECZ Mean"})

# Create a new DataFrame with rounded summary
summary_df = summary.round(2)



# Group and sum
population_sums = df.groupby("lecz")["Total_Population_2020"].sum()

# Optional: Rename for clarity
population_sums.index = population_sums.index.map({0: "Outside LECZ", 1: "Inside LECZ"})

# Print result
print("Total Population by LECZ Group:")
print(population_sums)

# Now you can use it like any regular DataFrame
print(summary_df)

```





```{python} 

# Select variables to plot
plot_vars = [
    "Total_Pop_Change_Pct",
    "Pop60plus_Change_Pct",
    "Vacant_Change_Pct",
    "Total_Population_2020",
    "Total_Population_2010",
    "lecz"
]

# Drop missing values
df_pair = gdf[plot_vars].dropna()
df_pair["lecz"] = df_pair["lecz"].astype(int)  # Ensure it's an int or categorical

# Optional: relabel LECZ for nicer legend
df_pair["LECZ_Label"] = df_pair["lecz"].map({0: "Outside LECZ", 1: "Inside LECZ"})

# Now plot (omit 'lecz' itself from x/y vars)
sns.pairplot(
    df_pair,
    vars=plot_vars[:-1],  # exclude 'lecz'
    hue="LECZ_Label",
    diag_kind="hist",
    corner=True,
    plot_kws=dict(marker="o", edgecolor="gray", alpha=0.7)
)

plt.suptitle("Pairwise Scatterplots by LECZ", y=1.02)
plt.tight_layout()
plt.show()  

```



# Pyramid Comparison


```{python}


# ─────────────────────────────────────────
# Helper: Combine narrow age bands
# ─────────────────────────────────────────
def combine_age_groups(df, suffix="", male_prefix="Male: ", female_prefix="Female: "):
    age_groups = {
        "Under 5 years": ["Under 5 years"],
        "5 to 9 years": ["5 to 9 years"],
        "10 to 14 years": ["10 to 14 years"],
        "15 to 19 years": ["15 to 17 years", "18 and 19 years"],
        "20 to 24 years": ["20 years", "21 years", "22 to 24 years"],
        "25 to 29 years": ["25 to 29 years"],
        "30 to 34 years": ["30 to 34 years"],
        "35 to 39 years": ["35 to 39 years"],
        "40 to 44 years": ["40 to 44 years"],
        "45 to 49 years": ["45 to 49 years"],
        "50 to 54 years": ["50 to 54 years"],
        "55 to 59 years": ["55 to 59 years"],
        "60 to 64 years": ["60 and 61 years", "62 to 64 years"],
        "65 to 69 years": ["65 and 66 years", "67 to 69 years"],
        "70 to 74 years": ["70 to 74 years"],
        "75 to 79 years": ["75 to 79 years"],
        "80 to 84 years": ["80 to 84 years"],
        "85 years and over": ["85 years and over"]
    }

    rows = []
    for label, group in age_groups.items():
        row = {"Age": label}
        for lecz in [0, 1]:
            male_cols = [f"{male_prefix}{g}{suffix}" for g in group if f"{male_prefix}{g}{suffix}" in df.columns]
            female_cols = [f"{female_prefix}{g}{suffix}" for g in group if f"{female_prefix}{g}{suffix}" in df.columns]

            male_total = df[df["lecz"] == lecz][male_cols].sum().sum() if male_cols else 0
            female_total = df[df["lecz"] == lecz][female_cols].sum().sum() if female_cols else 0

            row[f"Male_{lecz}"] = male_total
            row[f"Female_{lecz}"] = female_total
        rows.append(row)

    return pd.DataFrame(rows)

# ─────────────────────────────────────────
# Plot function
# ─────────────────────────────────────────
def plot_pyramid_stacked_by_lecz(pyr2010, pyr2020): 
    def abs_tick(x, pos):
        return f"{abs(int(x)):,}"

    age_labels = pyr2010["Age"]
    y = np.arange(len(age_labels))
    bar_height = 0.4

    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(12, 14), sharex=True)

    # OUTSIDE LECZ (lecz = 0) — Top
    ax1.barh(y - bar_height/2, -pyr2010["Male_0"], height=bar_height, color="#c6dbef", label="Male 2010")
    ax1.barh(y + bar_height/2, -pyr2020["Male_0"], height=bar_height, color="#2171b5", label="Male 2020")
    ax1.barh(y - bar_height/2, pyr2010["Female_0"], height=bar_height, color="#fdd0a2", label="Female 2010")
    ax1.barh(y + bar_height/2, pyr2020["Female_0"], height=bar_height, color="#e6550d", label="Female 2020")
    ax1.set_title("Population Pyramid – Outside LECZ")
    ax1.set_yticks(y)
    ax1.set_yticklabels(age_labels)
    ax1.axvline(0, color="gray", lw=0.8)
    ax1.legend(loc="lower right")
    ax1.xaxis.set_major_formatter(FuncFormatter(abs_tick))
    ax1.grid(axis="x", linestyle="--", linewidth=0.5, alpha=0.5)

    # INSIDE LECZ (lecz = 1) — Bottom
    ax2.barh(y - bar_height/2, -pyr2010["Male_1"], height=bar_height, color="#c6dbef")
    ax2.barh(y + bar_height/2, -pyr2020["Male_1"], height=bar_height, color="#2171b5")
    ax2.barh(y - bar_height/2, pyr2010["Female_1"], height=bar_height, color="#fdd0a2")
    ax2.barh(y + bar_height/2, pyr2020["Female_1"], height=bar_height, color="#e6550d")
    ax2.set_title("Population Pyramid – Inside LECZ")
    ax2.set_yticks(y)
    ax2.set_yticklabels(age_labels)
    ax2.axvline(0, color="gray", lw=0.8)
    ax2.xaxis.set_major_formatter(FuncFormatter(abs_tick))
    ax2.set_xlabel("Population")
    ax2.grid(axis="x", linestyle="--", linewidth=0.5, alpha=0.5)

    max_val = max(
        pyr2010[["Male_0", "Male_1", "Female_0", "Female_1"]].values.max(),
        pyr2020[["Male_0", "Male_1", "Female_0", "Female_1"]].values.max()
    )
    ax1.set_xlim(-max_val * 1.1, max_val * 1.1)

    plt.tight_layout()
    plt.show()


# ─────────────────────────────────────────
# Full execution
# ─────────────────────────────────────────
# Extract columns by suffix
pyr_cols_2010 = [col for col in gdf.columns if col.endswith("_2010") and (col.startswith("Male:") or col.startswith("Female:"))]
pyr_cols_2020 = [col for col in gdf.columns if col.endswith("_2020") and (col.startswith("Male:") or col.startswith("Female:"))]

# Filter and clean
df_2010 = gdf[["lecz"] + pyr_cols_2010].dropna()
df_2020 = gdf[["lecz"] + pyr_cols_2020].dropna()

# Combine age groups by LECZ
pyramid_2010 = combine_age_groups(df_2010, suffix="_2010")
pyramid_2020 = combine_age_groups(df_2020, suffix="_2020")

# Plot pyramids
plot_pyramid_stacked_by_lecz(pyramid_2010, pyramid_2020)

```

